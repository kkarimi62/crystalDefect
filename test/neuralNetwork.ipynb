{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cded12cf",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#import-libs\" data-toc-modified-id=\"import-libs-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>import libs</a></span></li><li><span><a href=\"#Train-NN\" data-toc-modified-id=\"Train-NN-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Train NN</a></span><ul class=\"toc-item\"><li><span><a href=\"#main():-classifier\" data-toc-modified-id=\"main():-classifier-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>main(): classifier</a></span></li><li><span><a href=\"#main():-regressor\" data-toc-modified-id=\"main():-regressor-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>main(): regressor</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot\" data-toc-modified-id=\"Plot-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Plot</a></span></li></ul></li><li><span><a href=\"#test-example:-2d\" data-toc-modified-id=\"test-example:-2d-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>test example: 2d</a></span><ul class=\"toc-item\"><li><span><a href=\"#fully-connected-in-sklearn\" data-toc-modified-id=\"fully-connected-in-sklearn-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>fully connected in sklearn</a></span></li><li><span><a href=\"#fully-connected-in-keras\" data-toc-modified-id=\"fully-connected-in-keras-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>fully connected in keras</a></span></li><li><span><a href=\"#cnn\" data-toc-modified-id=\"cnn-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>cnn</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117638d",
   "metadata": {},
   "source": [
    "# import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "49343b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf. file sections: ['flags', 'input files', 'descriptors', 'neural net', 'neural net classification', 'neural net regression', 'gnn', 'ml mc']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "confParser = configparser.ConfigParser()\n",
    "\n",
    "#--- parse conf. file\n",
    "confParser.read('configuration.ini')\n",
    "print('conf. file sections:',confParser.sections())\n",
    "#\n",
    "import os\n",
    "import sys\n",
    "list(map(lambda x:sys.path.append(x), confParser['input files']['lib_path'].split()))\n",
    "from dscribe.descriptors import SOAP, ACSF\n",
    "import ase\n",
    "import ase.io\n",
    "import ase.build\n",
    "from ase.io import lammpsdata\n",
    "import pdb\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import matplotlib.pyplot as plt\n",
    "if not eval(confParser['flags']['RemoteMachine']):\n",
    "    plt.rc('text', usetex=True)\n",
    "import pickle\n",
    "#\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#\n",
    "from scipy.stats import gaussian_kde\n",
    "#\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#--- user modules\n",
    "import LammpsPostProcess as lp\n",
    "import utility as utl\n",
    "import imp\n",
    "imp.reload(utl)\n",
    "imp.reload(lp)\n",
    "\n",
    "#--- increase width\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a2e60",
   "metadata": {},
   "source": [
    "# Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9cd8c31d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "\n",
    "    def __init__(self, verbose=False,\n",
    "                **kwargs\n",
    "                ):\n",
    "        self.verbose = verbose\n",
    "        for key in kwargs:\n",
    "            setattr(self,key,kwargs[key])\n",
    "            \n",
    "            \n",
    "        !mkdir $self.best_model\n",
    "\n",
    "    \n",
    "    def Parse(self,path,nruns):\n",
    "        self.perAtomData = {}\n",
    "        rwjs = utl.ReadWriteJson()\n",
    "        for irun in range(nruns):\n",
    "            if irun == 0:\n",
    "                #--- same configurations!\n",
    "                self.descriptors  = np.c_[rwjs.Read('%s/Run%s/descriptors.json'%(path,irun))[0]['data']]\n",
    "                self.shape        = np.c_[rwjs.Read('%s/Run%s/descriptors.json'%(path,irun))[0]['shape']].flatten()\n",
    "                self.positions    = np.c_[rwjs.Read('%s/Run%s/descriptors.json'%(path,irun))[0]['xyz']]\n",
    "                os.system('ln -s %s/Run%s/dumpFile/dump.xyz .'%(path,irun))\n",
    "            try:\n",
    "                data = np.loadtxt('%s/Run%s/perAtomData.txt'%(path,irun))\n",
    "                #--- displacement data\n",
    "                self.perAtomData[irun] = pd.DataFrame(np.c_[data],\n",
    "                columns='id\ttype\tx\ty\tz\tux\tuy\tuz\tenergy_barrier\tdefect_label'.split()\n",
    "                            )\n",
    "            except:\n",
    "#                 if self.verbose:\n",
    "#                     traceback.print_exc()\n",
    "                continue\n",
    "                \n",
    "        \n",
    "        self.nruns = list(self.perAtomData.keys())\n",
    "        self.nruns.sort()\n",
    "\n",
    "#     def Junk(self,path,nruns):\n",
    "#         self.perAtomData = {}\n",
    "#         self.Descriptors = {}\n",
    "#         self.Shape       = {}\n",
    "#         self.Positions   = {}\n",
    "#         self.Catalogs    = {}\n",
    "#         #\n",
    "#         rwjs = utl.ReadWriteJson()\n",
    "#         for irun in range(nruns):\n",
    "#             try:\n",
    "# #                 data_json               = rwjs.Read('%s/Run%s/descriptors.json'%(path,irun))[0]\n",
    "# #                 self.Descriptors[irun]  = np.c_[data_json['data']]\n",
    "# #                 self.Shape[irun]        = np.c_[data_json['shape']].flatten()\n",
    "# #                 self.Positions[irun]    = np.c_[data_json['xyz']]\n",
    "#                 os.system('ln -s %s/Run%s/dumpFile/dump.xyz ./dump.%s.xyz'%(path,irun,irun))\n",
    "#                 data                    = np.loadtxt('%s/Run%s/perAtomData.txt'%(path,irun))\n",
    "#                 self.perAtomData[irun]  = pd.DataFrame(np.c_[data],\n",
    "#                                                        columns ='id type x y z'.split()\n",
    "#                                                       )\n",
    "#                 self.Catalogs[irun]     = pd.read_csv('%s/Run%s/catalog.txt'%(path,irun))\n",
    "#             except:\n",
    "# #                 if self.verbose:\n",
    "# #                     traceback.print_exc()\n",
    "#                 continue\n",
    "                \n",
    "        \n",
    "#         self.nruns     = list(self.perAtomData.keys())\n",
    "#         self.nruns.sort()\n",
    "\n",
    "#         self.Descriptors[ 0 ] = pd.DataFrame(np.random.random(size=9876))\n",
    "#         #--- assert shape and positions are the same for all realizations\n",
    "# #         self.shape     = self.Shape[ self.nruns[ 0 ] ]\n",
    "# #         self.positions = self.Positions[ self.nruns[ 0 ] ]\n",
    "\n",
    "        \n",
    "        \n",
    "    def Parse2nd(self,path,nruns):\n",
    "        self.perAtomData = {}\n",
    "        self.Descriptors = {}\n",
    "        self.Shape       = {}\n",
    "        self.Positions   = {}\n",
    "        self.Catalogs    = {}\n",
    "        #\n",
    "        rwjs = utl.ReadWriteJson()\n",
    "        for irun in range(nruns):\n",
    "            try:\n",
    "                data_json               = rwjs.Read('%s/Run%s/descriptors.json'%(path,irun))[0]\n",
    "                self.Descriptors[irun]  = np.c_[data_json['data']]\n",
    "                self.Shape[irun]        = np.c_[data_json['shape']].flatten()\n",
    "                self.Positions[irun]    = np.c_[data_json['xyz']]\n",
    "                os.system('ln -s %s/Run%s/dumpFile/dump.xyz ./dump.%s.xyz'%(path,irun,irun))\n",
    "                data                    = np.loadtxt('%s/Run%s/perAtomData.txt'%(path,irun))\n",
    "                self.perAtomData[irun]  = pd.DataFrame(np.c_[data],\n",
    "                                                       columns ='id type x y z'.split()\n",
    "                                                      )\n",
    "                self.Catalogs[irun]     = pd.read_csv('%s/Run%s/catalog.txt'%(path,irun))\n",
    "            except:\n",
    "#                 if self.verbose:\n",
    "#                     traceback.print_exc()\n",
    "                continue\n",
    "                \n",
    "        \n",
    "        self.nruns     = list(self.perAtomData.keys())\n",
    "        self.nruns.sort()\n",
    "\n",
    "        #--- assert shape and positions are the same for all realizations\n",
    "        self.shape     = self.Shape[ self.nruns[ 0 ] ]\n",
    "        self.positions = self.Positions[ self.nruns[ 0 ] ]\n",
    "        \n",
    "    def Combine(self):\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('concatenating descriptors ...')\n",
    "#         pdb.set_trace()\n",
    "        #--- center atoms\n",
    "        center_atom_indices = list(map(lambda x:NeuralNetwork.GetCenterAtom( self.perAtomData[x])[0],self.nruns))\n",
    "        sdict = dict(zip(center_atom_indices,self.nruns))\n",
    "        \n",
    "        atom_ids = list(sdict.keys())\n",
    "        atom_ids.sort()\n",
    "        #         center_atom_indices = list( set( center_atom_indices ) )\n",
    "        data = np.concatenate(list(map(lambda x: np.c_[self.perAtomData[sdict[x]].iloc[ x ]],atom_ids)),axis=1).T\n",
    "#        descriptors_center_atoms = self.descriptors[atom_ids]\n",
    "        descriptors_center_atoms = np.c_[list(map(lambda x:self.Descriptors[sdict[x]][x], atom_ids))]\n",
    "    \n",
    "        #--- data frame\n",
    "#        print(data.shape)\n",
    "        irun = self.nruns[0]\n",
    "        df_combined = pd.DataFrame(data,columns=list(self.perAtomData[irun].keys()))\n",
    "    \n",
    "        #--- filter crystalline atoms\n",
    "        filtr = self.perAtomData[irun].defect_label == 0.0\n",
    "        df_crystalline = self.perAtomData[irun][filtr]\n",
    "        descriptors_crystalline = self.descriptors[filtr]\n",
    "\n",
    "        #--- merge\n",
    "        keys = list(df_combined.keys())\n",
    "        data_concat = np.concatenate([np.c_[df_combined[keys]],np.c_[df_crystalline[keys]]],axis=0) \n",
    "        self.perAtomData = pd.DataFrame(data_concat,\n",
    "                              columns=keys\n",
    "                             )\n",
    "\n",
    "        \n",
    "        #--- merge descriptors\n",
    "        self.descriptors = np.concatenate([descriptors_center_atoms,descriptors_crystalline],axis=0)\n",
    "\n",
    "        assert self.perAtomData.shape[ 0 ] == self.descriptors.shape[0], 'need more mc swaps: %s %s'\\\n",
    "        %(self.perAtomData.shape[ 0 ],self.descriptors.shape[0])\n",
    "                            \n",
    "    def Combine2nd(self):\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('concatenating descriptors ...')\n",
    "            \n",
    "        irun = self.nruns[0]\n",
    "        keys = list( self.perAtomData[ irun ].keys() )\n",
    "\n",
    "        #--- center atoms\n",
    "        data_concat         = np.concatenate(list(map(lambda x: np.c_[self.perAtomData[x]],self.nruns)),axis=0)\n",
    "        self.perAtomData    = pd.DataFrame(data_concat,\n",
    "                                 columns=keys\n",
    "                                )\n",
    "        self.descriptors    = np.concatenate(list(map(lambda x:self.Descriptors[x], self.nruns)))\n",
    "    \n",
    "#     def Junk2nd(self):\n",
    "        \n",
    "#         if self.verbose:\n",
    "#             print('concatenating descriptors ...')\n",
    "            \n",
    "#         irun = self.nruns[0]\n",
    "#         keys = list( self.perAtomData[ irun ].keys() )\n",
    "\n",
    "#         #--- center atoms\n",
    "#         data_concat         = np.concatenate(list(map(lambda x: np.c_[self.perAtomData[x]],self.nruns)),axis=0)\n",
    "#         self.perAtomData    = pd.DataFrame(data_concat,\n",
    "#                                  columns=keys\n",
    "#                                 )\n",
    "#         self.descriptors    = None #self.Descriptors[0]\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def GetCenterAtom(df):\n",
    "        disp_magnitude = df.ux**2+df.uy**2+df.uz**2\n",
    "        center_atom_indx = disp_magnitude.sort_values(ascending=False).index[0]\n",
    "        return center_atom_indx, int(df.iloc[ center_atom_indx ].id)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def zscore(slist):\n",
    "#         tmp = np.copy(slist)\n",
    "#         print(np.mean(tmp),np.std(tmp))\n",
    "#         tmp -= np.mean(tmp)\n",
    "#         tmp /= np.std(tmp)\n",
    "#         return tmp\n",
    "\n",
    "    def PCA(self,\n",
    "           n_components=2,\n",
    "            random_state = 1,\n",
    "           ):\n",
    "        #--- concat. data\n",
    "        X = self.descriptors\n",
    "        pca = PCA(n_components=n_components,random_state=random_state)\n",
    "        pca.fit(X)\n",
    "        X_transformed = pca.transform(X)\n",
    "\n",
    "        xdata = X_transformed[:,0]\n",
    "        ydata = X_transformed[:,1]\n",
    "        #\n",
    "        filtr_defects = self.perAtomData.defect_label == 0.0\n",
    "        #\n",
    "\n",
    "        legend = utl.Legends()\n",
    "        legend.Set(bbox_to_anchor=(1.1,.5, 0.5, 0.5))\n",
    "#         pdb.set_trace()\n",
    "        #ax = utl.PltErr(zscore(xdata)[filtr_defects],zscore(ydata)[filtr_defects],\n",
    "        ax = utl.PltErr(xdata[filtr_defects],ydata[filtr_defects],\n",
    "                  attrs={'fmt':'x','alpha':1,'label':'defect_free'},\n",
    "                        Plot = False,\n",
    "        #                 xlim=(-2,2),\n",
    "        #                 ylim=(-2,2),\n",
    "                  )\n",
    "\n",
    "        #utl.PltErr(zscore(xdata)[~filtr_defects],zscore(ydata)[~filtr_defects],\n",
    "        !mkdir png\n",
    "        utl.PltErr(xdata[~filtr_defects],ydata[~filtr_defects],\n",
    "                  attrs={'fmt':'.','color':'red','label':'defects'},\n",
    "                   ax=ax,\n",
    "                   xstr='pca_1',ystr='pca_2',\n",
    "                   legend = legend.Get(),\n",
    "                   title='png/pca.png'\n",
    "                  )\n",
    "    def Spectra(self,\n",
    "               nrows=100,\n",
    "               ):\n",
    "        assert nrows <= self.descriptors.shape[ 0 ]\n",
    "        !mkdir png\n",
    "        utl.PltBitmap(np.log10(np.abs(self.descriptors[:nrows,:])),\n",
    "                      xlabel=r'$\\mathrm{ndim}$',ylabel=r'$\\mathrm{natom}$',\n",
    "                      xlim=(0,self.descriptors.shape[1]),\n",
    "                      ylim=(0,nrows),\n",
    "                      colorbar=True,\n",
    "                      zscore=False,\n",
    "                      vminmax=(-3,3),\n",
    "                      title='png/feature_bitmap.png'\n",
    "                     )\n",
    "        \n",
    "    def SklearnMLP(self,X_train,y_train):\n",
    "        #-----------------------\n",
    "        #--- parameter grid\n",
    "        #-----------------------\n",
    "#         param_grid = {\n",
    "#                         'hidden_layer_sizes':self.hidden_layer_sizes,\n",
    "#                          #'activation' : ['tanh', 'relu'],\n",
    "#                          'learning_rate_init':self.learning_rate_init,\n",
    "# #                         'alpha':self.alpha, #--- regularization \n",
    "#                          #'learning_rate' : ['invscaling', 'adaptive'],\n",
    "#                         'n_iter_no_change':self.n_iter_no_change,\n",
    "# #                        'tol':self.tol,\n",
    "#                         'max_iter':self.max_iter,\n",
    "#                      } \n",
    "        mlp   =  MLPClassifier(random_state=1,\n",
    "                               hidden_layer_sizes = self.hidden_layer_sizes[0],\n",
    "                               learning_rate_init = self.learning_rate_init[0],\n",
    "                               n_iter_no_change   = self.n_iter_no_change[0],\n",
    "                               max_iter           = self.max_iter[0],\n",
    "                               verbose=self.verbose)\n",
    "#         clf  =  GridSearchCV(mlp, param_grid)\n",
    "#        clf.fit(X_train,y_train)\n",
    "        mlp.fit(X_train,y_train)\n",
    "        model =  mlp #clf.best_estimator_\n",
    "        loss  =  model.loss_curve_\n",
    "        val_loss = loss\n",
    "        return (model, loss, val_loss)\n",
    "\n",
    "    def KerasANN(self, X_train, y_train,X_test, y_test, ndime):\n",
    "\n",
    "        model     = keras.Sequential([ #--- The network architecture\n",
    "                                    layers.Dense(self.hidden_layer_size, activation=self.activation),\n",
    "                #                    layers.Dense(self.hidden_layer_size, activation=self.activation),\n",
    "                                    layers.Dense(ndime, activation='softmax')\n",
    "                                    ])\n",
    "        \n",
    "#         shape         =  (self.shape[0]*self.shape[1]*self.shape[2],)\n",
    "#         inputs        =  keras.Input(shape=shape)\n",
    "#         #------------------------------\n",
    "#         #--- The network architecture\n",
    "#         #------------------------------\n",
    "#         x             =  layers.Dense(   self.hidden_layer_size, activation=self.activation\n",
    "#                                        )(inputs)\n",
    "#         for i in range( self.number_hidden_layers ):\n",
    "#             x       = layers.Dense( self.hidden_layer_size, activation=self.activation\n",
    "#                                      )(x)\n",
    "#         #--- output layer\n",
    "# #         x       = layers.Flatten()(x)\n",
    "#         outputs = layers.Dense( ndime, activation=self.activation)(x)\n",
    "#         model   = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        \n",
    "        \n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate_init) #--- compilation step\n",
    "        model.compile( optimizer=optimizer,\n",
    "                       loss=\"sparse_categorical_crossentropy\",\n",
    "                       metrics=[\"mse\"]\n",
    "                     )\n",
    "        #--- save best model\n",
    "        !mkdir best_model\n",
    "#         callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnetClassifier_from_scratch.tf',  \n",
    "#                                                     monitor=\"mse\",\n",
    "#                                                     save_freq=10,\n",
    "#                                                     save_best_only=True)]\n",
    "\n",
    "        model.fit( X_train, y_train, \n",
    "           validation_data      = ( X_test, y_test ),\n",
    "#             callbacks           = callbacks,\n",
    "            epochs              = self.max_iter[0], \n",
    "            verbose             = self.verbose, \n",
    "            shuffle             = False, \n",
    "#             batch_size     = 32,\n",
    "#                     use_multiprocessing = True,\n",
    "#                     workers             = 4,\n",
    "         )    \n",
    "\n",
    "        model.save('best_model/convnetClassifier_from_scratch.tf')\n",
    "        loss      = model.history.history['loss']\n",
    "        val_loss  = model.history.history['val_loss']\n",
    "        best_model = model #keras.models.load_model(\"best_model/convnetClassifier_from_scratch.tf\")\n",
    "\n",
    "        return (best_model, loss, val_loss)\n",
    "\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def MapClassIds( y ):\n",
    "        ndime         = len(set(y.flatten()))\n",
    "        class_ids     = list(set(y.flatten()))\n",
    "        class_ids.sort()\n",
    "        map_class_ids = dict(zip(class_ids,range(ndime)))        \n",
    "        return ndime, np.c_[list(map(lambda x:[map_class_ids[x]],y.flatten()))]\n",
    "    \n",
    "    @staticmethod\n",
    "    def GetSubSetCrystallineAtoms(X,y,n_train):\n",
    "        #--- data frame\n",
    "        df = pd.DataFrame( y, columns=['topoID'] )\n",
    "        #--- groups\n",
    "        sdict = df.groupby(by='topoID').groups\n",
    "        if sdict[ 0 ].shape[ 0 ] < n_train:\n",
    "            return X, y\n",
    "        indices = np.random.choice(sdict[ 0 ], size=n_train, replace=False)\n",
    "        sdict[ 0 ] = indices\n",
    "        indices_total = np.concatenate( list(map(lambda x:sdict[x],sdict)) )\n",
    "        return X[indices_total], y[ indices_total ]\n",
    "    \n",
    "    def GetLabels( self, irun ):\n",
    "        nsize                                  = self.Descriptors[ irun ].shape[ 0 ]\n",
    "        y_labels                               = np.zeros(nsize,dtype=int)\n",
    "        nonCrystallineAtomsIndices             = self.Catalogs[ irun ].AtomIndex\n",
    "        nonCrystallineAtomsTopoIds             = self.Catalogs[ irun ].IniTopoId.astype(int)\n",
    "        y_labels[ nonCrystallineAtomsIndices ] = nonCrystallineAtomsTopoIds\n",
    "        return y_labels.reshape( ( nsize, 1 ) )\n",
    "\n",
    "    \n",
    "    def TrainClassifier(self,\n",
    "                       random_state=1,\n",
    "                       ):\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "\n",
    "        #--- get labels\n",
    "        y_labels = np.concatenate( list( map(lambda x:self.GetLabels(x), self.nruns ) ) )\n",
    "        assert self.descriptors.shape[ 0 ] == y_labels.shape[ 0 ]\n",
    "        \n",
    "        #--- map topo ids to integers 0, 1 ... ndime\n",
    "        ndime, y = NeuralNetwork.MapClassIds( y_labels )\n",
    "\n",
    "        \n",
    "        X      = np.c_[self.descriptors]\n",
    "\n",
    "        #--- filter: only train a subset of crystalline atoms\n",
    "        X, y   = NeuralNetwork.GetSubSetCrystallineAtoms( X, y, self.n_train )\n",
    "        \n",
    "        #---------------\n",
    "        #--- zscore X\n",
    "        #---------------        \n",
    "        X      = NeuralNetwork.Zscore( X, save_model = '%s/classifier.sav'%self.best_model )\n",
    "\n",
    "        #--- add noise\n",
    "        NeuralNetwork.AddGaussianNoise(X, scale = 0.001 )\n",
    "\n",
    "\n",
    "        #--- exclude void\n",
    "#        filtr  = self.perAtomData.type==2\n",
    "#         X      = X[~filtr]\n",
    "#         y      = y[~filtr]\n",
    "\n",
    "        #--- sample from crystalline atoms\n",
    "        \n",
    "        #-----------------------\n",
    "        #--- train-test split\n",
    "        #-----------------------\n",
    "#        train_size = self.n_train\n",
    "#        test_size  = int( self.n_train / 3 )\n",
    "        assert X.shape[0] >= self.n_train, 'increase nruns!' #train_size + train_size, 'increase nruns!'\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "#                                                               test_size=test_size, train_size=train_size,\n",
    "                                                              random_state=random_state)\n",
    "        if len(set(y_train.flatten())) < ndime:\n",
    "            'warning: not every class present in train set!'\n",
    "        if len(set(y_test.flatten()))  < ndime: \n",
    "            'warning: not every class present in test set!'\n",
    "        \n",
    "        #-----------------------\n",
    "        #--- train model\n",
    "        #-----------------------\n",
    "        if self.fully_connected: #--- dense nn\n",
    "            if self.implementation == 'sklearn':\n",
    "                (model, loss, val_loss) = self.SklearnMLP(X_train,y_train)\n",
    "                classes_x = model.predict(X_test) \n",
    "                \n",
    "            elif self.implementation == 'keras': #--- dense nn in keras\n",
    "                (model, loss, val_loss) = self.KerasANN(X_train, y_train,X_test, y_test, ndime)\n",
    "                predict_x = model.predict(X_test) \n",
    "                classes_x = np.argmax(predict_x,axis=1)\n",
    "                \n",
    "        elif self.cnn: #--- convolutional\n",
    "            (model, loss, val_loss), (X_train, X_test) =\\\n",
    "            self.ConvNetworkClassifier( y )\n",
    "            predict_x = model.predict(X_test) \n",
    "            classes_x = np.argmax(predict_x,axis=1)\n",
    "                    \n",
    "        #--- save loss data\n",
    "        !mkdir png\n",
    "        np.savetxt('png/val_loss_classification.txt',\n",
    "                   np.c_[range(len(loss)),loss,val_loss],\n",
    "                   header='epoch loss val_loss')\n",
    "\n",
    "        #--- confusion matrix\n",
    "        cm = confusion_matrix(y_test, classes_x,\n",
    "                         labels=range(ndime)\n",
    "                        )\n",
    "        np.savetxt('png/confusion.txt',np.c_[cm])\n",
    "\n",
    "        \n",
    "    def PrintDescriptors(self,descriptors,y,fout):\n",
    "        rwjs = utl.ReadWriteJson()\n",
    "        rwjs.Write([{'descriptors':np.c_[descriptors],\n",
    "                     'target':np.c_[y],\n",
    "                     'shape_descriptor':self.shape}],fout)\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def GetTopoIds( catalog, key ): \n",
    "        TopoIds = list( catalog.groupby( by = key ).groups.keys())\n",
    "        TopoIds.sort()\n",
    "        return TopoIds \n",
    "    \n",
    "    @staticmethod\n",
    "    def MapTopoIds( TopoIds ):\n",
    "        ndime         = len( TopoIds )\n",
    "        map_TopoIds = dict(zip(TopoIds,range(ndime)))        \n",
    "        return map_TopoIds\n",
    "\n",
    "    def GetTopoArrayIndex( self, IniTopoId, FinTopoId ):\n",
    "        mappedIniTopoId = self.mappedTopoIds[ IniTopoId ]\n",
    "        mappedFinTopoId = self.mappedTopoIds[ FinTopoId ]\n",
    "        assert mappedIniTopoId < self.n_unique_transition_paths and\\\n",
    "               mappedFinTopoId < self.n_unique_transition_paths \n",
    "        return mappedIniTopoId * self.n_unique_transition_paths * self.ndime + mappedFinTopoId * self.ndime\n",
    "\n",
    "    def GetTopoArrayIndex2nd( self, ux, uy, uz ):\n",
    "        aa = np.c_[[ux,uy,uz]].T\n",
    "        H, bin_edges = np.histogramdd(aa,bins=self.bins)        \n",
    "        assert H.sum() == 1.0\n",
    "        \n",
    "        return H.astype(int).flatten()\n",
    "        \n",
    "\n",
    "    def FillTargetMatrix( self, item ):\n",
    "#        pdb.set_trace()\n",
    "        ux = item.inifin_dr * item.DirX\n",
    "        uy = item.inifin_dr * item.DirY\n",
    "        uz = item.inifin_dr * item.DirZ\n",
    "        assert np.abs( ux ) < self.umax and\\\n",
    "                np.abs( uy ) < self.umax and\\\n",
    "                np.abs( uz ) < self.umax,'ux=%e, uy=%e, uz=%e increase self.umax!'%(ux,uy,uz)\n",
    "        irow                                   = int( item.AtomIndex )\n",
    "#        icol                                   = int( self.GetTopoArrayIndex( item.IniTopoId, item.FinTopoId ) )\n",
    "        H                                   = self.GetTopoArrayIndex2nd( ux, uy, uz )\n",
    "\n",
    "        self.y_targets[ irow ] += H           \n",
    "\n",
    "    def DiscretizeTransitionPath( self ):\n",
    "         #--- hard-coded values\n",
    "        xlin = np.arange(-self.umax,+self.umax+self.du,self.du)\n",
    "        ylin = np.arange(-self.umax,+self.umax+self.du,self.du)\n",
    "        zlin = np.arange(-self.umax,+self.umax+self.du,self.du)\n",
    "        self.nbinx = len(xlin)-1\n",
    "        self.nbiny = len(ylin)-1\n",
    "        self.nbinz = len(zlin)-1\n",
    "        self.bins = (xlin, ylin, zlin)\n",
    "        self.xv, self.yv, self.zv = np.meshgrid( self.bins[1][:-1], self.bins[0][:-1], self.bins[2][:-1] )\n",
    "\n",
    "#        print(yv[H==1],xv[H==1],zv[H==1])\n",
    "\n",
    "    def GetTargets( self, irun ):\n",
    "        #--- set-up y matrix\n",
    "#         IniTopoId                              = NeuralNetwork.GetTopoIds( self.Catalogs[ irun ], 'IniTopoId' )\n",
    "#         FinTopoId                              = NeuralNetwork.GetTopoIds( self.Catalogs[ irun ], 'FinTopoId' )\n",
    "#         TopoIds                                = list( set( FinTopoId + IniTopoId ) ) #--- transition path ids\n",
    "#         TopoIds.sort()\n",
    "#         self.mappedTopoIds                     = NeuralNetwork.MapTopoIds( TopoIds )\n",
    "        \n",
    "#         self.n_unique_transition_paths         = len( TopoIds )\n",
    "\n",
    "\n",
    "        nsize                                  = ( self.Descriptors[ irun ].shape[ 0 ], self.nbinx * self.nbiny * self.nbinz )\n",
    "        self.y_targets                         = np.zeros( nsize[ 0 ] * nsize[ 1 ], dtype=int ).reshape( nsize )\n",
    "        \n",
    "        #--- fill y matrix\n",
    "        self.Catalogs[ irun ].apply( lambda x: self.FillTargetMatrix( x ), axis = 1 )\n",
    "        \n",
    "        #--- assert self.y_targets is binary\n",
    "        assert set(self.y_targets.flatten()) == set(np.array([0,1])), 'decrease du!'\n",
    "        return self.y_targets\n",
    "        \n",
    "    def MaskCrystallineAtoms(self, irun ):\n",
    "        nsize                              = self.Descriptors[ irun ].shape[ 0 ]\n",
    "        mask                               = np.zeros(nsize,dtype=bool)\n",
    "        nonCrystallineAtomsIndices         = np.array( list( set( self.Catalogs[ irun ].AtomIndex ) ) )\n",
    "        mask[ nonCrystallineAtomsIndices ] = True\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def GetInputOutput( self, irun, indx ):\n",
    "        atomIndices     =  self.Catalogs[ irun ].AtomIndex\n",
    "        pixel_map_input =  self.Descriptors[ irun ][ atomIndices ]    \n",
    "        dr              =  np.c_[self.Catalogs[ irun ]['inifin_dr']].flatten()\n",
    "        dr_multi        = np.c_[ dr, dr, dr ]\n",
    "        vector_input    =  self.Catalogs[ irun ][ ' DirX      DirY      DirZ'.split() ] * dr_multi\n",
    "        output          =  self.Catalogs[ irun ][ 'barrier' ]\n",
    "        return [pixel_map_input, vector_input, output][ indx ]\n",
    "\n",
    "    def TrainRegressorBarriers(self,\n",
    "                       random_state=1,\n",
    "                       ):\n",
    "        '''\n",
    "        Multi-layer Perceptron regressor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        stratify : array-like, default=None\n",
    "        If not None, data is split in a stratified fashion, using this as\n",
    "        the class labels.\n",
    "        \n",
    "        y : array-like, target data\n",
    "        \n",
    "        random_state : initial seed, default=1\n",
    "        \n",
    "        printOvito : bool, default=False\n",
    "        \n",
    "        filtr : bool, default=False\n",
    "        if not None, data is filtered before calling train-test split\n",
    "        '''\n",
    "\n",
    "        \n",
    "        #--- get transition path bitmap\n",
    "#        self.DiscretizeTransitionPath()\n",
    "#        self.ndime                                  = 4 #--- hard-coded: (ux,uy,uz,E)\n",
    "        pixel_maps_input = np.concatenate( list( map(lambda x:self.GetInputOutput(x,0), self.nruns ) ) )\n",
    "        vectors_input    = np.concatenate( list( map(lambda x:self.GetInputOutput(x,1), self.nruns ) ) )\n",
    "        scalar_output    = np.concatenate( list( map(lambda x:self.GetInputOutput(x,2), self.nruns ) ) )\n",
    "\n",
    "        \n",
    "        #---------------\n",
    "        #--- zscore X\n",
    "        #---------------        \n",
    "        X      = np.c_[pixel_maps_input,vectors_input]\n",
    "        X      = NeuralNetwork.Zscore( X, save_model = '%s/scaler_regression_barriers.sav'%self.best_model )\n",
    "        y      = np.c_[ scalar_output ]\n",
    "        \n",
    "        if X.shape[ 0 ] - self.n_train < 0 :  \n",
    "            X = NeuralNetwork.Duplicate(X, new_size = self.n_train )\n",
    "            y = NeuralNetwork.Duplicate(y, new_size = self.n_train )\n",
    "\n",
    "        #--- add noise\n",
    "        NeuralNetwork.AddGaussianNoise(X, scale = 0.01 )\n",
    "        NeuralNetwork.AddGaussianNoise(y, scale = 0.01 )\n",
    "\n",
    "        #-----------------------\n",
    "        #--- train-test split\n",
    "        #-----------------------\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, #stratify=stratify,\n",
    "                                                            random_state=random_state)\n",
    "\n",
    "\n",
    "        #-----------------------\n",
    "        #--- train model\n",
    "        #-----------------------\n",
    "\n",
    "                \n",
    "        (model, loss, val_loss), (X_train, X_test) = self.ConvNetworkMixedInput(X_train, y_train, X_test, y_test )\n",
    "            \n",
    "            \n",
    "            \n",
    "            #--- validation\n",
    "        NeuralNetwork.Validation(loss, val_loss, \n",
    "                                 model, \n",
    "                                 X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "    def TrainRegressorTransitionPaths(self,#stratify,y,\n",
    "                       random_state=1,\n",
    "                       printOvito = False,\n",
    " #                      filtr = None,\n",
    "                       ):\n",
    "        '''\n",
    "        Multi-layer Perceptron regressor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        stratify : array-like, default=None\n",
    "        If not None, data is split in a stratified fashion, using this as\n",
    "        the class labels.\n",
    "        \n",
    "        y : array-like, target data\n",
    "        \n",
    "        random_state : initial seed, default=1\n",
    "        \n",
    "        printOvito : bool, default=False\n",
    "        \n",
    "        filtr : bool, default=False\n",
    "        if not None, data is filtered before calling train-test split\n",
    "        '''\n",
    "\n",
    "        \n",
    "        #--- get transition path bitmaps\n",
    "        self.DiscretizeTransitionPath()\n",
    "#        self.ndime                                  = 4 #--- hard-coded: (ux,uy,uz,E)\n",
    "        y = np.concatenate( list( map(lambda x:self.GetTargets(x), self.nruns ) ) )\n",
    "\n",
    "        \n",
    "        #--- filtr crystalline atoms\n",
    "        filtr = np.concatenate( list( map(lambda x:self.MaskCrystallineAtoms(x), self.nruns ) ) )\n",
    "\n",
    "        y     = y[ filtr ]\n",
    "        \n",
    "#        pdb.set_trace()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('dim(y)=',y.shape)\n",
    "\n",
    "        ndime  = y.shape[1] #--- dimension of the target vector\n",
    "        \n",
    "        #---------------\n",
    "        #--- zscore X\n",
    "        #---------------        \n",
    "        X      = np.c_[self.descriptors[filtr]]\n",
    "        X      = NeuralNetwork.Zscore( X, save_model = '%s/scaler_regression.sav'%self.best_model )\n",
    "        \n",
    "        \n",
    "        if X.shape[ 0 ] - self.n_train < 0 :  \n",
    "            X = NeuralNetwork.Duplicate(X, new_size = self.n_train )\n",
    "            y = NeuralNetwork.Duplicate(y, new_size = self.n_train )\n",
    "\n",
    "        #--- add noise\n",
    "        NeuralNetwork.AddGaussianNoise(X, scale = 0.1 )\n",
    "#        NeuralNetwork.AddGaussianNoise(y, scale = 0.1 )\n",
    "\n",
    "        #-----------------------\n",
    "        #--- train-test split\n",
    "        #-----------------------\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, #stratify=stratify,\n",
    "                                                            random_state=random_state)\n",
    "\n",
    "\n",
    "        #-----------------------\n",
    "        #--- train model\n",
    "        #-----------------------\n",
    "        if self.fully_connected: #--- dense nn\n",
    "            if self.implementation == 'sklearn':\n",
    "                #-----------------------\n",
    "                #--- parameter grid\n",
    "                #-----------------------\n",
    "                param_grid = {\n",
    "                                'hidden_layer_sizes':self.hidden_layer_sizes,\n",
    "                                 #'activation' : ['tanh', 'relu'],\n",
    "                                 'learning_rate_init':self.learning_rate_init,\n",
    "                                'alpha':self.alpha, #--- regularization \n",
    "                                 #'learning_rate' : ['invscaling', 'adaptive'],\n",
    "                                'n_iter_no_change':self.n_iter_no_change,\n",
    "                                'tol':self.tol,\n",
    "                                'max_iter':self.max_iter,\n",
    "                             } \n",
    "                mlp   =  MLPRegressor(random_state=random_state,verbose=self.verbose) #--- mlp regressor\n",
    "                regr  =  GridSearchCV(mlp, param_grid)\n",
    "                regr.fit(X_train,y_train)\n",
    "                model =  regr.best_estimator_\n",
    "                loss  =  model.loss_curve_\n",
    "                \n",
    "            elif self.implementation == 'keras': #--- dense nn in keras\n",
    "                model     = keras.Sequential([ #--- The network architecture\n",
    "                    layers.Dense(self.hidden_layer_size, activation=self.activation),\n",
    "#                    layers.Dense(self.hidden_layer_size, activation=self.activation),\n",
    "                    layers.Dense(ndime, activation='softmax')\n",
    "                    ])\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_init) #--- compilation step\n",
    "                model.compile( optimizer=optimizer,#\"rmsprop\",\n",
    "                               loss=\"mean_squared_error\",#\"sparse_categorical_crossentropy\",\n",
    "                               metrics=[\"mse\"]\n",
    "                             )\n",
    "                model.fit(X_train, y_train, #--- “Fitting”\n",
    "                          validation_data=(X_test, y_test),\n",
    "                          epochs=self.max_iter[0], verbose=self.verbose, batch_size=1)\n",
    "                loss      = model.history.history['loss']\n",
    "                val_loss  = model.history.history['val_loss']\n",
    "                \n",
    "        elif self.cnn: #--- convolutional\n",
    "            \n",
    "            (model, loss, val_loss), (X_train, X_test) =\\\n",
    "            self.ConvNetworkMultiLabelClassifier(X_train, y_train, X_test, y_test )\n",
    "            \n",
    "            NeuralNetwork.ValidationMultiLabelClassification(loss, val_loss, #--- validation\n",
    "                                 model, \n",
    "                                 X_train, X_test, y_train, y_test)\n",
    "            \n",
    "#             pdb.set_trace()\n",
    "#             (model, loss, val_loss), (X_train, X_test) =\\\n",
    "#             self.ConvNetwork(X_train, y_train, X_test, y_test )\n",
    "            \n",
    "            \n",
    "            \n",
    "            #--- validation\n",
    "#             NeuralNetwork.Validation(loss, val_loss, \n",
    "#                                      model, \n",
    "#                                      X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        #--- save in ovito file\n",
    "#         if printOvito:\n",
    "#             m = self.descriptors.shape[ 0 ]\n",
    "#             indices = np.arange( m )[ filtr ]\n",
    "        \n",
    "#             if indices.shape[ 0 ] - self.n_train < 0 :  \n",
    "#                 indices = NeuralNetwork.Duplicate(indices, new_size = self.n_train )\n",
    "            \n",
    "#             indices_train, indices_test, _, _ = train_test_split(indices, indices, #stratify=stratify,\n",
    "#                                                                 random_state=random_state)\n",
    "#             self.PrintOvito( indices_test, model )\n",
    "\n",
    "    @staticmethod\n",
    "    def Validation(loss, val_loss, model, X_train, X_test, y_train, y_test):\n",
    "        #-----------------------\n",
    "        #--- validation\n",
    "        #-----------------------\n",
    "        !mkdir png         #--- plot validation loss \n",
    "        ax = utl.PltErr(range(len(val_loss)), val_loss,\n",
    "                   attrs={'fmt':'-'}, Plot=False,\n",
    "                  )\n",
    "        utl.PltErr(range(len(loss)), loss,\n",
    "                   attrs={'fmt':'-'},\n",
    "                   ax=ax,\n",
    "                   yscale='log',xscale='log',\n",
    "#                   xlim=(1,self.max_iter[0]),\n",
    "                   xstr='epoch',ystr='loss',\n",
    "                   title='png/loss.png',\n",
    "                  )\n",
    "        \n",
    "        np.savetxt('png/loss.txt',np.c_[range(len(loss)),loss,val_loss],header='epoch loss val_loss')\n",
    "        \n",
    "        \n",
    "        #--- plot predictions\n",
    "        y_pred_test  = model.predict(X_test)        \n",
    "        y_pred_train = model.predict(X_train)        \n",
    "#         for idime, xstr in zip(range(3),'ux uy uz'.split()):\n",
    "#             ax = utl.PltErr(None,None,Plot=False)\n",
    "#             #\n",
    "#             utl.PltErr(y_test[:,idime],y_pred_test[:,idime],\n",
    "#                        attrs={'fmt':'x','color':'red','zorder':10,'markersize':6},\n",
    "#                        ax=ax,\n",
    "#                        Plot = False,\n",
    "\n",
    "#                       )\n",
    "#             #\n",
    "#             utl.PltErr(y_train[:,idime],y_pred_train[:,idime],\n",
    "#                        attrs={'fmt':'.','color':'blue','zorder':1,'markersize':6},\n",
    "#                        ax=ax,\n",
    "#                        Plot = False,\n",
    "\n",
    "#                       )\n",
    "#             #\n",
    "#             utl.PltErr(None,None,Plot=False,\n",
    "#                            title='png/scatter%s.png'%idime,\n",
    "#                             ax=ax,\n",
    "#                        xstr='%s actual'%xstr,ystr='%s predicted'%xstr,\n",
    "#                        xlim=(-2,2),ylim=(-2,2),\n",
    "#                            )\n",
    "\n",
    "        #--- energy\n",
    "        idime = 0 #3\n",
    "        xstr  = 'energy'\n",
    "        ax = utl.PltErr(None,None,Plot=False)\n",
    "        #\n",
    "        utl.PltErr(y_test[:,idime],y_pred_test[:,idime],\n",
    "                   attrs={'fmt':'x','color':'red','zorder':10,'markersize':6},\n",
    "                   ax=ax,\n",
    "                   Plot = False,\n",
    "\n",
    "                  )\n",
    "        utl.PltErr(y_train[:,idime],y_pred_train[:,idime],\n",
    "                   attrs={'fmt':'.','color':'blue','zorder':1,'markersize':6},\n",
    "                   ax=ax,\n",
    "                   Plot = False,\n",
    "\n",
    "                  )\n",
    "        #\n",
    "        utl.PltErr(None,None,Plot=False,\n",
    "                       title='png/scatter%s.png'%idime,\n",
    "                        ax=ax,\n",
    "                   xstr='%s actual'%xstr,ystr='%s predicted'%xstr,\n",
    "#                   xlim=(-2,2),ylim=(-2,2),\n",
    "                       )\n",
    "\n",
    "    @staticmethod\n",
    "    def ValidationMultiLabelClassification(loss, val_loss, model, X_train, X_test, y_train, y_test):\n",
    "        #-----------------------\n",
    "        #--- validation\n",
    "        #-----------------------\n",
    "        !mkdir png         #--- plot validation loss \n",
    "        ax = utl.PltErr(range(len(val_loss)), val_loss,\n",
    "                   attrs={'fmt':'-'}, Plot=False,\n",
    "                  )\n",
    "        utl.PltErr(range(len(loss)), loss,\n",
    "                   attrs={'fmt':'-'},\n",
    "                   ax=ax,\n",
    "                   yscale='log',xscale='log',\n",
    "#                   xlim=(1,self.max_iter[0]),\n",
    "                   xstr='epoch',ystr='loss',\n",
    "                   title='png/lossMultiLabelClassification.png',\n",
    "                  )\n",
    "        \n",
    "        np.savetxt('png/lossMultiLabelClassification.txt',np.c_[range(len(loss)),loss,val_loss],header='epoch loss val_loss')\n",
    "        \n",
    "        \n",
    "        #--- plot predictions\n",
    "        y_pred_test              = model.predict(X_test)        \n",
    "        y_pred_train             = model.predict(X_train)\n",
    "        \n",
    "        threshold                = 0.5 #--- hard-coded threshold\n",
    "        binary_predictions_test  = (y_pred_test > threshold).astype(int)\n",
    "        binary_predictions_train = (y_pred_train > threshold).astype(int)\n",
    "        binary_actual_test       = (y_test > threshold).astype(int)\n",
    "        binary_actual_train      = (y_train > threshold).astype(int)\n",
    "\n",
    "        \n",
    "        \n",
    "        #--- Compute the multilabel confusion matrix\n",
    "        conf_matrix = multilabel_confusion_matrix( binary_actual_test, binary_predictions_test )\n",
    "        ndime       = conf_matrix.shape[ 1 ] * conf_matrix.shape[ 2 ]\n",
    "        conf_matrix = conf_matrix.reshape((conf_matrix.shape[0],ndime))\n",
    "        np.savetxt('png/confusionMultiLabelClassification.txt',np.c_[conf_matrix])\n",
    "        \n",
    "        \n",
    "        #--- predict displacements\n",
    "        #--- reshape y_pred\n",
    "        \n",
    "            \n",
    "        \n",
    "#         disps_predictions_test = np.concatenate([list(map(lambda x: self.GetDispsFromBinaryMaps( x ) , binary_predictions_test ))])\n",
    "#         disps_actual_test      = np.concatenate([list(map(lambda x: self.GetDispsFromBinaryMaps( x ) , binary_actual_test ))])\n",
    "        \n",
    "# #         #--- plot predictions\n",
    "# #         y_pred_test  = model.predict(X_test)        \n",
    "# #         y_pred_train = model.predict(X_train)        \n",
    "#         for idime, xstr in zip(range(3),'ux uy uz'.split()):\n",
    "#             ax = utl.PltErr(None,None,Plot=False)\n",
    "#             #\n",
    "#             utl.PltErr(disps_actual_test[:,idime],disps_predictions_test[:,idime],\n",
    "#                        attrs={'fmt':'x','color':'red','zorder':10,'markersize':6},\n",
    "#                        ax=ax,\n",
    "#                        Plot = False,\n",
    "\n",
    "#                       )\n",
    "#             #\n",
    "# #             utl.PltErr(y_train[:,idime],y_pred_train[:,idime],\n",
    "# #                        attrs={'fmt':'.','color':'blue','zorder':1,'markersize':6},\n",
    "# #                        ax=ax,\n",
    "# #                        Plot = False,\n",
    "\n",
    "# #                       )\n",
    "#             #\n",
    "#             utl.PltErr(None,None,Plot=False,\n",
    "#                            title='png/scatter%s.png'%idime,\n",
    "#                             ax=ax,\n",
    "#                        xstr='%s actual'%xstr,ystr='%s predicted'%xstr,\n",
    "#                        xlim=(-3,3),ylim=(-3,3),\n",
    "#                            )\n",
    "\n",
    "    def GetDispsFromBinaryMaps( self, binaryMap ):\n",
    "        binaryMapReshaped = binaryMap.reshape((self.nbinx, self.nbiny, self.nbinz ))\n",
    "        filtr = binaryMapReshaped == 1\n",
    "        return np.c_[self.yv[filtr],self.xv[filtr],self.zv[filtr]]\n",
    "\n",
    "        \n",
    "    def PrintOvito( self, filtr, model ):\n",
    "        #--- save in ovito\n",
    "        X          = np.c_[self.descriptors[filtr]]\n",
    "        X          = NeuralNetwork.Zscore( X )\n",
    "        X_reshaped =  X.reshape((X.shape[0],self.shape[0],self.shape[1],self.shape[2],1))\n",
    "        y_pred     = model.predict( X_reshaped )\n",
    "        with open('original.xyz','w') as fp:\n",
    "            utl.PrintOvito(self.perAtomData.iloc[filtr], fp, '0', attr_list='id type x y z ux uy uz'.split())\n",
    "        with open('test.xyz','w') as fp:\n",
    "            xyz = self.perAtomData.iloc[filtr]['id type x y z'.split()]\n",
    "            cordc = pd.DataFrame(np.c_[xyz,y_pred[:,:3]],columns='id type x y z ux uy uz'.split())\n",
    "            utl.PrintOvito(cordc, fp, '0', attr_list='id type x y z ux uy uz'.split())\n",
    "                \n",
    "\n",
    "    def ConvNetworkMixedInput(self,X_train, y_train, X_test, y_test):\n",
    "        '''\n",
    "        Convolutional neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : array-like training x input\n",
    "        \n",
    "        y_train : array-like, training y input\n",
    "        \n",
    "        X_test : array-like test x input\n",
    "        \n",
    "        y_test : array-like, training y input\n",
    "\n",
    "        Return\n",
    "        ---------- ( , loss,  )\n",
    "        best_model : cnn object, best trained model based on on the validation loss\n",
    "        \n",
    "        loss : array-like, mse loss\n",
    "\n",
    "        val_loss : array-like, validation loss\n",
    "\n",
    "        '''\n",
    "#         tf.random.set_random_seed(812)\n",
    "\n",
    "        shape         =  (self.shape[0],self.shape[1],self.shape[2],1) #--- rows, cols, thickness, channels: pixel map\n",
    "        shape_vector_input = 3\n",
    "        \n",
    "        kernel_size   =  self.kernel_size \n",
    "        epochs        =  self.max_iter[0]\n",
    "        activation    =  self.activation\n",
    "        padding       = 'same'\n",
    "        filters       =  self.n_channels\n",
    "        learning_rate = self.learning_rate_init[0]\n",
    "        #\n",
    "        ndime         =  y_train.shape[1]\n",
    "        n_train       =  X_train.shape[0]\n",
    "        n_test        =  X_test.shape[0]\n",
    "        assert        shape[0] * shape[1] * shape[2] == X_train.shape[ 1 ] - shape_vector_input\n",
    "        pixel_map_input        =  keras.Input(shape=shape)\n",
    "        vector_input           =  keras.Input(shape=(shape_vector_input,))\n",
    "        #\n",
    "\n",
    "        #------------------------------\n",
    "        #--- The network architecture\n",
    "        #------------------------------\n",
    "        x             =  layers.Conv3D(   filters     =  filters, \n",
    "                                          kernel_size =  kernel_size,\n",
    "                                          activation  =  activation,\n",
    "                                          padding     =  padding\n",
    "                                       )(pixel_map_input)\n",
    "        filters       *=  2\n",
    "        for i in range( self.number_hidden_layers ):\n",
    "            x       = layers.AveragePooling3D( pool_size = 2 )( x )\n",
    "            x       = layers.Conv3D( filters       =  filters, \n",
    "                                     kernel_size   =  kernel_size,\n",
    "                                     activation    =  activation,\n",
    "                                     padding       =  padding\n",
    "                                     )(x)\n",
    "            filters *= 2\n",
    "        x       = layers.Flatten()(x)\n",
    "            \n",
    "        #--- concatenate flattened map with vector\n",
    "        combined = keras.layers.concatenate( [ x, vector_input ] )\n",
    "        \n",
    "        #--- output layer\n",
    "        outputs = layers.Dense( ndime )( combined ) #, activation=activation)(x)\n",
    "        model   = keras.Model(inputs=[pixel_map_input,vector_input], outputs=outputs)\n",
    "        if self.verbose:\n",
    "            print('cnn model summary:',model.summary())\n",
    "\n",
    "        #--- The compilation step\n",
    "        optimizer = tf.keras.optimizers.Adam( learning_rate = learning_rate )\n",
    "        model.compile( optimizer =  optimizer,\n",
    "                       loss      =  \"mean_squared_error\",\n",
    "                       metrics   =  [\"mse\"]\n",
    "                     )\n",
    "\n",
    "        model.summary()\n",
    "        #--- save best model\n",
    "#         callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnet_from_scratch.tf',  \n",
    "#                                                    monitor=\"loss\",\n",
    "#                                                   save_freq=10,\n",
    "#                                                     save_best_only=True)]\n",
    "\n",
    "        #--- “Fitting” the model X_train_transfrmd, y_train\n",
    "        nn = X_train.shape[ 1 ]\n",
    "        X_train_pixels = X_train[:,0:nn-shape_vector_input]\n",
    "        X_test_pixels  = X_test[:,0:nn-shape_vector_input]\n",
    "        assert    X_train_pixels.shape[ 1 ] ==    shape[0] * shape[1] * shape[2]\n",
    "        X_train_vector = X_train[:,nn-shape_vector_input:nn]\n",
    "        X_test_vector  = X_test[:,nn-shape_vector_input:nn]\n",
    "        assert    X_train_vector.shape[ 1 ] ==   shape_vector_input\n",
    "        \n",
    "    \n",
    "        X_train_pixels =  X_train_pixels.reshape((n_train,shape[0],shape[1],shape[2],1))\n",
    "        X_test_pixels  =  X_test_pixels.reshape((n_test,shape[0],shape[1],shape[2],1))\n",
    "        \n",
    "        model.fit( [X_train_pixels,X_train_vector], y_train, \n",
    "                   validation_data      = ( [X_test_pixels,X_test_vector], y_test ),\n",
    "#                    callbacks            = callbacks,\n",
    "                    epochs              = epochs, \n",
    "                    verbose             = self.verbose, \n",
    "                    shuffle             = False, \n",
    "#                    batch_size          = 1,\n",
    "#                     batch_size     = 128,\n",
    "#                     use_multiprocessing = True,\n",
    "#                     workers             = 4,\n",
    "                 )\n",
    "\n",
    "        #--- validation loss\n",
    "        model.save('best_model/convnetRegressorMixedInput_from_scratch.tf')\n",
    "        loss       = model.history.history['loss']\n",
    "        val_loss   = model.history.history['val_loss']\n",
    "        best_model =model #keras.models.load_model(\"best_model/convnet_from_scratch.tf\")\n",
    "\n",
    "        \n",
    "        return ( best_model, loss, val_loss ), ([X_train_pixels,X_train_vector], [X_test_pixels,X_test_vector])\n",
    "    \n",
    "    def ConvNetwork(self,X_train, y_train, X_test, y_test):\n",
    "        '''\n",
    "        Convolutional neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : array-like training x input\n",
    "        \n",
    "        y_train : array-like, training y input\n",
    "        \n",
    "        X_test : array-like test x input\n",
    "        \n",
    "        y_test : array-like, training y input\n",
    "\n",
    "        Return\n",
    "        ---------- ( , loss,  )\n",
    "        best_model : cnn object, best trained model based on on the validation loss\n",
    "        \n",
    "        loss : array-like, mse loss\n",
    "\n",
    "        val_loss : array-like, validation loss\n",
    "\n",
    "        '''\n",
    "#         tf.random.set_random_seed(812)\n",
    "\n",
    "        shape         =  (self.shape[0],self.shape[1],self.shape[2],1) #--- rows, cols, thickness, channels\n",
    "        kernel_size   =  self.kernel_size \n",
    "        epochs        =  self.max_iter[0]\n",
    "        activation    =  self.activation\n",
    "        padding       = 'same'\n",
    "        filters       =  self.n_channels\n",
    "        learning_rate = self.learning_rate_init[0]\n",
    "        #\n",
    "        ndime         =  y_train.shape[1]\n",
    "        n_train       =  X_train.shape[0]\n",
    "        n_test        =  X_test.shape[0]\n",
    "        assert        shape[0] * shape[1] * shape[2] == X_train.shape[ 1 ]\n",
    "        inputs        =  keras.Input(shape=shape)\n",
    "        #\n",
    "\n",
    "        #------------------------------\n",
    "        #--- The network architecture\n",
    "        #------------------------------\n",
    "        x             =  layers.Conv3D(   filters     =  filters, \n",
    "                                          kernel_size =  kernel_size,\n",
    "                                          activation  =  activation,\n",
    "                                          padding     =  padding\n",
    "                                       )(inputs)\n",
    "        filters       *=  2\n",
    "        for i in range( self.number_hidden_layers ):\n",
    "            x       = layers.AveragePooling3D( pool_size = 2 )( x )\n",
    "            x       = layers.Conv3D( filters       =  filters, \n",
    "                                     kernel_size   =  kernel_size,\n",
    "                                     activation    =  activation,\n",
    "                                     padding       =  padding\n",
    "                                     )(x)\n",
    "            filters *= 2\n",
    "            \n",
    "        #--- output layer\n",
    "        x       = layers.Flatten()(x)\n",
    "        outputs = layers.Dense( ndime )( x ) #, activation=activation)(x)\n",
    "        model   = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        if self.verbose:\n",
    "            print('cnn model summary:',model.summary())\n",
    "\n",
    "        #--- The compilation step\n",
    "        optimizer = tf.keras.optimizers.Adam( learning_rate = learning_rate )\n",
    "        model.compile( optimizer =  optimizer,\n",
    "                       loss      =  \"mean_squared_error\",\n",
    "                       metrics   =  [\"mse\"]\n",
    "                     )\n",
    "\n",
    "        #--- save best model\n",
    "#         callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnet_from_scratch.tf',  \n",
    "#                                                    monitor=\"loss\",\n",
    "#                                                   save_freq=10,\n",
    "#                                                     save_best_only=True)]\n",
    "\n",
    "        #--- “Fitting” the model X_train_transfrmd, y_train\n",
    "        X_train_reshaped =  X_train.reshape((n_train,shape[0],shape[1],shape[2],1))\n",
    "        X_test_reshaped  =  X_test.reshape((n_test,shape[0],shape[1],shape[2],1))\n",
    "        model.fit( X_train_reshaped, y_train, \n",
    "                   validation_data      = ( X_test_reshaped, y_test ),\n",
    "#                    callbacks            = callbacks,\n",
    "                    epochs              = epochs, \n",
    "                    verbose             = self.verbose, \n",
    "                    shuffle             = False, \n",
    "#                    batch_size          = 1,\n",
    "#                     batch_size     = 128,\n",
    "#                     use_multiprocessing = True,\n",
    "#                     workers             = 4,\n",
    "                 )\n",
    "\n",
    "        #--- validation loss\n",
    "        model.save('best_model/convnetRegressor_from_scratch.tf')\n",
    "        loss       = model.history.history['loss']\n",
    "        val_loss   = model.history.history['val_loss']\n",
    "        best_model =model #keras.models.load_model(\"best_model/convnet_from_scratch.tf\")\n",
    "\n",
    "        \n",
    "        return ( best_model, loss, val_loss ), (X_train_reshaped, X_test_reshaped)\n",
    "    \n",
    "    def ConvNetworkMultiLabelClassifier(self,X_train, y_train, X_test, y_test):\n",
    "        '''\n",
    "        Convolutional neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : array-like training x input\n",
    "        \n",
    "        y_train : array-like, training y input\n",
    "        \n",
    "        X_test : array-like test x input\n",
    "        \n",
    "        y_test : array-like, training y input\n",
    "\n",
    "        Return\n",
    "        ---------- ( , loss,  )\n",
    "        best_model : cnn object, best trained model based on on the validation loss\n",
    "        \n",
    "        loss : array-like, mse loss\n",
    "\n",
    "        val_loss : array-like, validation loss\n",
    "\n",
    "        '''\n",
    "#         tf.random.set_random_seed(812)\n",
    "\n",
    "        shape         =  (self.shape[0],self.shape[1],self.shape[2],1) #--- rows, cols, thickness, channels\n",
    "        kernel_size   =  self.kernel_size \n",
    "        epochs        =  self.max_iter[0]\n",
    "        activation    =  'relu' #self.activation\n",
    "        padding       = 'same'\n",
    "        filters       =  self.n_channels\n",
    "        learning_rate = self.learning_rate_init[0]\n",
    "        #\n",
    "        ndime         =  y_train.shape[1]\n",
    "        n_train       =  X_train.shape[0]\n",
    "        n_test        =  X_test.shape[0]\n",
    "        assert        shape[0] * shape[1] * shape[2] == X_train.shape[ 1 ]\n",
    "        inputs        =  keras.Input(shape=shape)\n",
    "        #\n",
    "\n",
    "        #------------------------------\n",
    "        #--- The network architecture\n",
    "        #------------------------------\n",
    "        x             =  layers.Conv3D(   filters     =  filters, \n",
    "                                          kernel_size =  kernel_size,\n",
    "                                          activation  =  activation,\n",
    "                                          padding     =  padding\n",
    "                                       )(inputs)\n",
    "        filters       *=  2\n",
    "        for i in range( self.number_hidden_layers ):\n",
    "            x       = layers.AveragePooling3D( pool_size = 2 )( x )\n",
    "            x       = layers.Conv3D( filters       =  filters, \n",
    "                                     kernel_size   =  kernel_size,\n",
    "                                     activation    =  activation,\n",
    "                                     padding       =  padding\n",
    "                                     )(x)\n",
    "            filters *= 2\n",
    "            \n",
    "        #--- output layer\n",
    "        x       = layers.Flatten()(x)\n",
    "        outputs = layers.Dense( ndime, activation='sigmoid' )( x ) #, activation=activation)(x)\n",
    "        model   = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        if self.verbose:\n",
    "            print('cnn model summary:',model.summary())\n",
    "\n",
    "        #--- The compilation step\n",
    "        optimizer = tf.keras.optimizers.Adam( learning_rate = learning_rate )\n",
    "        model.compile( optimizer =  optimizer,\n",
    "                       loss      =  \"binary_crossentropy\",\n",
    "                       metrics   =  [\"mse\"]\n",
    "                     )\n",
    "\n",
    "        #--- save best model\n",
    "#         callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnet_from_scratch.tf',  \n",
    "#                                                    monitor=\"loss\",\n",
    "#                                                   save_freq=10,\n",
    "#                                                     save_best_only=True)]\n",
    "\n",
    "        #--- “Fitting” the model X_train_transfrmd, y_train\n",
    "        X_train_reshaped =  X_train.reshape((n_train,shape[0],shape[1],shape[2],1))\n",
    "        X_test_reshaped  =  X_test.reshape((n_test,shape[0],shape[1],shape[2],1))\n",
    "        model.fit( X_train_reshaped, y_train, \n",
    "                   validation_data      = ( X_test_reshaped, y_test ),\n",
    "#                    callbacks            = callbacks,\n",
    "                    epochs              = epochs, \n",
    "                    verbose             = self.verbose, \n",
    "                    shuffle             = False, \n",
    "#                    batch_size          = 1,\n",
    "#                     batch_size     = 128,\n",
    "#                     use_multiprocessing = True,\n",
    "#                     workers             = 4,\n",
    "                 )\n",
    "\n",
    "        #--- validation loss\n",
    "        model.save('best_model/convnetMultiLabelClassifier_from_scratch.tf')\n",
    "        loss       = model.history.history['loss']\n",
    "        val_loss   = model.history.history['val_loss']\n",
    "        best_model =model #keras.models.load_model(\"best_model/convnet_from_scratch.tf\")\n",
    "\n",
    "        \n",
    "        return ( best_model, loss, val_loss ), (X_train_reshaped, X_test_reshaped)\n",
    "    \n",
    "    def ConvNetworkClassifier(self,y,\n",
    "                               random_state=1\n",
    "                               ):\n",
    "        '''\n",
    "        Convolutional neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : array-like training x input\n",
    "        \n",
    "        y_train : array-like, training y input\n",
    "        \n",
    "        X_test : array-like test x input\n",
    "        \n",
    "        y_test : array-like, training y input\n",
    "\n",
    "        Return\n",
    "        ---------- ( , loss,  )\n",
    "        best_model : cnn object, best trained model based on on the validation loss\n",
    "        \n",
    "        loss : array-like, mse loss\n",
    "\n",
    "        val_loss : array-like, validation loss\n",
    "\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('dim(y)=',y.shape)\n",
    "        \n",
    "\n",
    "        #---------------\n",
    "        #--- zscore X\n",
    "        #---------------        \n",
    "        X      = np.c_[self.descriptors ]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "        X      = scaler.transform( X )\n",
    "    \n",
    "        if self.verbose:\n",
    "            print('X.shape:=',X.shape)\n",
    "            \n",
    "            \n",
    "            \n",
    "        #-----------------------\n",
    "        #--- train-test split\n",
    "        #-----------------------\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                            random_state=random_state)\n",
    "\n",
    "        \n",
    "        \n",
    "        #---- set model parameters\n",
    "        shape         =  (self.shape[0],self.shape[1],self.shape[2],1) #--- rows, cols, thickness, channels\n",
    "        kernel_size   =  self.kernel_size \n",
    "        epochs        =  self.max_iter[0]\n",
    "        activation    =  self.activation\n",
    "        padding       = 'same'\n",
    "        filters       =  self.n_channels\n",
    "        learning_rate = self.learning_rate_init[0]\n",
    "        #\n",
    "        ndime         =  y_train.shape[1]\n",
    "        n_train       =  X_train.shape[0]\n",
    "        n_test        =  X_test.shape[0]\n",
    "        assert        shape[0] * shape[1] * shape[2] == X_train.shape[ 1 ]\n",
    "        inputs        =  keras.Input(shape=shape)\n",
    "\n",
    "        #------------------------------\n",
    "        #--- The network architecture\n",
    "        #------------------------------\n",
    "        x             =  layers.Conv3D(   filters     =  filters, \n",
    "                                          kernel_size =  kernel_size,\n",
    "                                          activation  =  activation,\n",
    "                                          padding     =  padding\n",
    "                                       )(inputs)\n",
    "        filters       *=  2\n",
    "        for i in range( self.number_hidden_layers ):\n",
    "            x       = layers.AveragePooling3D( pool_size = 2 )( x )\n",
    "            x       = layers.Conv3D( filters       =  filters, \n",
    "                                     kernel_size   =  kernel_size,\n",
    "                                     activation    =  activation,\n",
    "                                     padding       =  padding\n",
    "                                     )(x)\n",
    "            filters *= 2\n",
    "            \n",
    "        #--- output layer\n",
    "        x       = layers.Flatten()(x)\n",
    "        outputs = layers.Dense( ndime, activation=activation)(x)\n",
    "        model   = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        if self.verbose:\n",
    "            print('cnn model summary:',model.summary())\n",
    "\n",
    "        #--- The compilation step\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate) #--- compilation step\n",
    "        model.compile( optimizer =  optimizer,\n",
    "                       loss=[\"binary_crossentropy\",\"sparse_categorical_crossentropy\"][1],\n",
    "                       metrics   =  [\"mse\"]\n",
    "                     )\n",
    "\n",
    "        #--- save best model\n",
    "        !mkdir best_model\n",
    "#         callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnetClassifier_from_scratch.tf',  \n",
    "#                                                     monitor=\"accuracy\",\n",
    "#                                                     save_freq=10,\n",
    "#                                                     save_best_only=True)]\n",
    "\n",
    "        #--- “Fitting” the model X_train_transfrmd, y_train\n",
    "        X_train_reshaped =  X_train.reshape((n_train,shape[0],shape[1],shape[2],1))\n",
    "        X_test_reshaped  =  X_test.reshape((n_test,shape[0],shape[1],shape[2],1))\n",
    "        model.fit( X_train_reshaped, y_train, \n",
    "                   validation_data      = ( X_test_reshaped, y_test ),\n",
    "                   callbacks            = callbacks,\n",
    "                    epochs              = epochs, \n",
    "                    verbose             = self.verbose, \n",
    "                    shuffle             = False, \n",
    "#                     batch_size     = 128,\n",
    "#                     use_multiprocessing = True,\n",
    "#                     workers             = 4,\n",
    "                 )\n",
    "\n",
    "        #--- validation loss\n",
    "        model.save('best_model/convnetClassifier_from_scratch.tf')\n",
    "        loss       = model.history.history['loss']\n",
    "        val_loss   = model.history.history['val_loss']\n",
    "        best_model = model #keras.models.load_model(\"best_model/convnetClassifier_from_scratch.tf\")\n",
    "\n",
    "        \n",
    "        return ( best_model, loss, val_loss ), (X_train_reshaped, X_test_reshaped)\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def Duplicate(X, new_size = 100 ):\n",
    "        m = m0 = X.shape[ 0 ]\n",
    "#        n = X.shape[ 1 ]\n",
    "        augmented_x = np.copy( X )\n",
    "\n",
    "        while m <= new_size:\n",
    "            augmented_x = np.concatenate([augmented_x,X],axis = 0)\n",
    "            #\n",
    "            m = augmented_x.shape[ 0 ]\n",
    "\n",
    "        assert m > new_size\n",
    "\n",
    "        return augmented_x[:new_size]\n",
    "\n",
    "    @staticmethod    \n",
    "    def AddGaussianNoise(X,scale = 0.1):\n",
    "\n",
    "        epsilon_x = np.random.normal(scale=scale,size=X.size).reshape(X.shape)\n",
    "        X += epsilon_x\n",
    "        \n",
    "    \n",
    "    def PrintDensityMap(self, atomIndx, irun, fout):\n",
    "        with open(fout,'w') as fp:\n",
    "                    disp           = np.c_[self.perAtomData[ irun ].iloc[atomIndx]['ux uy uz'.split()]].flatten()\n",
    "                    df             = pd.DataFrame(np.c_[self.Positions[ irun ].T,self.Descriptors[ irun ][atomIndx]],\n",
    "                                                  columns='x y z mass'.split())\n",
    "                    utl.PrintOvito(df, fp, 'disp = %s'%disp, attr_list='x y z mass'.split())\n",
    "                    \n",
    "    @staticmethod\n",
    "    def Zscore( X, **kwargs ):\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "        \n",
    "        if 'save_model' in kwargs:\n",
    "            pickle.dump( scaler, open( kwargs[ 'save_model' ], 'wb' ) )\n",
    "        return scaler.transform( X )\n",
    "\n",
    "#     def SaveConf(self,fout):\n",
    "#         with open(fout,'w') as fp:\n",
    "#             np.savetxt(fp,np.c_[self.perAtomData],header=' '.join(list(self.perAtomData.keys())))\n",
    "\n",
    "#     def Test(self,y,\n",
    "#                                    random_state=1\n",
    "#                                    ):\n",
    "#             '''\n",
    "#             Convolutional neural network.\n",
    "\n",
    "#             Parameters\n",
    "#             ----------\n",
    "#             X_train : array-like training x input\n",
    "\n",
    "#             y_train : array-like, training y input\n",
    "\n",
    "#             X_test : array-like test x input\n",
    "\n",
    "#             y_test : array-like, training y input\n",
    "\n",
    "#             Return\n",
    "#             ---------- ( , loss,  )\n",
    "#             best_model : cnn object, best trained model based on on the validation loss\n",
    "\n",
    "#             loss : array-like, mse loss\n",
    "\n",
    "#             val_loss : array-like, validation loss\n",
    "\n",
    "#             '''\n",
    "\n",
    "#             if self.verbose:\n",
    "#                 print('dim(y)=',y.shape)\n",
    "\n",
    "#             ndime  = y.shape[1] #--- dimension of the target vector\n",
    "\n",
    "\n",
    "#             #---------------\n",
    "#             #--- zscore X\n",
    "#             #---------------        \n",
    "#             X      = np.c_[self.descriptors ]\n",
    "#             scaler = StandardScaler()\n",
    "#             scaler.fit(X)\n",
    "#             X      = scaler.transform( X )\n",
    "\n",
    "#             if self.verbose:\n",
    "#                 print('X.shape:=',X.shape)\n",
    "\n",
    "\n",
    "\n",
    "#             #-----------------------\n",
    "#             #--- train-test split\n",
    "#             #-----------------------\n",
    "#             X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "#                                                                 random_state=random_state)\n",
    "\n",
    "\n",
    "\n",
    "#             #---- set model parameters\n",
    "#             shape         =  (self.shape[0],self.shape[1],self.shape[2],1) #--- rows, cols, thickness, channels\n",
    "#             kernel_size   =  self.kernel_size \n",
    "#             epochs        =  self.max_iter[0]\n",
    "#             activation    =  self.activation\n",
    "#             padding       = 'same'\n",
    "#             filters       =  self.n_channels\n",
    "#             learning_rate = self.learning_rate_init[0]\n",
    "#             #\n",
    "#             ndime         =  y_train.shape[1]\n",
    "#             n_train       =  X_train.shape[0]\n",
    "#             n_test        =  X_test.shape[0]\n",
    "#             assert        shape[0] * shape[1] * shape[2] == X_train.shape[ 1 ]\n",
    "#             inputs        =  keras.Input(shape=shape)\n",
    "#             #\n",
    "#     #         pdb.set_trace()\n",
    "#             #------------------------------\n",
    "#             #--- The network architecture\n",
    "#             #------------------------------\n",
    "#             model     = keras.Sequential([\n",
    "#                 layers.Dense(self.hidden_layer_size, activation=\"relu\"),\n",
    "#     #             layers.Dense(self.hidden_layer_size), #activation=\"relu\"),\n",
    "#                 layers.Dense(2, activation=\"softmax\")\n",
    "#                 ])\n",
    "#             optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate) #--- compilation step\n",
    "\n",
    "#             model.compile( optimizer=\"rmsprop\",\n",
    "#                            loss=\"sparse_categorical_crossentropy\",\n",
    "#                            metrics=[\"mse\"])\n",
    "\n",
    "\n",
    "#             #--- “Fitting” the model X_train_transfrmd, y_train\n",
    "#             X_train_reshaped =  X_train \n",
    "#             X_test_reshaped  =  X_test\n",
    "#             model.fit( X_train_reshaped, y_train, \n",
    "#                        validation_data      = ( X_test_reshaped, y_test ),\n",
    "#     #                     callbacks=callbacks,\n",
    "#                         epochs              = epochs, \n",
    "#                         verbose             = self.verbose, \n",
    "#                         shuffle             = False, \n",
    "#     #                     batch_size     = 128,\n",
    "#                         use_multiprocessing = True,\n",
    "#                         workers             = 4,\n",
    "#                      )        \n",
    "\n",
    "\n",
    "#             #--- save best model\n",
    "#             !mkdir best_model\n",
    "#     #         callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnetClassifier_from_scratch.tf',  \n",
    "#     #                                                    monitor=\"val_loss\",\n",
    "#     #                                                   save_freq=10,\n",
    "#     #                                                     save_best_only=True)]\n",
    "\n",
    "\n",
    "#             #--- validation loss\n",
    "#             loss       = model.history.history['loss']\n",
    "#             val_loss   = model.history.history['val_loss']\n",
    "#             best_model = model #keras.models.load_model(\"best_model/convnet_from_scratch.tf\")\n",
    "\n",
    "\n",
    "#             !mkdir png\n",
    "#             utl.PltErr(range(len(loss)), loss,\n",
    "#                        yscale='log',\n",
    "#                        xstr='epoch',ystr='loss',\n",
    "#                        title='png/loss_classification.png',\n",
    "#                       )\n",
    "\n",
    "#     #         pdb.set_trace()\n",
    "#             #--- confusion matrix\n",
    "#             cm = confusion_matrix(y_test, model.predict_classes(X_test),\n",
    "#                              labels=[0, 1]\n",
    "#                             )\n",
    "#             print('cm=',cm)\n",
    "#             np.savetxt('png/confusion.txt',np.c_[cm])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2c303869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.Catalogs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f8fc70",
   "metadata": {},
   "source": [
    "## main(): classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dfe6a63d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " def main():\n",
    " \n",
    "    if not eval(confParser['neural net classification']['classification']):\n",
    "        return\n",
    "    \n",
    "    nn = NeuralNetwork(\n",
    "                        hidden_layer_sizes   = eval(confParser['neural net classification']['hidden_layer_sizes']),\n",
    "                        learning_rate_init   = eval(confParser['neural net classification']['learning_rate_init']),\n",
    "                        n_iter_no_change     = eval(confParser['neural net classification']['n_iter_no_change']),\n",
    "                        tol                  = eval(confParser['neural net classification']['tol']),\n",
    "                        max_iter             = eval(confParser['neural net classification']['max_iter']),\n",
    "                        alpha                = eval(confParser['neural net classification']['alpha']),\n",
    "                        hidden_layer_size    = eval(confParser['neural net classification']['hidden_layer_size']),\n",
    "                        fully_connected      = eval(confParser['neural net classification']['fully_connected']),\n",
    "                        implementation       = eval(confParser['neural net classification']['implementation']),\n",
    "                        cnn                  = eval(confParser['neural net classification']['cnn']),\n",
    "                        n_channels           = eval(confParser['neural net classification']['n_channels']),\n",
    "                        kernel_size          = eval(confParser['neural net classification']['kernel_size']),\n",
    "                        activation           = eval(confParser['neural net classification']['activation']),\n",
    "                        number_hidden_layers = eval(confParser['neural net classification']['number_hidden_layers']),\n",
    "                        n_train              = eval(confParser['neural net classification']['n_train']),\n",
    "                        best_model           = 'best_model',\n",
    "                        verbose              = True \n",
    "                    )\n",
    "    \n",
    "    nn.Parse2nd( path  = confParser['neural net']['input_path'],\n",
    "              nruns = eval(confParser['neural net classification']['nruns']))\n",
    "\n",
    "    nn.Combine2nd() #--- concat. descriptors\n",
    "\n",
    "    #--- classifier\n",
    "    nn.TrainClassifier()\n",
    "#        nn.Test(np.c_[nn.perAtomData.defect_label].astype(int))\n",
    "    \n",
    "    \n",
    "    return nn\n",
    "\n",
    "#model_clf = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0db348",
   "metadata": {},
   "source": [
    "## main(): regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e01971ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " def main():\n",
    " \n",
    "    if not eval(confParser['neural net regression']['regression']):\n",
    "        return\n",
    "\n",
    "    nn = NeuralNetwork(\n",
    "                        hidden_layer_sizes   = eval(confParser['neural net regression']['hidden_layer_sizes']),\n",
    "                        learning_rate_init   = eval(confParser['neural net regression']['learning_rate_init']),\n",
    "                        n_iter_no_change     = eval(confParser['neural net regression']['n_iter_no_change']),\n",
    "                        tol                  = eval(confParser['neural net regression']['tol']),\n",
    "                        max_iter             = eval(confParser['neural net regression']['max_iter']),\n",
    "                        alpha                = eval(confParser['neural net regression']['alpha']),\n",
    "                        hidden_layer_size    = eval(confParser['neural net regression']['hidden_layer_size']),\n",
    "                        fully_connected      = eval(confParser['neural net regression']['fully_connected']),\n",
    "                        implementation       = eval(confParser['neural net regression']['implementation']),\n",
    "                        cnn                  = eval(confParser['neural net regression']['cnn']),\n",
    "                        n_channels           = eval(confParser['neural net regression']['n_channels']),\n",
    "                        kernel_size          = eval(confParser['neural net regression']['kernel_size']),\n",
    "                        activation           = eval(confParser['neural net regression']['activation']),\n",
    "                        number_hidden_layers = eval(confParser['neural net regression']['number_hidden_layers']),\n",
    "                        n_train              = eval(confParser['neural net regression']['n_train']),\n",
    "                        du                   = eval(confParser['neural net regression']['du']),\n",
    "                        umax                 = eval(confParser['neural net regression']['umax']),\n",
    "                        best_model           = 'best_model',\n",
    "                        verbose              = True \n",
    "                    )\n",
    "    \n",
    "    nn.Parse2nd( path  = confParser['neural net']['input_path'],\n",
    "              nruns = eval(confParser['neural net regression']['nruns']))\n",
    "\n",
    "    nn.Combine2nd() #--- concat. descriptors\n",
    "\n",
    "    nn.TrainRegressorTransitionPaths()\n",
    "    nn.TrainRegressorBarriers()\n",
    "    \n",
    "    return nn\n",
    "\n",
    "#model_regr = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864aefa4",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6f1a5a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if eval(confParser['flags']['RemoteMachine']):\n",
    "        return\n",
    "    \n",
    "\n",
    "    \n",
    "    #--- ann\n",
    "    number_hidden_layers  = dict(zip(range(4),[1]))\n",
    "    hidden_layer_size     = dict(zip(range(4),[1]))\n",
    "    n_channels            = dict(zip(range(4),[1]))\n",
    "    activations           = dict(zip(range(20),['relu']))\n",
    "#     string[ inums ] = \"\\t\\'5\\':\\'neuralNet/20x20/ann/classifier/layer%s/channel%s/activation%s/layer_size%s\\',\\n\" % (key_n,key_c,key_a,key_h) #--- change job name\n",
    "    \n",
    "    #--- cnn\n",
    "#     number_hidden_layers  = dict(zip(range(4),[1,2,3]))\n",
    "#     hidden_layer_size     = dict(zip(range(4),[1]))\n",
    "#     n_channels            = dict(zip(range(4),[8,16,32,64]))\n",
    "#     activations           = dict(zip(range(20),['linear']))\n",
    "\n",
    "    runs = range(8)\n",
    "    \n",
    "    legend = utl.Legends()\n",
    "    legend.Set(fontsize=14,bbox_to_anchor=(1.5, 0.3, 0.5, 0.5))\n",
    "    symbols = utl.Symbols()\n",
    "    \n",
    "    nphi = len(number_hidden_layers)\n",
    "    #---\n",
    "    count = 0\n",
    "    ax = utl.PltErr(None, None, Plot=False )\n",
    "    for key_n in number_hidden_layers:\n",
    "        number_hidden_layer = number_hidden_layers[key_n]\n",
    "#         if number_hidden_layer != 2:\n",
    "#             continue\n",
    "        for key_c in n_channels:\n",
    "            n_channel = n_channels[key_c]\n",
    "#             if n_channel != 16:\n",
    "#                 continue\n",
    "            for key_a in activations:\n",
    "                activation = activations[key_a]\n",
    "                for key_h in hidden_layer_size:\n",
    "                    nsize = hidden_layer_size[key_h]\n",
    "\n",
    "        #---\t\n",
    "#                    path = 'neuralNet/20x20/cnn/classifier/layer%s/channel%s/activation%s/layer_size%s'%(key_n,key_c,key_a,key_h) #--- change job name\n",
    "                    path = 'neuralNet/ni/interestitials/test2nd' #--- change job name\n",
    "                    fp = ['confusion.txt', 'val_loss_classification.txt','loss.txt'][ 2 ]\n",
    "                    for irun in runs:\n",
    "                        try:\n",
    "                            data = np.loadtxt('%s/Run%s/png/%s'%(path,irun,fp))\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                        if fp == 'confusion.txt':\n",
    "                            accuracy_crystals = data[0,0]/np.sum(data[0,:])\n",
    "                            accuracy_defects = data[1,1]/np.sum(data[1,:])\n",
    "                            print(data)\n",
    "                            utl.PltErr(accuracy_crystals, accuracy_defects,\n",
    "                               attrs=symbols.GetAttrs(count=count%7,nevery=800,\n",
    "                                    label='%s layers, %s channels, act. %s'%(number_hidden_layer,n_channel,activation)), \n",
    "                                       Plot=False,\n",
    "                                       ax=ax,\n",
    "                                       )\n",
    "                        else:\n",
    "                            epoch = data[:,0]\n",
    "                            loss = data[:,1]\n",
    "                            val_loss = data[:,2]\n",
    "\n",
    "                            utl.PltErr(epoch, loss,\n",
    "                               attrs=symbols.GetAttrs(count=count%7,nevery=10,\n",
    "                                    label='train:%s layers, %s channels, act. %s'%(number_hidden_layer,n_channel,activation)), \n",
    "                                       Plot=False,\n",
    "                                       ax=ax,\n",
    "                                       )\n",
    "                            utl.PltErr(epoch, val_loss,\n",
    "                               attrs=symbols.GetAttrs(count=(count+1)%7,nevery=10,\n",
    "                                    label='test:%s layers, %s channels, act. %s'%(number_hidden_layer,n_channel,activation)), \n",
    "                                       Plot=False,\n",
    "                                       ax=ax,\n",
    "                                       )\n",
    "                    count += 1\n",
    "    ax = utl.PltErr(None, None,\n",
    "                        yscale='log',xscale='log',\n",
    "                       xstr='epoch',ystr='validation loss',\n",
    "#                     ylim=(1e-1,1e1),\n",
    "                    ax=ax,\n",
    "                    legend=legend.Get(),\n",
    "                       title='png/training_loss.png',\n",
    "                   )\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0de490d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm=np.loadtxt('neuralNet/ni/interestitials/test2nd/Run0/png/confusionMultiLabelClassification.txt').astype(int)\n",
    "# falseNegative = list(map(lambda x: 1.0*x[0]/(x[0]+x[1]), cm))\n",
    "# truePositive  = list(map(lambda x: 1.0*x[3]/(x[2]+x[3]), cm))\n",
    "# filtr  = cm[:,3] > 0\n",
    "# cm[filtr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b74daa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# legend = utl.Legends()\n",
    "# legend.Set(fontsize=14,bbox_to_anchor=(1.5, 0.3, 0.5, 0.5))\n",
    "# symbols = utl.Symbols()\n",
    "\n",
    "# fp = ['confusion.txt', 'val_loss_classification.txt'][0]\n",
    "# data = np.loadtxt('neuralNet/ni/kmc/inactive/Run0/png/%s'%(fp))\n",
    "# ax = utl.PltErr(None, None, Plot=False )\n",
    "# if fp == 'confusion.txt':\n",
    "#     accuracy_crystals = data[0,0]/np.sum(data[0,:])\n",
    "#     accuracy_defects = data[1,1]/np.sum(data[1,:])\n",
    "#     print(data)\n",
    "#     utl.PltErr(accuracy_crystals, accuracy_defects,\n",
    "#        attrs=symbols.GetAttrs(count=0,nevery=800,\n",
    "#             ), \n",
    "#                Plot=False,\n",
    "#                ax=ax,\n",
    "#                )\n",
    "# else:\n",
    "#     epoch = data[:,0]\n",
    "#     loss = data[:,1]\n",
    "#     val_loss = data[:,2]\n",
    "\n",
    "#     utl.PltErr(epoch, val_loss,\n",
    "#        attrs=symbols.GetAttrs(count=0,nevery=10,\n",
    "#             ), \n",
    "#                Plot=False,\n",
    "#                ax=ax,\n",
    "#                )\n",
    "    \n",
    "# ax = utl.PltErr(None, None,\n",
    "# yscale='log',xscale='log',\n",
    "# xstr='epoch',ystr='validation loss',\n",
    "# #                     ylim=(1e-1,1e1),\n",
    "# ax=ax,\n",
    "# # legend=legend.Get(),\n",
    "# title='png/training_loss.png',\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f506974a",
   "metadata": {},
   "source": [
    "## test example: 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1be92045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# y=np.c_[[1.725966,1.725967],\n",
    "#             [-1.725966,1.725967],\n",
    "#             [-1.725966,-1.725967],\n",
    "#             [1.725966,-1.725967],\n",
    "#            ].T\n",
    "\n",
    "# X=np.concatenate([list(map(lambda x:np.load('png/descriptor%s.npy'%x).flatten(),range(4)))],axis=1)\n",
    "\n",
    "# #--- zscore\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X)\n",
    "# X_transfrmd = scaler.transform( X )\n",
    "\n",
    "# X_train_transfrmd, X_test_transfrmd, y_train, y_test = train_test_split(X_transfrmd, y, test_size=0.25)\n",
    "# print(y_test)\n",
    "\n",
    "\n",
    "# print(X_train_transfrmd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69377834",
   "metadata": {},
   "source": [
    "### fully connected in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9c56d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #        pdb.set_trace()\n",
    "# #--- tune parameters\n",
    "\n",
    "# #--- train\n",
    "# mlp = MLPRegressor(random_state=1,\n",
    "#                     verbose=True,\n",
    "#                    n_iter_no_change=100000,\n",
    "#                     max_iter=100,#00,\n",
    "#                    hidden_layer_sizes=(1000,1000),\n",
    "# #                    shuffle=False,\n",
    "# #                     alpha=1e-1,\n",
    "\n",
    "#                   )\n",
    "# mlp.fit(X_train_transfrmd,y_train)\n",
    "\n",
    "# #--- validate\n",
    "# !mkdir png\n",
    "# utl.PltErr(range(len(mlp.loss_curve_)), mlp.loss_curve_,\n",
    "#            attrs={'fmt':'-'},\n",
    "#            yscale='log',xscale='log',\n",
    "# #           xlim=(1,self.max_iter[0]),\n",
    "#            xstr='epoch',ystr='loss',\n",
    "#            title='png/loss.png',\n",
    "#           )\n",
    "\n",
    "# # #         pdb.set_trace()\n",
    "# y_pred =mlp.predict(X_test_transfrmd)        \n",
    "# y_pred_train = mlp.predict(X_train_transfrmd)        \n",
    "# for idime, xstr in zip(range(2),'ux uy'.split()):\n",
    "#     ax = utl.PltErr(None,None,Plot=False)\n",
    "#     #\n",
    "#     utl.PltErr(y_test[:,idime],y_pred[:,idime],\n",
    "#                attrs={'fmt':'x','color':'red','zorder':10,'markersize':6},\n",
    "#                ax=ax,\n",
    "#                Plot = False,\n",
    "\n",
    "#               )\n",
    "#     #\n",
    "#     utl.PltErr(y_train[:,idime],y_pred_train[:,idime],\n",
    "#                attrs={'fmt':'.','color':'blue','zorder':1,'markersize':6},\n",
    "#                ax=ax,\n",
    "#                Plot = False,\n",
    "\n",
    "#               )\n",
    "#     #\n",
    "#     utl.PltErr(None,None,Plot=False,\n",
    "#                    title='png/scatter%s.png'%idime,\n",
    "#                     ax=ax,\n",
    "#                xstr='%s actual'%xstr,ystr='%s predicted'%xstr,\n",
    "#                xlim=(-3,3),ylim=(-3,3),\n",
    "#                    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5e1353cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp.best_loss_, mlp.loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "157c537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ux,uy=mlp.predict(X_test_transfrmd)[0]\n",
    "# ax=utl.PltErr([0,ux],[0,uy],\n",
    "#               Plot=False\n",
    "#           )\n",
    "# utl.PltErr([0,y_test[0][0]],[0,y_test[0][1]],\n",
    "#            xlim=(-3,3),ylim=(-3,3),\n",
    "#             ax=ax\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a952ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ux,uy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ce616c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = utl.PltErr(None,None,Plot=False)\n",
    "\n",
    "# for i in range(2):\n",
    "#     utl.PltErr(range(data.descriptors[0,:].shape[0]),data.descriptors[i,:],\n",
    "#               attrs={'fmt':'-'},#,'color':'C0'},\n",
    "#                xscale='log',yscale='log',\n",
    "#                ax=ax,\n",
    "#                Plot=False,\n",
    "#               )\n",
    "\n",
    "# utl.PltErr(range(data.descriptors[100,:].shape[0]),data.descriptors[100,:],\n",
    "#           attrs={'fmt':'-','color':'C0'},\n",
    "#            xscale='log',yscale='log',\n",
    "#            ax=ax,\n",
    "#            Plot=False,\n",
    "#           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a5598bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.Spectra(nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1d061978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPRegressor\n",
    "# from sklearn.datasets import make_regression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X, y = make_regression(n_samples=200, random_state=1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "#                                                     random_state=1)\n",
    "# regr = MLPRegressor(verbose=False,\n",
    "#                     random_state=1, \n",
    "# #                     learning_rate='adaptive',\n",
    "# #                    early_stopping=True, \n",
    "#                      n_iter_no_change=1, \n",
    "#                     tol=1e-2,\n",
    "#                      max_iter=10000000,\n",
    "# #                     solver='sgd',\n",
    "#                    ).fit(X_train, y_train)\n",
    "# regr.tol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5eecba",
   "metadata": {},
   "source": [
    "### fully connected in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0a27a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #--- The network architecture\n",
    "# model = keras.Sequential([\n",
    "#     layers.Dense(512), #activation=\"relu\"),\n",
    "# #     layers.Dense(1000), #activation=\"relu\"),\n",
    "#     layers.Dense(2) #, activation=\"relu\")\n",
    "#     ])\n",
    "\n",
    "# #--- The compilation step\n",
    "# optimizer = tf.keras.optimizers.Adam() #learning_rate=1e-4)\n",
    "# model.compile( optimizer=optimizer,#\"rmsprop\",\n",
    "#                loss=\"mean_squared_error\",#\"sparse_categorical_crossentropy\",\n",
    "#                metrics=[\"mse\"]\n",
    "#              )\n",
    "\n",
    "# #--- Preparing the image data\n",
    "# # train_images = train_images.reshape((60000, 28 * 28))\n",
    "# # train_images = train_images.astype(\"float32\") / 255\n",
    "# # test_images = test_images.reshape((10000, 28 * 28))\n",
    "# # test_images = test_images.astype(\"float32\") / 255\n",
    "\n",
    "# #--- “Fitting” the model X_train_transfrmd,y_train\n",
    "# model.fit(X_train_transfrmd, y_train, \n",
    "#             validation_data=(X_test_transfrmd, y_test),\n",
    "\n",
    "#           epochs=100, verbose=False)#, batch_size=128)\n",
    "\n",
    "# loss = model.history.history['loss']\n",
    "# val_loss = model.history.history['val_loss']\n",
    "# #--- validate\n",
    "\n",
    "# ax = utl.PltErr(range(len(val_loss)), val_loss,\n",
    "#            attrs={'fmt':'-'}, Plot=False,\n",
    "#           )\n",
    "# utl.PltErr(range(len(loss)), loss,\n",
    "#            attrs={'fmt':'-'},\n",
    "#            ax=ax,\n",
    "#            yscale='log',xscale='log',\n",
    "#            xlim=(1,100),\n",
    "#            xstr='epoch',ystr='loss',\n",
    "#            title='png/loss.png',\n",
    "#           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262922c9",
   "metadata": {},
   "source": [
    "### cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "70f22b71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tf.random.set_random_seed(812)\n",
    "\n",
    "# shape=(300,300,1)\n",
    "# kernel_size = (3,3)\n",
    "# epochs = 1000\n",
    "# activation = ['linear','sigmoid','relu'][0]\n",
    "# padding='same'\n",
    "# filters = 1\n",
    "# #\n",
    "# ndime = y_train.shape[1]\n",
    "# n_train = X_train_transfrmd.shape[0]\n",
    "# n_test = X_test_transfrmd.shape[0]\n",
    "# assert shape[0]*shape[1]*shape[2] == X_train_transfrmd.shape[1]\n",
    "# inputs = keras.Input(shape=shape)\n",
    "# #\n",
    "# x = layers.Conv2D(filters=filters, kernel_size=kernel_size,activation=activation,padding=padding)(inputs)\n",
    "# # x = layers.AveragePooling2D(pool_size=2)(x)\n",
    "# # x = layers.Conv2D(filters=2*filters, kernel_size=kernel_size,activation=activation,padding=padding)(x)\n",
    "# # x = layers.AveragePooling2D(pool_size=2)(x)\n",
    "# # x = layers.Conv2D(filters=4*filters, kernel_size=kernel_size,activation=activation,padding=padding)(x)\n",
    "# # x = layers.AveragePooling2D(pool_size=2)(x)\n",
    "# # x = layers.Conv2D(filters=8*filters, kernel_size=kernel_size,activation=activation,padding=padding)(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# outputs = layers.Dense( ndime, activation=activation)(x)\n",
    "\n",
    "# #--- The network architecture\n",
    "# model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "# #--- The compilation step\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5,epsilon=1e-08)\n",
    "# model.compile( optimizer=optimizer,#\"rmsprop\",\n",
    "#                loss=\"mean_squared_error\",#\"sparse_categorical_crossentropy\",\n",
    "#                metrics=[\"mse\"]\n",
    "#              )\n",
    "\n",
    "# #--- save best model \n",
    "# callbacks=[keras.callbacks.ModelCheckpoint( filepath='png/convnet_from_scratch.keras',  \n",
    "#                                            monitor=\"val_loss\",\n",
    "#                                            save_freq=10,\n",
    "#                                             save_best_only=True)]\n",
    "\n",
    "# #--- “Fitting” the model X_train_transfrmd,y_train\n",
    "# X_train_reshaped = X_train_transfrmd.reshape((n_train,shape[0],shape[1],1))\n",
    "# X_test_reshaped = X_test_transfrmd.reshape((n_test,shape[0],shape[1],1))\n",
    "# model.fit(X_train_reshaped, y_train, \n",
    "#             validation_data=(X_test_reshaped, y_test),\n",
    "#             #callbacks=callbacks,\n",
    "#           epochs=epochs, verbose=False, shuffle=False)#, batch_size=128)\n",
    "\n",
    "# loss = model.history.history['loss']\n",
    "# val_loss = model.history.history['val_loss']\n",
    "# #--- validate\n",
    "\n",
    "# ax = utl.PltErr(range(len(val_loss)), val_loss,\n",
    "#            attrs={'fmt':'-'}, Plot=False,\n",
    "#           )\n",
    "# utl.PltErr(range(len(loss)), loss,\n",
    "#            attrs={'fmt':'-'},\n",
    "#            ax=ax,\n",
    "#            yscale='log',xscale='log',\n",
    "#            xlim=(1,epochs),\n",
    "#            xstr='epoch',ystr='loss',\n",
    "#            title='png/loss.png',\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "673ae920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model = keras.models.load_model(\"png/convnet_from_scratch.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9fa9c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ux,uy=best_model.predict(X_test_reshaped)[0]\n",
    "# ax=utl.PltErr([0,ux],[0,uy],\n",
    "#               Plot=False\n",
    "#           )\n",
    "# utl.PltErr([0,y_test[0][0]],[0,y_test[0][1]],\n",
    "#            xlim=(-3,3),ylim=(-3,3),\n",
    "#             ax=ax\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "eaf166dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (ux,uy), y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf163c",
   "metadata": {},
   "source": [
    "# gnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dd8bf5",
   "metadata": {},
   "source": [
    "# fixed output size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c366ea",
   "metadata": {},
   "source": [
    "## build catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3318f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evlist_dir = '%s/EVLIST_DIR'%confParser['input files']['input_path']\n",
    "events_dir = '%s/SPEC_EVENTS_DIR'%confParser['input files']['input_path']\n",
    "lib_path   = confParser['input files']['lib_path'].split()[0] #'../../HeaDef/postprocess'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2b5e7413",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: junk.xyz: No such file or directory\n",
      "input :../simulations/ni/pure/test/Run0/SPEC_EVENTS_DIR/spec_event_135_1_0.xyz\n",
      "output disp:3.899477958679199 s\n",
      "parsing junk.xyz\n",
      "elapsed time 0.034010887145996094 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.520380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.520380</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010411</td>\n",
       "      <td>-3.551225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.551225</td>\n",
       "      <td>-0.010411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.537112</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>-0.007376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  type         x         y         z\n",
       "0   1     1  0.000000  0.000000  3.520380\n",
       "1   2     1  0.000000  3.520380  0.000000\n",
       "2   3     1  0.000000 -0.010411 -3.551225\n",
       "3   4     1  0.000000 -3.551225 -0.010411\n",
       "4   5     1 -3.537112 -0.007376 -0.007376"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :../simulations/ni/pure/test/Run0/SPEC_EVENTS_DIR/spec_event_136_1_0.xyz\n",
      "output disp:1.5025091171264648 s\n",
      "parsing junk.xyz\n",
      "elapsed time 0.009266853332519531 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.520380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.520380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.010411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.551225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.551225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>-3.537112</td>\n",
       "      <td>-0.007376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  type         x         y         z\n",
       "0   1     1  0.000000  0.000000  3.520380\n",
       "1   2     1  3.520380  0.000000  0.000000\n",
       "2   3     1 -0.010411  0.000000 -3.551225\n",
       "3   4     1 -3.551225  0.000000 -0.010411\n",
       "4   5     1 -0.007376 -3.537112 -0.007376"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :../simulations/ni/pure/test/Run0/SPEC_EVENTS_DIR/spec_event_140_1_0.xyz\n",
      "output disp:1.3091959953308105 s\n",
      "parsing junk.xyz\n",
      "elapsed time 0.009696006774902344 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.520380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.520380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.551225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.551225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>-3.537112</td>\n",
       "      <td>-0.007376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  type         x         y         z\n",
       "0   1     1 -3.520380  0.000000  0.000000\n",
       "1   2     1  0.000000  0.000000  3.520380\n",
       "2   3     1  3.551225  0.000000 -0.010411\n",
       "3   4     1  0.010411  0.000000 -3.551225\n",
       "4   5     1  0.007376 -3.537112 -0.007376"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :../simulations/ni/pure/test/Run0/SPEC_EVENTS_DIR/spec_event_155_1_0.xyz\n",
      "output disp:1.9590892791748047 s\n",
      "parsing junk.xyz\n",
      "elapsed time 0.03179025650024414 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.520380</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.520380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.551225</td>\n",
       "      <td>-0.010411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010411</td>\n",
       "      <td>-3.551225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.537112</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>-0.007376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  type         x         y         z\n",
       "0   1     1  0.000000 -3.520380  0.000000\n",
       "1   2     1  0.000000  0.000000  3.520380\n",
       "2   3     1  0.000000  3.551225 -0.010411\n",
       "3   4     1  0.000000  0.010411 -3.551225\n",
       "4   5     1 -3.537112  0.007376 -0.007376"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :../simulations/ni/pure/test/Run0/SPEC_EVENTS_DIR/spec_event_233_1_0.xyz\n",
      "output disp:1.0455858707427979 s\n",
      "parsing junk.xyz\n",
      "elapsed time 0.09363627433776855 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.520380</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.520380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.010411</td>\n",
       "      <td>-3.551225</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.551225</td>\n",
       "      <td>-0.010411</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>-3.537112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  type         x         y         z\n",
       "0   1     1  0.000000  3.520380  0.000000\n",
       "1   2     1  3.520380  0.000000  0.000000\n",
       "2   3     1 -0.010411 -3.551225  0.000000\n",
       "3   4     1 -3.551225 -0.010411  0.000000\n",
       "4   5     1 -0.007376 -0.007376 -3.537112"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :../simulations/ni/pure/test/Run0/SPEC_EVENTS_DIR/spec_event_234_1_0.xyz\n",
      "output disp:0.8603792190551758 s\n",
      "parsing junk.xyz\n",
      "elapsed time 0.01347208023071289 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.520380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.520380</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010411</td>\n",
       "      <td>3.551225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.551225</td>\n",
       "      <td>0.010411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.537112</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>0.007376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  type         x         y         z\n",
       "0   1     1  0.000000  0.000000 -3.520380\n",
       "1   2     1  0.000000  3.520380  0.000000\n",
       "2   3     1  0.000000 -0.010411  3.551225\n",
       "3   4     1  0.000000 -3.551225  0.010411\n",
       "4   5     1 -3.537112 -0.007376  0.007376"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :../simulations/ni/pure/test/Run0/SPEC_EVENTS_DIR/spec_event_235_1_0.xyz\n",
      "output disp:1.2264249324798584 s\n",
      "parsing junk.xyz\n",
      "elapsed time 0.008865833282470703 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.520380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.520380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.010411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.551225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.551225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>-3.537112</td>\n",
       "      <td>0.007376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  type         x         y         z\n",
       "0   1     1  0.000000  0.000000 -3.520380\n",
       "1   2     1  3.520380  0.000000  0.000000\n",
       "2   3     1 -0.010411  0.000000  3.551225\n",
       "3   4     1 -3.551225  0.000000  0.010411\n",
       "4   5     1 -0.007376 -3.537112  0.007376"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :../simulations/ni/pure/test/Run0/SPEC_EVENTS_DIR/spec_event_236_1_0.xyz\n",
      "output disp:0.8771510124206543 s\n",
      "parsing junk.xyz\n",
      "elapsed time 0.008526086807250977 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.520380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.520380</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.551225</td>\n",
       "      <td>-0.010411</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010411</td>\n",
       "      <td>-3.551225</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>-3.537112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  type         x         y         z\n",
       "0   1     1 -3.520380  0.000000  0.000000\n",
       "1   2     1  0.000000  3.520380  0.000000\n",
       "2   3     1  3.551225 -0.010411  0.000000\n",
       "3   4     1  0.010411 -3.551225  0.000000\n",
       "4   5     1  0.007376 -0.007376 -3.537112"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :../simulations/ni/pure/test/Run0/SPEC_EVENTS_DIR/spec_event_239_1_0.xyz\n",
      "output disp:1.000929832458496 s\n",
      "parsing junk.xyz\n",
      "elapsed time 0.009063243865966797 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.520380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.520380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.551225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.551225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>-3.537112</td>\n",
       "      <td>0.007376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  type         x         y         z\n",
       "0   1     1 -3.520380  0.000000  0.000000\n",
       "1   2     1  0.000000  0.000000 -3.520380\n",
       "2   3     1  3.551225  0.000000  0.010411\n",
       "3   4     1  0.010411  0.000000  3.551225\n",
       "4   5     1  0.007376 -3.537112  0.007376"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :../simulations/ni/pure/test/Run0/SPEC_EVENTS_DIR/spec_event_252_1_0.xyz\n",
      "output disp:1.1703450679779053 s\n",
      "parsing junk.xyz\n",
      "elapsed time 0.013833045959472656 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.520380</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.520380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.010411</td>\n",
       "      <td>3.551225</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.551225</td>\n",
       "      <td>0.010411</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>-3.537112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  type         x         y         z\n",
       "0   1     1  0.000000 -3.520380  0.000000\n",
       "1   2     1  3.520380  0.000000  0.000000\n",
       "2   3     1 -0.010411  3.551225  0.000000\n",
       "3   4     1 -3.551225  0.010411  0.000000\n",
       "4   5     1 -0.007376  0.007376 -3.537112"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :../simulations/ni/pure/test/Run0/SPEC_EVENTS_DIR/spec_event_254_1_0.xyz\n",
      "output disp:1.1971960067749023 s\n",
      "parsing junk.xyz\n",
      "elapsed time 0.0110321044921875 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.520380</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.520380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.551225</td>\n",
       "      <td>0.010411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010411</td>\n",
       "      <td>3.551225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.537112</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>0.007376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  type         x         y         z\n",
       "0   1     1  0.000000 -3.520380  0.000000\n",
       "1   2     1  0.000000  0.000000 -3.520380\n",
       "2   3     1  0.000000  3.551225  0.010411\n",
       "3   4     1  0.000000  0.010411  3.551225\n",
       "4   5     1 -3.537112  0.007376  0.007376"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :../simulations/ni/pure/test/Run0/SPEC_EVENTS_DIR/spec_event_256_1_0.xyz\n",
      "output disp:1.1886210441589355 s\n",
      "parsing junk.xyz\n",
      "elapsed time 0.008410215377807617 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.520380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.520380</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.551225</td>\n",
       "      <td>0.010411</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010411</td>\n",
       "      <td>3.551225</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>-3.537112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  type         x         y         z\n",
       "0   1     1 -3.520380  0.000000  0.000000\n",
       "1   2     1  0.000000 -3.520380  0.000000\n",
       "2   3     1  3.551225  0.010411  0.000000\n",
       "3   4     1  0.010411  3.551225  0.000000\n",
       "4   5     1  0.007376  0.007376 -3.537112"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def ParseEvList_dir():\n",
    "    files = os.listdir(evlist_dir)\n",
    "    events={}\n",
    "    for sfile in files:\n",
    "        try:\n",
    "            kmc_step = int(sfile.split('_')[-1])\n",
    "    #        print(kmc_step)\n",
    "            filee=open('%s/%s'%(evlist_dir,sfile)) #--- open file\n",
    "            events[kmc_step] = pd.read_csv(filee,delim_whitespace=True).iloc[1:]#delimiter='')\n",
    "        except:\n",
    "            continue\n",
    "    return events\n",
    "\n",
    "def ModifyCatalog(  ):\n",
    "    '''\n",
    "    Return a dataframe including per-atom energy and defect type \n",
    "    '''        \n",
    "    kmc_step     = 1\n",
    "\n",
    "    #--- discard refined == False\n",
    "    filtr = catalog[ kmc_step ].refined == 'T'\n",
    "    catalog[ kmc_step ] = catalog[ kmc_step ][ filtr ]\n",
    "\n",
    "    #--- displacements\n",
    "    fout = 'transition_paths.json'\n",
    "    os.system('rm %s'%fout)\n",
    "    catalog[ kmc_step ].apply(lambda x:GetDispForEvents(x,fout),axis=1)\n",
    "\n",
    "    #--- modify catalog\n",
    "#     self.catalog[ kmc_step ]['refined'] = 1 #--- add column: 1\n",
    "#     self.catalog[ kmc_step ][ 'DirX DirY DirZ'.split() ] = np.c_[ list( disps_events ) ] #--- add columns: ux, uy, uz\n",
    "#     self.catalog[ kmc_step ].reset_index(drop = True, inplace=True) #--- reset index\n",
    "\n",
    "#     #--- add atom indices\n",
    "#     AtomIds     = np.c_[self.catalog[ kmc_step ].AtomId].astype(int).flatten() #--- add atom index\n",
    "#     AtomIndices = EnergyBarrier.GetDataFrameIndex(self.perAtomData, key='id', val=AtomIds)\n",
    "#     self.catalog[ kmc_step ]['AtomIndex'] = AtomIndices\n",
    "\n",
    "def GetDispForEvents( item, fout ):\n",
    "    #--- parse spec_event file including the diffusion path\n",
    "    try:\n",
    "        diffusion_path_dict = ParseSpecEvents_dir( '%s/spec_event_%s_%s_0'\\\n",
    "                                                                %( events_dir, item.AtomId, item.Spec_id ) ) \n",
    "        diffusion_path_xyz = Displacement( '%s/spec_event_%s_%s_0.xyz'\\\n",
    "                                                                 %( events_dir, item.AtomId, item.Spec_id ), 'junk.xyz' ) \n",
    "        assert diffusion_path_dict[ 'active_atom_label' ] == int( item.AtomId )\n",
    "        assert diffusion_path_dict[ 'Spec_eventId' ]      == int( item.Spec_id )\n",
    "        assert diffusion_path_dict[ 'Original_eventId' ]  == int( item.eventId )\n",
    "        assert diffusion_path_dict[ 'event_label' ][ 0 ]  == int( item.IniTopoId ) and\\\n",
    "               diffusion_path_dict[ 'event_label' ][ 1 ]  == int( item.SadTopoId ) and\\\n",
    "               diffusion_path_dict[ 'event_label' ][ 2 ]  == int( item.FinTopoId ) \n",
    "\n",
    "        #--- save as json\n",
    "        with open(fout,'a') as fp:\n",
    "            rwjs = utl.ReadWriteJson(append=True)\n",
    "            rwjs.Write( [diffusion_path_xyz.to_dict()], fp )\n",
    "        \n",
    "        return diffusion_path_dict[ 'delr_main_atom' ]\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        return [np.nan, np.nan, np.nan ]\n",
    "\n",
    "def ParseSpecEvents_dir(fp):\n",
    "    '''\n",
    "    return energy barriers associated with hopping events\n",
    "    '''\n",
    "    with open( fp ) as filee: #'%s/%s'%(self.events_dir,sfile)) #--- open file\n",
    "        xstrs             = filee.readlines()\n",
    "        Spec_eventId      = int(xstrs[0].split()[-1]) #--- specific event id\n",
    "        Original_eventId  = int(xstrs[1].split()[-1]) #--- Original eventId\n",
    "        Refinement_step   = int(xstrs[2].split()[-1]) #--- Original eventId\n",
    "        event_label       = list(map(int,xstrs[3].split()[-1:-4:-1])) #--- Original eventId\n",
    "        energy_barrier    = float(xstrs[4].split()[-1]) #--- energy\n",
    "        delr_main_atom    = list(map(float,xstrs[12].split()[-1:-4:-1]))                 \n",
    "        active_atom_label = int(xstrs[15].split()[-1])\n",
    "\n",
    "    event_label.reverse()\n",
    "    delr_main_atom.reverse()\n",
    "    return {'Spec_eventId':Spec_eventId, 'Original_eventId':Original_eventId, 'Refinement_step':Refinement_step,\\\n",
    "            'event_label':event_label,   'energy_barrier':energy_barrier,     'delr_main_atom':delr_main_atom,\\\n",
    "            'active_atom_label':active_atom_label}\n",
    "        \n",
    "def Displacement(fp, fout,verbose=True):\n",
    "    '''\n",
    "    Return total displacements \n",
    "    '''\n",
    "    !rm $fout\n",
    "#    pdb.set_trace()\n",
    "    #--- fetch parameters\n",
    "\n",
    "    #--- call ovito\n",
    "    t0 = time.time()\n",
    "    if verbose:\n",
    "        print('input :%s'%fp)\n",
    "    os.system('ovitos %s/OvitosCna.py %s %s 1 7 %s'%(lib_path,fp,fout,'header.txt'))\n",
    "    if verbose:\n",
    "        print('output disp:%s s'%(time.time()-t0))\n",
    "\n",
    "    #--- parse disp files\n",
    "    if verbose:\n",
    "        print('parsing %s'%fout)\n",
    "    t0 = time.time()\n",
    "    lmpDisp = lp.ReadDumpFile( fout )\n",
    "    lmpDisp.GetCords( ncount = sys.maxsize )\n",
    "    if verbose:\n",
    "        print('elapsed time %s s'%(time.time()-t0))\n",
    "        display(lmpDisp.coord_atoms_broken[0].head())\n",
    "    \n",
    "    dispSad = lmpDisp.coord_atoms_broken[1]['x y z'.split()] - lmpDisp.coord_atoms_broken[0]['x y z'.split()]\n",
    "    dispFin = lmpDisp.coord_atoms_broken[2]['x y z'.split()] - lmpDisp.coord_atoms_broken[0]['x y z'.split()]\n",
    "    idType_xyz = lmpDisp.coord_atoms_broken[0]['id type x y z'.split()]\n",
    "    df = pd.DataFrame(np.c_[idType_xyz, dispSad,dispFin],columns='id type x y z ux_sad uy_sad uz_sad ux_fin uy_fin uz_fin'.split())\n",
    "    \n",
    "    return df\n",
    "            \n",
    "catalog          = ParseEvList_dir()\n",
    "ModifyCatalog()\n",
    "#catalog[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "14dce741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>ux_sad</th>\n",
       "      <th>uy_sad</th>\n",
       "      <th>uz_sad</th>\n",
       "      <th>ux_fin</th>\n",
       "      <th>uy_fin</th>\n",
       "      <th>uz_fin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.520380</td>\n",
       "      <td>0.026960</td>\n",
       "      <td>2.710000e-06</td>\n",
       "      <td>-0.066493</td>\n",
       "      <td>-0.010883</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.016851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.520380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.066401</td>\n",
       "      <td>-4.377000e-05</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.016856</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.010880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.010411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.551225</td>\n",
       "      <td>0.009176</td>\n",
       "      <td>-4.112000e-05</td>\n",
       "      <td>0.012709</td>\n",
       "      <td>-0.000283</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.007821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.551225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010411</td>\n",
       "      <td>0.012597</td>\n",
       "      <td>7.000000e-07</td>\n",
       "      <td>0.009154</td>\n",
       "      <td>-0.007829</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>-3.537112</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>-0.014014</td>\n",
       "      <td>-6.602020e-02</td>\n",
       "      <td>-0.013971</td>\n",
       "      <td>-0.003582</td>\n",
       "      <td>-0.014026</td>\n",
       "      <td>-0.003578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>3.537112</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>-0.013940</td>\n",
       "      <td>6.597610e-02</td>\n",
       "      <td>-0.013986</td>\n",
       "      <td>-0.003577</td>\n",
       "      <td>0.014020</td>\n",
       "      <td>-0.003579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.760190</td>\n",
       "      <td>1.760190</td>\n",
       "      <td>-0.043298</td>\n",
       "      <td>-1.720990e-01</td>\n",
       "      <td>0.033284</td>\n",
       "      <td>-0.011148</td>\n",
       "      <td>-0.001658</td>\n",
       "      <td>-0.011207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.760190</td>\n",
       "      <td>1.760190</td>\n",
       "      <td>-0.043249</td>\n",
       "      <td>1.720758e-01</td>\n",
       "      <td>0.033277</td>\n",
       "      <td>-0.011146</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>-0.011208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.760190</td>\n",
       "      <td>-1.760190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033322</td>\n",
       "      <td>-1.721280e-01</td>\n",
       "      <td>-0.043224</td>\n",
       "      <td>-0.011204</td>\n",
       "      <td>-0.001661</td>\n",
       "      <td>-0.011144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.760190</td>\n",
       "      <td>1.760190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033366</td>\n",
       "      <td>1.720518e-01</td>\n",
       "      <td>-0.043227</td>\n",
       "      <td>-0.011202</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>-0.011145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.788934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.760190</td>\n",
       "      <td>0.085131</td>\n",
       "      <td>-3.650000e-06</td>\n",
       "      <td>-0.037046</td>\n",
       "      <td>0.018860</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.011859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.760190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.788934</td>\n",
       "      <td>-0.036965</td>\n",
       "      <td>-4.737000e-05</td>\n",
       "      <td>0.085215</td>\n",
       "      <td>-0.011856</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.018865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.775532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.775532</td>\n",
       "      <td>0.031540</td>\n",
       "      <td>-1.655000e-05</td>\n",
       "      <td>0.031594</td>\n",
       "      <td>0.005559</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.005562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.776922</td>\n",
       "      <td>-1.767566</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>0.032770</td>\n",
       "      <td>-6.134400e-03</td>\n",
       "      <td>0.010623</td>\n",
       "      <td>0.006084</td>\n",
       "      <td>0.005887</td>\n",
       "      <td>-0.003474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.776922</td>\n",
       "      <td>1.767566</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>0.032813</td>\n",
       "      <td>6.112800e-03</td>\n",
       "      <td>0.010625</td>\n",
       "      <td>0.006087</td>\n",
       "      <td>-0.005891</td>\n",
       "      <td>-0.003474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>-1.767566</td>\n",
       "      <td>-1.776922</td>\n",
       "      <td>0.010617</td>\n",
       "      <td>-6.120900e-03</td>\n",
       "      <td>0.032892</td>\n",
       "      <td>-0.003475</td>\n",
       "      <td>0.005886</td>\n",
       "      <td>0.006092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>1.767566</td>\n",
       "      <td>-1.776922</td>\n",
       "      <td>0.010655</td>\n",
       "      <td>6.049300e-03</td>\n",
       "      <td>0.032891</td>\n",
       "      <td>-0.003473</td>\n",
       "      <td>-0.005894</td>\n",
       "      <td>0.006091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875098</td>\n",
       "      <td>-2.736000e-05</td>\n",
       "      <td>0.875087</td>\n",
       "      <td>1.747429</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>1.747427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  type         x         y         z    ux_sad        uy_sad  \\\n",
       "0    1.0   1.0  0.000000  0.000000  3.520380  0.026960  2.710000e-06   \n",
       "1    2.0   1.0  3.520380  0.000000  0.000000 -0.066401 -4.377000e-05   \n",
       "2    3.0   1.0 -0.010411  0.000000 -3.551225  0.009176 -4.112000e-05   \n",
       "3    4.0   1.0 -3.551225  0.000000 -0.010411  0.012597  7.000000e-07   \n",
       "4    5.0   1.0 -0.007376 -3.537112 -0.007376 -0.014014 -6.602020e-02   \n",
       "5    6.0   1.0 -0.007376  3.537112 -0.007376 -0.013940  6.597610e-02   \n",
       "6    7.0   1.0  0.000000 -1.760190  1.760190 -0.043298 -1.720990e-01   \n",
       "7    8.0   1.0  0.000000  1.760190  1.760190 -0.043249  1.720758e-01   \n",
       "8    9.0   1.0  1.760190 -1.760190  0.000000  0.033322 -1.721280e-01   \n",
       "9   10.0   1.0  1.760190  1.760190  0.000000  0.033366  1.720518e-01   \n",
       "10  11.0   1.0 -1.788934  0.000000  1.760190  0.085131 -3.650000e-06   \n",
       "11  12.0   1.0  1.760190  0.000000 -1.788934 -0.036965 -4.737000e-05   \n",
       "12  13.0   1.0 -1.775532  0.000000 -1.775532  0.031540 -1.655000e-05   \n",
       "13  14.0   1.0 -1.776922 -1.767566 -0.007376  0.032770 -6.134400e-03   \n",
       "14  15.0   1.0 -1.776922  1.767566 -0.007376  0.032813  6.112800e-03   \n",
       "15  16.0   1.0 -0.007376 -1.767566 -1.776922  0.010617 -6.120900e-03   \n",
       "16  17.0   1.0 -0.007376  1.767566 -1.776922  0.010655  6.049300e-03   \n",
       "17  18.0   1.0  0.000000  0.000000  0.000000  0.875098 -2.736000e-05   \n",
       "\n",
       "      uz_sad    ux_fin    uy_fin    uz_fin  \n",
       "0  -0.066493 -0.010883 -0.000002  0.016851  \n",
       "1   0.027027  0.016856 -0.000005 -0.010880  \n",
       "2   0.012709 -0.000283 -0.000004 -0.007821  \n",
       "3   0.009154 -0.007829 -0.000001 -0.000284  \n",
       "4  -0.013971 -0.003582 -0.014026 -0.003578  \n",
       "5  -0.013986 -0.003577  0.014020 -0.003579  \n",
       "6   0.033284 -0.011148 -0.001658 -0.011207  \n",
       "7   0.033277 -0.011146  0.001653 -0.011208  \n",
       "8  -0.043224 -0.011204 -0.001661 -0.011144  \n",
       "9  -0.043227 -0.011202  0.001653 -0.011145  \n",
       "10 -0.037046  0.018860 -0.000002 -0.011859  \n",
       "11  0.085215 -0.011856 -0.000005  0.018865  \n",
       "12  0.031594  0.005559 -0.000003  0.005562  \n",
       "13  0.010623  0.006084  0.005887 -0.003474  \n",
       "14  0.010625  0.006087 -0.005891 -0.003474  \n",
       "15  0.032892 -0.003475  0.005886  0.006092  \n",
       "16  0.032891 -0.003473 -0.005894  0.006091  \n",
       "17  0.875087  1.747429 -0.000003  1.747427  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rwjs = utl.ReadWriteJson()\n",
    "transition_paths = rwjs.Read( 'transition_paths.json' )\n",
    "pd.DataFrame(transition_paths[ 1 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4ba84ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm 'ovito.xyz'\n",
    "for indx, item in enumerate( transition_paths ):\n",
    "    cordc = pd.DataFrame(item)\n",
    "    with open('ovito.xyz','a') as fp: \n",
    "        utl.PrintOvito(cordc, fp, 'itime=%s'%indx, attr_list='id type x y z ux_fin uy_fin uz_fin'.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fff5f9",
   "metadata": {},
   "source": [
    "## graph net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7ed54e9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class GraphNeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dims, activation):\n",
    "        super(GraphNeuralNet, self).__init__()\n",
    "        self.output_dims = output_dims\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "        self.fc_out = nn.ModuleList([nn.Linear(hidden_dims[-1], dim) for dim in output_dims])\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x, adj_matrices):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation(hidden_layer(x))\n",
    "        outputs = []\n",
    "        for adj_matrix in adj_matrices:\n",
    "            hidden = torch.matmul(adj_matrix, x)\n",
    "            for fc_out in self.fc_out:\n",
    "                outputs.append(fc_out(hidden))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def compute_adjacency_matrices(input_data, rcut):\n",
    "    adj_matrices = []\n",
    "    \n",
    "    for positions in input_data:\n",
    "        num_atoms = positions.shape[0]\n",
    "        adj_matrix = torch.zeros((num_atoms, num_atoms), dtype=torch.float)\n",
    "        \n",
    "        for i in range(num_atoms):\n",
    "            for j in range(i + 1, num_atoms):\n",
    "                distance = torch.norm(positions[i] - positions[j])\n",
    "                if distance <= rcut:\n",
    "                    adj_matrix[i, j] = 1\n",
    "                    adj_matrix[j, i] = 1\n",
    "            assert adj_matrix[i,:].sum() > 0, 'dangling node : increase the cutoff!'\n",
    "        adj_matrices.append(adj_matrix)\n",
    "    \n",
    "    #--- assert no \n",
    "    return adj_matrices\n",
    "\n",
    "\n",
    "def augment_data(input_data, target_displacements, noise_std):\n",
    "    augmented_input_data = []\n",
    "    augmented_target_displacements = []\n",
    "\n",
    "    for data, target in zip(input_data, target_displacements):\n",
    "        # Add Gaussian noise to input data\n",
    "        noisy_data = data + torch.randn_like(data) * noise_std\n",
    "        augmented_input_data.append(noisy_data)\n",
    "\n",
    "        # Add Gaussian noise to target displacements\n",
    "        noisy_target = target + torch.randn_like(target) * noise_std\n",
    "        augmented_target_displacements.append(noisy_target)\n",
    "\n",
    "    return augmented_input_data, augmented_target_displacements\n",
    "\n",
    "def standardize_data(data, mean, std):\n",
    "    return (data - mean) / std\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "aaf54ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(): # Example usage\n",
    "    input_dim = eval(confParser['gnn']['input_dim']) #3  # Dimensionality of atom positions (e.g., x, y, z coordinates)\n",
    "    hidden_dim = eval(confParser['gnn']['hidden_dim']) #[64]  # Dimensionality of hidden layers\n",
    "    output_dims = eval(confParser['gnn']['output_dims']) #[3]  # Dimensionality of displacement vectors for each snapshot\n",
    "    activation = eval(confParser['gnn']['activation']) #nn.ReLU() #nn.Identity()) #F.relu)\n",
    "    lr = eval(confParser['gnn']['lr'])# 1.0e-4\n",
    "    ntrain = eval(confParser['gnn']['ntrain'])#100\n",
    "    num_epochs = eval(confParser['gnn']['num_epochs'])#20000\n",
    "    noise_std   = eval(confParser['gnn']['noise_std'])#0.1\n",
    "\n",
    "    num_snapshots = len( transition_paths )\n",
    "    snapshots     = range(num_snapshots)\n",
    "\n",
    "    model = GraphNeuralNet(input_dim, hidden_dim, output_dims,activation) #\n",
    "\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Example training data\n",
    "    num_atoms = [ len(transition_paths[ i ]['id']) for i in snapshots ]\n",
    "    input_data = [torch.from_numpy( np.c_[pd.DataFrame(transition_paths[ i ])['x y z'.split()]] ).float() for i in snapshots]  \n",
    "\n",
    "\n",
    "\n",
    "    # Example target data (displacement vectors for each snapshot and each path)\n",
    "    target_displacements = [torch.from_numpy( np.c_[pd.DataFrame(transition_paths[ i ])['ux_fin uy_fin uz_fin'.split()]] ).float() for i in snapshots for dim in output_dims]\n",
    "\n",
    "\n",
    "    # Augment the dataset to have order 100 snapshots\n",
    "    augmented_input_data = []\n",
    "    augmented_target_displacements = []\n",
    "    input_data_tensor = torch.stack(input_data)\n",
    "    ntrain_initial = input_data_tensor.shape[0]*input_data_tensor.shape[1]\n",
    "    n_repeat = np.max([1,int(ntrain/ntrain_initial)])\n",
    "\n",
    "    for _ in range(n_repeat):  # Repeat the augmentation process 10 times\n",
    "        augmented_input, augmented_target = augment_data(input_data, target_displacements, noise_std)\n",
    "        augmented_input_data.extend(augmented_input)\n",
    "        augmented_target_displacements.extend(augmented_target)\n",
    "\n",
    "    adj_matrices = compute_adjacency_matrices(augmented_input_data, rcut=3.0) #[torch.randint(0, 2, (num_atoms[i], num_atoms[i])).float() for i in range(num_snapshots)]  # Random adjacency matrices for each snapshot\n",
    "\n",
    "\n",
    "\n",
    "    # Concatenate input data along a new dimension to form a single tensor\n",
    "    input_data_tensor = torch.stack(augmented_input_data)\n",
    "    print('input_data_tensor.shape:',input_data_tensor.shape)\n",
    "\n",
    "    # Standardize the augmented input data\n",
    "    mean = input_data_tensor.mean(dim=(0, 1))\n",
    "    std = input_data_tensor.std(dim=(0, 1))\n",
    "    standardized_input_data = [standardize_data(data, mean, std) for data in augmented_input_data]\n",
    "\n",
    "\n",
    "    # Convert input data to tensors\n",
    "    #input_data_tensor = torch.stack(augmented_input_data)\n",
    "    target_displacements_tensor = torch.stack(augmented_target_displacements)\n",
    "\n",
    "    total_loss_hist = []\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_displacements = model(input_data_tensor, adj_matrices)\n",
    "    #    predicted_displacements_tensor = torch.stack( predicted_displacements )\n",
    "        losses = []\n",
    "        for indx, i in enumerate(snapshots):\n",
    "            pred = predicted_displacements[ indx ][ indx ]\n",
    "            snapshot_losses = criterion(pred, augmented_target_displacements[indx])\n",
    "    #        snapshot_losses = [criterion(pred, augmented_target_displacements[indx]) for pred in predicted_displacements[indx]]\n",
    "    #        pdb.set_trace()\n",
    "            losses.append(snapshot_losses)\n",
    "    #        losses.extend(snapshot_losses)\n",
    "    #    loss = criterion(predicted_displacements_tensor, target_displacements_tensor)\n",
    "        total_loss = sum(losses)\n",
    "        total_loss.backward()\n",
    "    #    loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_hist += [total_loss.detach().numpy()]\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Total Loss: {total_loss.item()}')\n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "39cf9e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in model.parameters():\n",
    "#     print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "514e9b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = plt.figure(figsize=(10,10)).add_subplot(projection='3d',)\n",
    "\n",
    "# irun=6\n",
    "# xyz = input_data_tensor[irun] \n",
    "# adj_mat = adj_matrices[irun]\n",
    "\n",
    "\n",
    "# ax.plot(xyz[:,0],xyz[:,1],xyz[:,2],\n",
    "#         '.', ms=20,\n",
    "#         )\n",
    "\n",
    "# n=adj_mat.shape[0]\n",
    "# for i,item in enumerate(adj_mat):\n",
    "#     for j in range(i,n):\n",
    "#         if item[j]:\n",
    "#             ax.plot([xyz[i,0],xyz[j,0]],[xyz[i,1],xyz[j,1]],[xyz[i,2],xyz[j,2]],\n",
    "#                     '-', color='black',\n",
    "#                     )\n",
    "\n",
    "# u = augmented_target_displacements[irun][-1]\n",
    "\n",
    "# ax.plot([xyz[-1,0],u[0]],[xyz[-1,1],u[1]],[xyz[-1,2],u[2]],\n",
    "#         '-', color='red',\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85754c6",
   "metadata": {},
   "source": [
    "## mpnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "966d3598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# class MPNN(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super(MPNN, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "#         self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "#     def message_passing_layer(self, x, adj_matrices):\n",
    "#         # Ensure adj_matrices is a list of tensors\n",
    "#         adj_matrices = [adj.unsqueeze(0) for adj in adj_matrices]\n",
    "\n",
    "#         # Expand node features to include neighboring node features\n",
    "# #        expanded_x = [torch.matmul(adj.unsqueeze(0), x.unsqueeze(0)).squeeze(0) for adj in adj_matrices]\n",
    "#         expanded_x = [torch.matmul(adj,yy) for adj, yy in zip(adj_matrices,x)]\n",
    "# #        expanded_x = [torch.matmul(adj.unsqueeze(0), adj).squeeze(0) for adj in adj_matrices]\n",
    "\n",
    "#         # Concatenate node features with neighboring node features\n",
    "#         pdb.set_trace()\n",
    "#         concatenated_x = [torch.cat((x, exp_x), dim=1) for exp_x in expanded_x]\n",
    "\n",
    "#         # Apply linear transformation\n",
    "#         transformed_x = [self.fc1(cat_x) for cat_x in concatenated_x]\n",
    "\n",
    "#         # Apply activation function\n",
    "#         x = [F.relu(trans_x) for trans_x in transformed_x]\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "#     def readout_layer(self, x):\n",
    "#         # Concatenate tensors in the list along the batch dimension\n",
    "#         concatenated_x = torch.stack(x, dim=0)\n",
    "\n",
    "#         # Apply global pooling operation (e.g., mean or sum) along the batch dimension\n",
    "#         return torch.mean(concatenated_x, dim=0)\n",
    "\n",
    "#     def forward(self, x, adj_matrices):\n",
    "#         x = self.message_passing_layer(x, adj_matrices)\n",
    "#         output = self.readout_layer(x)\n",
    "#         return output\n",
    "\n",
    "# # Example usage\n",
    "# input_dim = 3  # Dimensionality of atom positions (e.g., x, y, z coordinates)\n",
    "# hidden_dim = 64  # Dimensionality of hidden layers\n",
    "# output_dim = 3  # Dimensionality of displacement vectors for each snapshot\n",
    "\n",
    "# # Create model instance\n",
    "# model = MPNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# # Define optimizer and loss function\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1.0e-4)\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# # Example training data\n",
    "# # Assuming input_data is a list of tensors containing initial positions for each snapshot\n",
    "# # Assuming adj_matrices is a list of adjacency matrices for each snapshot\n",
    "# # Assuming target_displacements is a list of tensors containing displacement vectors for each snapshot\n",
    "# # You need to replace these with your actual data\n",
    "# input_data = [torch.randn(10, input_dim).float() for _ in range(10)]  # Example random initial positions\n",
    "# adj_matrices = [torch.randint(0, 2, (10, 10)).float() for _ in range(10)]  # Example random adjacency matrices\n",
    "# target_displacements = [torch.randn(10, output_dim) for _ in range(10)]  # Example random target displacements\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 1000\n",
    "# for epoch in range(num_epochs):\n",
    "#     optimizer.zero_grad()\n",
    "#     predicted_displacements = model(input_data, adj_matrices)\n",
    "#     loss = criterion(predicted_displacements, target_displacements)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     if epoch % 100 == 0:\n",
    "#         print(f'Epoch {epoch}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c0dbc8",
   "metadata": {},
   "source": [
    "## tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b7858caf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "tf.GradientTape.gradients() does not support graph control flow operations like tf.cond or tf.while at this time. Use tf.gradients() instead. If you need this feature, please file a feature request at https://github.com/tensorflow/tensorflow/issues/new",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-ef97345b435c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/gnnEnv/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/gnnEnv/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/gnnEnv/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/gnnEnv/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_grad.py\u001b[0m in \u001b[0;36m_ExitGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    149\u001b[0m   \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m   \u001b[0mop_ctxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_control_flow_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m   \u001b[0mgrad_ctxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_control_flow_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/gnnEnv/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_get_control_flow_context\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_control_flow_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     raise NotImplementedError(\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;34m\"tf.GradientTape.gradients() does not support graph control flow \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"operations like tf.cond or tf.while at this time. Use tf.gradients() \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;34m\"instead. If you need this feature, please file a feature request at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: tf.GradientTape.gradients() does not support graph control flow operations like tf.cond or tf.while at this time. Use tf.gradients() instead. If you need this feature, please file a feature request at https://github.com/tensorflow/tensorflow/issues/new"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class GraphNeuralNetwork(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(GraphNeuralNetwork, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "        self.fc_out = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, x, adj_matrices):\n",
    "        # x: Node features (batch_size, num_nodes, input_dim)\n",
    "        # adj_matrices: Adjacency matrices (batch_size, num_nodes, num_nodes)\n",
    "        \n",
    "        # Apply first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # Iterate over adjacency matrices and update node features\n",
    "        for adj_matrix in adj_matrices:\n",
    "            x = tf.matmul(adj_matrix, x)\n",
    "            x = self.fc2(x)\n",
    "        \n",
    "        # Apply output fully connected layer\n",
    "        output = self.fc_out(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "def compute_adjacency_matrices(input_data, rcut):\n",
    "    adj_matrices = []\n",
    "    \n",
    "    for positions in input_data:\n",
    "        num_atoms = positions.shape[0]\n",
    "        adj_matrix = np.zeros(num_atoms*num_atoms).reshape((num_atoms, num_atoms))\n",
    "        \n",
    "        for i in range(num_atoms):\n",
    "            for j in range(i + 1, num_atoms):\n",
    "                distance = torch.norm(positions[i] - positions[j])\n",
    "                if distance <= rcut:\n",
    "                    adj_matrix[i, j] = 1\n",
    "                    adj_matrix[j, i] = 1\n",
    "            assert adj_matrix[i,:].sum() > 0, 'dangling node : increase the cutoff!'\n",
    "        adj_matrices.append(adj_matrix)\n",
    "    \n",
    "    #--- assert no \n",
    "    return np.c_[adj_matrices]\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    hidden_dim = 32  # Dimensionality of hidden layers\n",
    "    output_dim = 3  # Dimensionality of output vectors\n",
    "    snapshots = range(len(transition_paths))\n",
    "\n",
    "    model = GraphNeuralNetwork(hidden_dim, output_dim)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    # Generate dummy input data and adjacency matrices\n",
    "#     batch_size = 2\n",
    "#     num_nodes = 10\n",
    "    input_dim = 3\n",
    "\n",
    "    #x = tf.random.normal((batch_size, num_nodes, input_dim))\n",
    "    x = np.c_[[np.c_[pd.DataFrame(transition_paths[ i ])['x y z'.split()]] for i in snapshots]]\n",
    "    x = tf.convert_to_tensor(x,dtype=tf.float32)\n",
    "\n",
    "    #adj_matrices = tf.random.uniform((batch_size, num_nodes, num_nodes)) # for _ in range(2)]  # Example with 2 adjacency matrices\n",
    "    input_data = [torch.from_numpy( np.c_[pd.DataFrame(transition_paths[ i ])['x y z'.split()]] ).float() for i in snapshots]  \n",
    "    adj_matrices = compute_adjacency_matrices(torch.stack(input_data), rcut=3.0)\n",
    "    adj_matrices = tf.convert_to_tensor(adj_matrices,dtype=tf.float32)\n",
    "\n",
    "    # Example target data\n",
    "    #target = tf.random.normal((batch_size, num_nodes, output_dim))\n",
    "    target = np.c_[[np.c_[pd.DataFrame(transition_paths[ i ])['ux_fin uy_fin uz_fin'.split()]] for i in snapshots]]\n",
    "    target = tf.convert_to_tensor(target,dtype=tf.float32)\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    total_loss_hist = []\n",
    "    num_epochs = 100000\n",
    "    for epoch in range(num_epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(x, adj_matrices)\n",
    "    #         pdb.set_trace()\n",
    "            loss = loss_fn(target, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "\n",
    "        if epoch % 10 == 0: \n",
    "            total_loss_hist.append(loss.numpy())\n",
    "            print('Epoch %s, Loss: %e'%(epoch,loss.numpy()))\n",
    "    \n",
    "    return model, num_epochs, total_loss_hist\n",
    "\n",
    "model, num_epochs, total_loss_hist = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2ce5ea0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: png: File exists\r\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!mkdir png\n",
    "utl.PltErr(range(num_epochs),total_loss_hist,\n",
    "          xscale='log',yscale='log',\n",
    "           title='png/loss.png'\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "be7217a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_prediction(model, input_data, adj_matrices, target_displacements):\n",
    "    ax = utl.PltErr(None,None,Plot=False)\n",
    "    predicted_displacements = model(input_data, adj_matrices)\n",
    "    for i_snapshot,u_pred in enumerate(predicted_displacements):\n",
    "        \n",
    "        u_act  = target_displacements[i_snapshot]\n",
    "\n",
    "        colors='black red green'.split()\n",
    "        for idime in range(3):\n",
    "            utl.PltErr(u_act[:,idime],u_pred[:,idime],\n",
    "                   attrs={'fmt':'x','color':colors[idime]},\n",
    "                  ax=ax, Plot=False,\n",
    "                  )\n",
    "\n",
    "        utl.PltErr( None,None,\n",
    "                   Plot=False,\n",
    "        ax=ax,\n",
    "                #xlim=(-2,2),ylim=(-2,2),\n",
    "                   title='png/disp.png'\n",
    "                  )\n",
    "\n",
    "\n",
    "snapshots = range(len(transition_paths))\n",
    "x = np.c_[[np.c_[pd.DataFrame(transition_paths[ i ])['x y z'.split()]] for i in snapshots]]\n",
    "x = tf.convert_to_tensor(x,dtype=tf.float32)\n",
    "\n",
    "#adj_matrices = tf.random.uniform((batch_size, num_nodes, num_nodes)) # for _ in range(2)]  # Example with 2 adjacency matrices\n",
    "input_data = [torch.from_numpy( np.c_[pd.DataFrame(transition_paths[ i ])['x y z'.split()]] ).float() for i in snapshots]  \n",
    "adj_matrices = compute_adjacency_matrices(torch.stack(input_data), rcut=3.0)\n",
    "adj_matrices = tf.convert_to_tensor(adj_matrices,dtype=tf.float32)\n",
    "\n",
    "# Example target data\n",
    "#target = tf.random.normal((batch_size, num_nodes, output_dim))\n",
    "target = np.c_[[np.c_[pd.DataFrame(transition_paths[ i ])['ux_fin uy_fin uz_fin'.split()]] for i in snapshots]]\n",
    "target = tf.convert_to_tensor(target,dtype=tf.float32)\n",
    "\n",
    "\n",
    "make_prediction(model, x, adj_matrices, target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50fcb35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "269.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
