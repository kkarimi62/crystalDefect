{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cded12cf",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#import-libs\" data-toc-modified-id=\"import-libs-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>import libs</a></span></li><li><span><a href=\"#Train-NN\" data-toc-modified-id=\"Train-NN-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Train NN</a></span><ul class=\"toc-item\"><li><span><a href=\"#main():-classifier\" data-toc-modified-id=\"main():-classifier-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>main(): classifier</a></span></li><li><span><a href=\"#main():-regressor\" data-toc-modified-id=\"main():-regressor-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>main(): regressor</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot\" data-toc-modified-id=\"Plot-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Plot</a></span></li></ul></li><li><span><a href=\"#test-example:-2d\" data-toc-modified-id=\"test-example:-2d-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>test example: 2d</a></span><ul class=\"toc-item\"><li><span><a href=\"#fully-connected-in-sklearn\" data-toc-modified-id=\"fully-connected-in-sklearn-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>fully connected in sklearn</a></span></li><li><span><a href=\"#fully-connected-in-keras\" data-toc-modified-id=\"fully-connected-in-keras-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>fully connected in keras</a></span></li><li><span><a href=\"#cnn\" data-toc-modified-id=\"cnn-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>cnn</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117638d",
   "metadata": {},
   "source": [
    "# import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49343b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf. file sections: ['flags', 'input files', 'EnergyBarrier', 'neural net', 'neural net classification', 'neural net regression', 'ml mc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'LammpsPostProcess' from '../../HeaDef/postprocess/LammpsPostProcess.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "confParser = configparser.ConfigParser()\n",
    "\n",
    "#--- parse conf. file\n",
    "confParser.read('configuration.ini')\n",
    "print('conf. file sections:',confParser.sections())\n",
    "#\n",
    "import os\n",
    "import sys\n",
    "list(map(lambda x:sys.path.append(x), confParser['input files']['lib_path'].split()))\n",
    "from dscribe.descriptors import SOAP, ACSF\n",
    "import ase\n",
    "import ase.io\n",
    "import ase.build\n",
    "from ase.io import lammpsdata\n",
    "import pdb\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import matplotlib.pyplot as plt\n",
    "if not eval(confParser['flags']['RemoteMachine']):\n",
    "    plt.rc('text', usetex=True)\n",
    "#\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#\n",
    "from scipy.stats import gaussian_kde\n",
    "#\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#--- user modules\n",
    "import LammpsPostProcess as lp\n",
    "import utility as utl\n",
    "import imp\n",
    "imp.reload(utl)\n",
    "imp.reload(lp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a2e60",
   "metadata": {},
   "source": [
    "# Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd8c31d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "\n",
    "    def __init__(self, verbose=False,\n",
    "                **kwargs\n",
    "                ):\n",
    "        self.verbose = verbose\n",
    "        for key in kwargs:\n",
    "            setattr(self,key,kwargs[key])\n",
    "    \n",
    "    def Parse(self,path,nruns):\n",
    "        self.perAtomData = {}\n",
    "        rwjs = utl.ReadWriteJson()\n",
    "        for irun in range(nruns):\n",
    "            if irun == 0:\n",
    "                #--- same configurations!\n",
    "                self.descriptors  = np.c_[rwjs.Read('%s/Run%s/descriptors.json'%(path,irun))[0]['data']]\n",
    "                self.shape        = np.c_[rwjs.Read('%s/Run%s/descriptors.json'%(path,irun))[0]['shape']].flatten()\n",
    "                self.positions    = np.c_[rwjs.Read('%s/Run%s/descriptors.json'%(path,irun))[0]['xyz']]\n",
    "                os.system('ln -s %s/Run%s/dumpFile/dump.xyz .'%(path,irun))\n",
    "            try:\n",
    "                data = np.loadtxt('%s/Run%s/perAtomData.txt'%(path,irun))\n",
    "                #--- displacement data\n",
    "                self.perAtomData[irun] = pd.DataFrame(np.c_[data],\n",
    "                columns='id\ttype\tx\ty\tz\tux\tuy\tuz\tenergy_barrier\tdefect_label'.split()\n",
    "                            )\n",
    "            except:\n",
    "#                 if self.verbose:\n",
    "#                     traceback.print_exc()\n",
    "                continue\n",
    "                \n",
    "        \n",
    "        self.nruns = list(self.perAtomData.keys())\n",
    "        self.nruns.sort()\n",
    "        \n",
    "    def Combine(self):\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('concatenating descriptors ...')\n",
    "#         pdb.set_trace()\n",
    "        #--- center atoms\n",
    "        center_atom_indices = list(map(lambda x:NeuralNetwork.GetCenterAtom( self.perAtomData[x])[0],self.nruns))\n",
    "        sdict = dict(zip(center_atom_indices,self.nruns))\n",
    "        \n",
    "        atom_ids = list(sdict.keys())\n",
    "        atom_ids.sort()\n",
    "        #         center_atom_indices = list( set( center_atom_indices ) )\n",
    "        data = np.concatenate(list(map(lambda x: np.c_[self.perAtomData[sdict[x]].iloc[ x ]],atom_ids)),axis=1).T\n",
    "        descriptors_center_atoms = self.descriptors[atom_ids]\n",
    "    \n",
    "        #--- data frame\n",
    "#        print(data.shape)\n",
    "        irun = self.nruns[0]\n",
    "        df_combined = pd.DataFrame(data,columns=list(self.perAtomData[irun].keys()))\n",
    "    \n",
    "        #--- filter crystaliine atoms\n",
    "        filtr = self.perAtomData[irun].defect_label == 0.0\n",
    "        df_crystalline = self.perAtomData[irun][filtr]\n",
    "        descriptors_crystalline = self.descriptors[filtr]\n",
    "\n",
    "        #--- merge\n",
    "        keys = list(df_combined.keys())\n",
    "        data_concat = np.concatenate([np.c_[df_combined[keys]],np.c_[df_crystalline[keys]]],axis=0) \n",
    "        self.perAtomData = pd.DataFrame(data_concat,\n",
    "                              columns=keys\n",
    "                             )\n",
    "\n",
    "        \n",
    "        #--- merge descriptors\n",
    "        self.descriptors = np.concatenate([descriptors_center_atoms,descriptors_crystalline],axis=0)\n",
    "\n",
    "        assert self.perAtomData.shape[ 0 ] == self.descriptors.shape[0], 'need more mc swaps: %s %s'\\\n",
    "        %(self.perAtomData.shape[ 0 ],self.descriptors.shape[0])\n",
    "                            \n",
    "                                 \n",
    "    \n",
    "    @staticmethod\n",
    "    def GetCenterAtom(df):\n",
    "        disp_magnitude = df.ux**2+df.uy**2+df.uz**2\n",
    "        center_atom_indx = disp_magnitude.sort_values(ascending=False).index[0]\n",
    "        return center_atom_indx, int(df.iloc[ center_atom_indx ].id)\n",
    "\n",
    "    @staticmethod\n",
    "    def zscore(slist):\n",
    "        tmp = np.copy(slist)\n",
    "        print(np.mean(tmp),np.std(tmp))\n",
    "        tmp -= np.mean(tmp)\n",
    "        tmp /= np.std(tmp)\n",
    "        return tmp\n",
    "\n",
    "    def PCA(self,\n",
    "           n_components=2,\n",
    "            random_state = 1,\n",
    "           ):\n",
    "        #--- concat. data\n",
    "        X = self.descriptors\n",
    "        pca = PCA(n_components=n_components,random_state=random_state)\n",
    "        pca.fit(X)\n",
    "        X_transformed = pca.transform(X)\n",
    "\n",
    "        xdata = X_transformed[:,0]\n",
    "        ydata = X_transformed[:,1]\n",
    "        #\n",
    "        filtr_defects = self.perAtomData.defect_label == 0.0\n",
    "        #\n",
    "\n",
    "        legend = utl.Legends()\n",
    "        legend.Set(bbox_to_anchor=(1.1,.5, 0.5, 0.5))\n",
    "#         pdb.set_trace()\n",
    "        #ax = utl.PltErr(zscore(xdata)[filtr_defects],zscore(ydata)[filtr_defects],\n",
    "        ax = utl.PltErr(xdata[filtr_defects],ydata[filtr_defects],\n",
    "                  attrs={'fmt':'x','alpha':1,'label':'defect_free'},\n",
    "                        Plot = False,\n",
    "        #                 xlim=(-2,2),\n",
    "        #                 ylim=(-2,2),\n",
    "                  )\n",
    "\n",
    "        #utl.PltErr(zscore(xdata)[~filtr_defects],zscore(ydata)[~filtr_defects],\n",
    "        !mkdir png\n",
    "        utl.PltErr(xdata[~filtr_defects],ydata[~filtr_defects],\n",
    "                  attrs={'fmt':'.','color':'red','label':'defects'},\n",
    "                   ax=ax,\n",
    "                   xstr='pca_1',ystr='pca_2',\n",
    "                   legend = legend.Get(),\n",
    "                   title='png/pca.png'\n",
    "                  )\n",
    "    def Spectra(self,\n",
    "               nrows=100,\n",
    "               ):\n",
    "        assert nrows <= self.descriptors.shape[ 0 ]\n",
    "        !mkdir png\n",
    "        utl.PltBitmap(np.log10(np.abs(self.descriptors[:nrows,:])),\n",
    "                      xlabel=r'$\\mathrm{ndim}$',ylabel=r'$\\mathrm{natom}$',\n",
    "                      xlim=(0,self.descriptors.shape[1]),\n",
    "                      ylim=(0,nrows),\n",
    "                      colorbar=True,\n",
    "                      zscore=False,\n",
    "                      vminmax=(-3,3),\n",
    "                      title='png/feature_bitmap.png'\n",
    "                     )\n",
    "        \n",
    "    def SklearnMLP(self,X_train,y_train):\n",
    "        #-----------------------\n",
    "        #--- parameter grid\n",
    "        #-----------------------\n",
    "#         param_grid = {\n",
    "#                         'hidden_layer_sizes':self.hidden_layer_sizes,\n",
    "#                          #'activation' : ['tanh', 'relu'],\n",
    "#                          'learning_rate_init':self.learning_rate_init,\n",
    "# #                         'alpha':self.alpha, #--- regularization \n",
    "#                          #'learning_rate' : ['invscaling', 'adaptive'],\n",
    "#                         'n_iter_no_change':self.n_iter_no_change,\n",
    "# #                        'tol':self.tol,\n",
    "#                         'max_iter':self.max_iter,\n",
    "#                      } \n",
    "        mlp   =  MLPClassifier(random_state=1,\n",
    "                               hidden_layer_sizes = self.hidden_layer_sizes[0],\n",
    "                               learning_rate_init = self.learning_rate_init[0],\n",
    "                               n_iter_no_change   = self.n_iter_no_change[0],\n",
    "                               max_iter           = self.max_iter[0],\n",
    "                               verbose=self.verbose)\n",
    "#         clf  =  GridSearchCV(mlp, param_grid)\n",
    "#        clf.fit(X_train,y_train)\n",
    "        mlp.fit(X_train,y_train)\n",
    "        model =  mlp #clf.best_estimator_\n",
    "        loss  =  model.loss_curve_\n",
    "        val_loss = loss\n",
    "        return (model, loss, val_loss)\n",
    "\n",
    "    def KerasANN(self, X_train, y_train,X_test, y_test, ndime):\n",
    "\n",
    "        model     = keras.Sequential([ #--- The network architecture\n",
    "                                    layers.Dense(self.hidden_layer_size, activation=self.activation),\n",
    "                #                    layers.Dense(self.hidden_layer_size, activation=self.activation),\n",
    "                                    layers.Dense(ndime, activation='softmax')\n",
    "                                    ])\n",
    "        \n",
    "#         shape         =  (self.shape[0]*self.shape[1]*self.shape[2],)\n",
    "#         inputs        =  keras.Input(shape=shape)\n",
    "#         #------------------------------\n",
    "#         #--- The network architecture\n",
    "#         #------------------------------\n",
    "#         x             =  layers.Dense(   self.hidden_layer_size, activation=self.activation\n",
    "#                                        )(inputs)\n",
    "#         for i in range( self.number_hidden_layers ):\n",
    "#             x       = layers.Dense( self.hidden_layer_size, activation=self.activation\n",
    "#                                      )(x)\n",
    "#         #--- output layer\n",
    "# #         x       = layers.Flatten()(x)\n",
    "#         outputs = layers.Dense( ndime, activation=self.activation)(x)\n",
    "#         model   = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        \n",
    "        \n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate_init) #--- compilation step\n",
    "        model.compile( optimizer=optimizer,\n",
    "                       loss=\"sparse_categorical_crossentropy\",\n",
    "                       metrics=[\"mse\"]\n",
    "                     )\n",
    "        #--- save best model\n",
    "        !mkdir best_model\n",
    "#         callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnetClassifier_from_scratch.tf',  \n",
    "#                                                     monitor=\"mse\",\n",
    "#                                                     save_freq=10,\n",
    "#                                                     save_best_only=True)]\n",
    "\n",
    "        model.fit( X_train, y_train, \n",
    "           validation_data      = ( X_test, y_test ),\n",
    "#             callbacks           = callbacks,\n",
    "            epochs              = self.max_iter[0], \n",
    "            verbose             = self.verbose, \n",
    "            shuffle             = False, \n",
    "#             batch_size     = 32,\n",
    "#                     use_multiprocessing = True,\n",
    "#                     workers             = 4,\n",
    "         )    \n",
    "\n",
    "        model.save('best_model/convnetClassifier_from_scratch.tf')\n",
    "        loss      = model.history.history['loss']\n",
    "        val_loss  = model.history.history['val_loss']\n",
    "        best_model = model #keras.models.load_model(\"best_model/convnetClassifier_from_scratch.tf\")\n",
    "\n",
    "        return (best_model, loss, val_loss)\n",
    "                             \n",
    "    def TrainClassifier(self,y,\n",
    "                       random_state=1,\n",
    "                       ):\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "\n",
    "        ndime = 2 #--- binary classification\n",
    "        #---------------\n",
    "        #--- zscore X\n",
    "        #---------------        \n",
    "        X      = np.c_[self.descriptors]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "        X      = scaler.transform( X )\n",
    "\n",
    "        #--- train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                            random_state=random_state)\n",
    "\n",
    "        #-----------------------\n",
    "        #--- train model\n",
    "        #-----------------------\n",
    "        if self.fully_connected: #--- dense nn\n",
    "            if self.implementation == 'sklearn':\n",
    "                (model, loss, val_loss) = self.SklearnMLP(X_train,y_train)\n",
    "                classes_x = model.predict(X_test) \n",
    "                \n",
    "            elif self.implementation == 'keras': #--- dense nn in keras\n",
    "                (model, loss, val_loss) = self.KerasANN(X_train, y_train,X_test, y_test, ndime)\n",
    "                predict_x = model.predict(X_test) \n",
    "                classes_x = np.argmax(predict_x,axis=1)\n",
    "                \n",
    "        elif self.cnn: #--- convolutional\n",
    "            (model, loss, val_loss), (X_train, X_test) =\\\n",
    "            self.ConvNetworkClassifier( y )\n",
    "            predict_x = model.predict(X_test) \n",
    "            classes_x = np.argmax(predict_x,axis=1)\n",
    "                    \n",
    "        #--- save loss data\n",
    "        !mkdir png\n",
    "        np.savetxt('png/val_loss_classification.txt',\n",
    "                   np.c_[range(len(loss)),loss,val_loss],\n",
    "                   header='epoch loss val_loss')\n",
    "\n",
    "        #--- confusion matrix\n",
    "        cm = confusion_matrix(y_test, classes_x,\n",
    "                         labels=[0, 1]\n",
    "                        )\n",
    "        np.savetxt('png/confusion.txt',np.c_[cm])\n",
    "\n",
    "        \n",
    "    def PrintDescriptors(self,descriptors,y,fout):\n",
    "        rwjs = utl.ReadWriteJson()\n",
    "        rwjs.Write([{'descriptors':np.c_[descriptors],\n",
    "                     'target':np.c_[y],\n",
    "                     'shape_descriptor':self.shape}],fout)\n",
    "        \n",
    "\n",
    "    def TrainRegressor(self,stratify,y,\n",
    "                       random_state=1,\n",
    "                       printOvito = False,\n",
    "                       filtr = None,\n",
    "                       ):\n",
    "        '''\n",
    "        Multi-layer Perceptron regressor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        stratify : array-like, default=None\n",
    "        If not None, data is split in a stratified fashion, using this as\n",
    "        the class labels.\n",
    "        \n",
    "        y : array-like, target data\n",
    "        \n",
    "        random_state : initial seed, default=1\n",
    "        \n",
    "        printOvito : bool, default=False\n",
    "        \n",
    "        filtr : bool, default=False\n",
    "        if not None, data is filtered before calling train-test split\n",
    "        '''\n",
    "        if self.verbose:\n",
    "            print('dim(y)=',y.shape)\n",
    "\n",
    "        ndime  = y.shape[1] #--- dimension of the target vector\n",
    "        \n",
    "        #---------------\n",
    "        #--- zscore X\n",
    "        #---------------        \n",
    "        X      = np.c_[self.descriptors[filtr]]\n",
    "        self.PrintDescriptors(X,y,'ml_kmc_dataset.json')\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "        X      = scaler.transform( X )\n",
    "        \n",
    "        #--- add noise\n",
    "        X_augmented, y_augmented = NeuralNetwork.AddGaussianNoise(X,y,scale = 0.1, new_size = 100 )\n",
    "        \n",
    "\n",
    "        #-----------------------\n",
    "        #--- train-test split\n",
    "        #-----------------------\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_augmented, y_augmented, stratify=stratify,\n",
    "                                                            random_state=random_state)\n",
    "\n",
    "\n",
    "        #-----------------------\n",
    "        #--- train model\n",
    "        #-----------------------\n",
    "        if self.fully_connected: #--- dense nn\n",
    "            if self.implementation == 'sklearn':\n",
    "                #-----------------------\n",
    "                #--- parameter grid\n",
    "                #-----------------------\n",
    "                param_grid = {\n",
    "                                'hidden_layer_sizes':self.hidden_layer_sizes,\n",
    "                                 #'activation' : ['tanh', 'relu'],\n",
    "                                 'learning_rate_init':self.learning_rate_init,\n",
    "                                'alpha':self.alpha, #--- regularization \n",
    "                                 #'learning_rate' : ['invscaling', 'adaptive'],\n",
    "                                'n_iter_no_change':self.n_iter_no_change,\n",
    "                                'tol':self.tol,\n",
    "                                'max_iter':self.max_iter,\n",
    "                             } \n",
    "                mlp   =  MLPRegressor(random_state=random_state,verbose=self.verbose) #--- mlp regressor\n",
    "                regr  =  GridSearchCV(mlp, param_grid)\n",
    "                regr.fit(X_train,y_train)\n",
    "                model =  regr.best_estimator_\n",
    "                loss  =  model.loss_curve_\n",
    "                \n",
    "            elif self.implementation == 'keras': #--- dense nn in keras\n",
    "                model     = keras.Sequential([ #--- The network architecture\n",
    "                    layers.Dense(self.hidden_layer_size, activation=self.activation),\n",
    "#                    layers.Dense(self.hidden_layer_size, activation=self.activation),\n",
    "                    layers.Dense(ndime, activation='softmax')\n",
    "                    ])\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_init) #--- compilation step\n",
    "                model.compile( optimizer=optimizer,#\"rmsprop\",\n",
    "                               loss=\"mean_squared_error\",#\"sparse_categorical_crossentropy\",\n",
    "                               metrics=[\"mse\"]\n",
    "                             )\n",
    "                model.fit(X_train, y_train, #--- “Fitting”\n",
    "                          validation_data=(X_test, y_test),\n",
    "                          epochs=self.max_iter[0], verbose=self.verbose, batch_size=1)\n",
    "                loss      = model.history.history['loss']\n",
    "                val_loss  = model.history.history['val_loss']\n",
    "                \n",
    "        elif self.cnn: #--- convolutional\n",
    "            (model, loss, val_loss), (X_train, X_test) =\\\n",
    "            self.ConvNetwork(X_train, y_train, X_test, y_test )\n",
    "            \n",
    "\n",
    "        #-----------------------\n",
    "        #--- validation\n",
    "        #-----------------------\n",
    "        !mkdir png         #--- plot validation loss \n",
    "        ax = utl.PltErr(range(len(val_loss)), val_loss,\n",
    "                   attrs={'fmt':'-'}, Plot=False,\n",
    "                  )\n",
    "        utl.PltErr(range(len(loss)), loss,\n",
    "                   attrs={'fmt':'-'},\n",
    "                   ax=ax,\n",
    "                   yscale='log',xscale='log',\n",
    "                   xlim=(1,self.max_iter[0]),\n",
    "                   xstr='epoch',ystr='loss',\n",
    "                   title='png/loss.png',\n",
    "                  )\n",
    "        \n",
    "        np.savetxt('png/loss.txt',np.c_[range(len(loss)),loss,val_loss],header='epoch loss val_loss')\n",
    "        \n",
    "        #--- plot predictions\n",
    "        y_pred       = model.predict(X_test)        \n",
    "        y_pred_train = model.predict(X_train)        \n",
    "        for idime, xstr in zip(range(3),'ux uy uz'.split()):\n",
    "            ax = utl.PltErr(None,None,Plot=False)\n",
    "            #\n",
    "            utl.PltErr(y_test[:,idime],y_pred[:,idime],\n",
    "                       attrs={'fmt':'x','color':'red','zorder':10,'markersize':6},\n",
    "                       ax=ax,\n",
    "                       Plot = False,\n",
    "\n",
    "                      )\n",
    "            #\n",
    "            utl.PltErr(y_train[:,idime],y_pred_train[:,idime],\n",
    "                       attrs={'fmt':'.','color':'blue','zorder':1,'markersize':6},\n",
    "                       ax=ax,\n",
    "                       Plot = False,\n",
    "\n",
    "                      )\n",
    "            #\n",
    "            utl.PltErr(None,None,Plot=False,\n",
    "                           title='png/scatter%s.png'%idime,\n",
    "                            ax=ax,\n",
    "                       xstr='%s actual'%xstr,ystr='%s predicted'%xstr,\n",
    "                       xlim=(-2,2),ylim=(-2,2),\n",
    "                           )\n",
    "        \n",
    "        #--- save in ovito\n",
    "        if printOvito:\n",
    "            X_train, X_test, _, _ = train_test_split(self.perAtomData[filtr]['id type x y z'.split()], y, stratify=stratify,\n",
    "                                                    random_state=random_state)\n",
    "#            pdb.set_trace()\n",
    "            with open('original.xyz','w') as fp:\n",
    "                utl.PrintOvito(self.perAtomData[filtr], fp, '0', attr_list='id type x y z ux uy uz'.split())\n",
    "            with open('train.xyz','w') as fp:\n",
    "                cordc = pd.DataFrame(np.c_[X_train,y_pred_train],columns='id type x y z ux uy uz'.split())\n",
    "                utl.PrintOvito(cordc, fp, '0', attr_list='id type x y z ux uy uz'.split())\n",
    "            with open('test.xyz','w') as fp:\n",
    "                cordc = pd.DataFrame(np.c_[X_test,y_pred],columns='id type x y z ux uy uz'.split())\n",
    "                utl.PrintOvito(cordc, fp, '0', attr_list='id type x y z ux uy uz'.split())\n",
    "                \n",
    "\n",
    "    def ConvNetwork(self,X_train, y_train, X_test, y_test):\n",
    "        '''\n",
    "        Convolutional neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : array-like training x input\n",
    "        \n",
    "        y_train : array-like, training y input\n",
    "        \n",
    "        X_test : array-like test x input\n",
    "        \n",
    "        y_test : array-like, training y input\n",
    "\n",
    "        Return\n",
    "        ---------- ( , loss,  )\n",
    "        best_model : cnn object, best trained model based on on the validation loss\n",
    "        \n",
    "        loss : array-like, mse loss\n",
    "\n",
    "        val_loss : array-like, validation loss\n",
    "\n",
    "        '''\n",
    "#         tf.random.set_random_seed(812)\n",
    "\n",
    "        shape         =  (self.shape[0],self.shape[1],self.shape[2],1) #--- rows, cols, thickness, channels\n",
    "        kernel_size   =  self.kernel_size \n",
    "        epochs        =  self.max_iter[0]\n",
    "        activation    =  self.activation\n",
    "        padding       = 'same'\n",
    "        filters       =  self.n_channels\n",
    "        learning_rate = self.learning_rate_init[0]\n",
    "        #\n",
    "        ndime         =  y_train.shape[1]\n",
    "        n_train       =  X_train.shape[0]\n",
    "        n_test        =  X_test.shape[0]\n",
    "        assert        shape[0] * shape[1] * shape[2] == X_train.shape[ 1 ]\n",
    "        inputs        =  keras.Input(shape=shape)\n",
    "        #\n",
    "\n",
    "        #------------------------------\n",
    "        #--- The network architecture\n",
    "        #------------------------------\n",
    "        x             =  layers.Conv3D(   filters     =  filters, \n",
    "                                          kernel_size =  kernel_size,\n",
    "                                          activation  =  activation,\n",
    "                                          padding     =  padding\n",
    "                                       )(inputs)\n",
    "        filters       *=  2\n",
    "        for i in range( self.number_hidden_layers ):\n",
    "            x       = layers.AveragePooling3D( pool_size = 2 )( x )\n",
    "            x       = layers.Conv3D( filters       =  filters, \n",
    "                                     kernel_size   =  kernel_size,\n",
    "                                     activation    =  activation,\n",
    "                                     padding       =  padding\n",
    "                                     )(x)\n",
    "            filters *= 2\n",
    "            \n",
    "        #--- output layer\n",
    "        x       = layers.Flatten()(x)\n",
    "        outputs = layers.Dense( ndime, activation=activation)(x)\n",
    "        model   = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        if self.verbose:\n",
    "            print('cnn model summary:',model.summary())\n",
    "\n",
    "        #--- The compilation step\n",
    "        optimizer = tf.keras.optimizers.Adam( learning_rate = learning_rate )\n",
    "        model.compile( optimizer =  optimizer,\n",
    "                       loss      =  \"mean_squared_error\",\n",
    "                       metrics   =  [\"mse\"]\n",
    "                     )\n",
    "\n",
    "        #--- save best model\n",
    "        !mkdir best_model\n",
    "#         callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnet_from_scratch.tf',  \n",
    "#                                                    monitor=\"loss\",\n",
    "#                                                   save_freq=10,\n",
    "#                                                     save_best_only=True)]\n",
    "\n",
    "        #--- “Fitting” the model X_train_transfrmd, y_train\n",
    "        X_train_reshaped =  X_train.reshape((n_train,shape[0],shape[1],shape[2],1))\n",
    "        X_test_reshaped  =  X_test.reshape((n_test,shape[0],shape[1],shape[2],1))\n",
    "        model.fit( X_train_reshaped, y_train, \n",
    "                   validation_data      = ( X_test_reshaped, y_test ),\n",
    "#                    callbacks            = callbacks,\n",
    "                    epochs              = epochs, \n",
    "                    verbose             = self.verbose, \n",
    "                    shuffle             = False, \n",
    "                    batch_size          = 1,\n",
    "#                     batch_size     = 128,\n",
    "#                     use_multiprocessing = True,\n",
    "#                     workers             = 4,\n",
    "                 )\n",
    "\n",
    "        #--- validation loss\n",
    "        model.save('best_model/convnetRegressor_from_scratch.tf')\n",
    "        loss       = model.history.history['loss']\n",
    "        val_loss   = model.history.history['val_loss']\n",
    "        best_model =model #keras.models.load_model(\"best_model/convnet_from_scratch.tf\")\n",
    "\n",
    "        \n",
    "        return ( best_model, loss, val_loss ), (X_train_reshaped, X_test_reshaped)\n",
    "    \n",
    "\n",
    "    def ConvNetworkClassifier(self,y,\n",
    "                               random_state=1\n",
    "                               ):\n",
    "        '''\n",
    "        Convolutional neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : array-like training x input\n",
    "        \n",
    "        y_train : array-like, training y input\n",
    "        \n",
    "        X_test : array-like test x input\n",
    "        \n",
    "        y_test : array-like, training y input\n",
    "\n",
    "        Return\n",
    "        ---------- ( , loss,  )\n",
    "        best_model : cnn object, best trained model based on on the validation loss\n",
    "        \n",
    "        loss : array-like, mse loss\n",
    "\n",
    "        val_loss : array-like, validation loss\n",
    "\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('dim(y)=',y.shape)\n",
    "        \n",
    "\n",
    "        #---------------\n",
    "        #--- zscore X\n",
    "        #---------------        \n",
    "        X      = np.c_[self.descriptors ]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "        X      = scaler.transform( X )\n",
    "    \n",
    "        if self.verbose:\n",
    "            print('X.shape:=',X.shape)\n",
    "            \n",
    "            \n",
    "            \n",
    "        #-----------------------\n",
    "        #--- train-test split\n",
    "        #-----------------------\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                            random_state=random_state)\n",
    "\n",
    "        \n",
    "        \n",
    "        #---- set model parameters\n",
    "        shape         =  (self.shape[0],self.shape[1],self.shape[2],1) #--- rows, cols, thickness, channels\n",
    "        kernel_size   =  self.kernel_size \n",
    "        epochs        =  self.max_iter[0]\n",
    "        activation    =  self.activation\n",
    "        padding       = 'same'\n",
    "        filters       =  self.n_channels\n",
    "        learning_rate = self.learning_rate_init[0]\n",
    "        #\n",
    "        ndime         =  y_train.shape[1]\n",
    "        n_train       =  X_train.shape[0]\n",
    "        n_test        =  X_test.shape[0]\n",
    "        assert        shape[0] * shape[1] * shape[2] == X_train.shape[ 1 ]\n",
    "        inputs        =  keras.Input(shape=shape)\n",
    "\n",
    "        #------------------------------\n",
    "        #--- The network architecture\n",
    "        #------------------------------\n",
    "        x             =  layers.Conv3D(   filters     =  filters, \n",
    "                                          kernel_size =  kernel_size,\n",
    "                                          activation  =  activation,\n",
    "                                          padding     =  padding\n",
    "                                       )(inputs)\n",
    "        filters       *=  2\n",
    "        for i in range( self.number_hidden_layers ):\n",
    "            x       = layers.AveragePooling3D( pool_size = 2 )( x )\n",
    "            x       = layers.Conv3D( filters       =  filters, \n",
    "                                     kernel_size   =  kernel_size,\n",
    "                                     activation    =  activation,\n",
    "                                     padding       =  padding\n",
    "                                     )(x)\n",
    "            filters *= 2\n",
    "            \n",
    "        #--- output layer\n",
    "        x       = layers.Flatten()(x)\n",
    "        outputs = layers.Dense( ndime, activation=activation)(x)\n",
    "        model   = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        if self.verbose:\n",
    "            print('cnn model summary:',model.summary())\n",
    "\n",
    "        #--- The compilation step\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate) #--- compilation step\n",
    "        model.compile( optimizer =  optimizer,\n",
    "                       loss=[\"binary_crossentropy\",\"sparse_categorical_crossentropy\"][1],\n",
    "                       metrics   =  [\"mse\"]\n",
    "                     )\n",
    "\n",
    "        #--- save best model\n",
    "        !mkdir best_model\n",
    "#         callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnetClassifier_from_scratch.tf',  \n",
    "#                                                     monitor=\"accuracy\",\n",
    "#                                                     save_freq=10,\n",
    "#                                                     save_best_only=True)]\n",
    "\n",
    "        #--- “Fitting” the model X_train_transfrmd, y_train\n",
    "        X_train_reshaped =  X_train.reshape((n_train,shape[0],shape[1],shape[2],1))\n",
    "        X_test_reshaped  =  X_test.reshape((n_test,shape[0],shape[1],shape[2],1))\n",
    "        model.fit( X_train_reshaped, y_train, \n",
    "                   validation_data      = ( X_test_reshaped, y_test ),\n",
    "                   callbacks            = callbacks,\n",
    "                    epochs              = epochs, \n",
    "                    verbose             = self.verbose, \n",
    "                    shuffle             = False, \n",
    "#                     batch_size     = 128,\n",
    "#                     use_multiprocessing = True,\n",
    "#                     workers             = 4,\n",
    "                 )\n",
    "\n",
    "        #--- validation loss\n",
    "        model.save('best_model/convnetClassifier_from_scratch.tf')\n",
    "        loss       = model.history.history['loss']\n",
    "        val_loss   = model.history.history['val_loss']\n",
    "        best_model = model #keras.models.load_model(\"best_model/convnetClassifier_from_scratch.tf\")\n",
    "\n",
    "        \n",
    "        return ( best_model, loss, val_loss ), (X_train_reshaped, X_test_reshaped)\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def AddGaussianNoise(X,y,scale = 0.1, new_size = 100 ):\n",
    "        m = X.shape[ 0 ]\n",
    "        n = X.shape[ 1 ]\n",
    "        ndime = y.shape[ 1 ]\n",
    "        augmented_x = np.copy( X )\n",
    "        augmented_y = np.copy( y )\n",
    "\n",
    "        while m < new_size:\n",
    "            epsilon_x = np.random.normal(scale=scale,size=m*n).reshape((m,n))\n",
    "            epsilon_y = np.random.normal(scale=scale,size=m*ndime).reshape((m,ndime))\n",
    "            #\n",
    "            perturbed_x = X + epsilon_x\n",
    "            perturbed_y = y + epsilon_y\n",
    "            #\n",
    "            augmented_x = np.concatenate([augmented_x,perturbed_x],axis = 0)\n",
    "            augmented_y = np.concatenate([augmented_y,perturbed_y],axis = 0)\n",
    "            #\n",
    "            m = augmented_x.shape[ 0 ]\n",
    "\n",
    "        assert m >= new_size\n",
    "\n",
    "        return augmented_x, augmented_y\n",
    "        \n",
    "    \n",
    "    def PrintDensityMap(self, atomIndx, fout):\n",
    "        with open(fout,'w') as fp:\n",
    "                    disp           = np.c_[self.perAtomData.iloc[atomIndx]['ux uy uz'.split()]].flatten()\n",
    "                    df             = pd.DataFrame(np.c_[self.positions.T,self.descriptors[atomIndx]],\n",
    "                                                  columns='x y z mass'.split())\n",
    "                    utl.PrintOvito(df, fp, 'disp = %s'%disp, attr_list='x y z mass'.split())\n",
    "#     def SaveConf(self,fout):\n",
    "#         with open(fout,'w') as fp:\n",
    "#             np.savetxt(fp,np.c_[self.perAtomData],header=' '.join(list(self.perAtomData.keys())))\n",
    "\n",
    "#     def Test(self,y,\n",
    "#                                    random_state=1\n",
    "#                                    ):\n",
    "#             '''\n",
    "#             Convolutional neural network.\n",
    "\n",
    "#             Parameters\n",
    "#             ----------\n",
    "#             X_train : array-like training x input\n",
    "\n",
    "#             y_train : array-like, training y input\n",
    "\n",
    "#             X_test : array-like test x input\n",
    "\n",
    "#             y_test : array-like, training y input\n",
    "\n",
    "#             Return\n",
    "#             ---------- ( , loss,  )\n",
    "#             best_model : cnn object, best trained model based on on the validation loss\n",
    "\n",
    "#             loss : array-like, mse loss\n",
    "\n",
    "#             val_loss : array-like, validation loss\n",
    "\n",
    "#             '''\n",
    "\n",
    "#             if self.verbose:\n",
    "#                 print('dim(y)=',y.shape)\n",
    "\n",
    "#             ndime  = y.shape[1] #--- dimension of the target vector\n",
    "\n",
    "\n",
    "#             #---------------\n",
    "#             #--- zscore X\n",
    "#             #---------------        \n",
    "#             X      = np.c_[self.descriptors ]\n",
    "#             scaler = StandardScaler()\n",
    "#             scaler.fit(X)\n",
    "#             X      = scaler.transform( X )\n",
    "\n",
    "#             if self.verbose:\n",
    "#                 print('X.shape:=',X.shape)\n",
    "\n",
    "\n",
    "\n",
    "#             #-----------------------\n",
    "#             #--- train-test split\n",
    "#             #-----------------------\n",
    "#             X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "#                                                                 random_state=random_state)\n",
    "\n",
    "\n",
    "\n",
    "#             #---- set model parameters\n",
    "#             shape         =  (self.shape[0],self.shape[1],self.shape[2],1) #--- rows, cols, thickness, channels\n",
    "#             kernel_size   =  self.kernel_size \n",
    "#             epochs        =  self.max_iter[0]\n",
    "#             activation    =  self.activation\n",
    "#             padding       = 'same'\n",
    "#             filters       =  self.n_channels\n",
    "#             learning_rate = self.learning_rate_init[0]\n",
    "#             #\n",
    "#             ndime         =  y_train.shape[1]\n",
    "#             n_train       =  X_train.shape[0]\n",
    "#             n_test        =  X_test.shape[0]\n",
    "#             assert        shape[0] * shape[1] * shape[2] == X_train.shape[ 1 ]\n",
    "#             inputs        =  keras.Input(shape=shape)\n",
    "#             #\n",
    "#     #         pdb.set_trace()\n",
    "#             #------------------------------\n",
    "#             #--- The network architecture\n",
    "#             #------------------------------\n",
    "#             model     = keras.Sequential([\n",
    "#                 layers.Dense(self.hidden_layer_size, activation=\"relu\"),\n",
    "#     #             layers.Dense(self.hidden_layer_size), #activation=\"relu\"),\n",
    "#                 layers.Dense(2, activation=\"softmax\")\n",
    "#                 ])\n",
    "#             optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate) #--- compilation step\n",
    "\n",
    "#             model.compile( optimizer=\"rmsprop\",\n",
    "#                            loss=\"sparse_categorical_crossentropy\",\n",
    "#                            metrics=[\"mse\"])\n",
    "\n",
    "\n",
    "#             #--- “Fitting” the model X_train_transfrmd, y_train\n",
    "#             X_train_reshaped =  X_train \n",
    "#             X_test_reshaped  =  X_test\n",
    "#             model.fit( X_train_reshaped, y_train, \n",
    "#                        validation_data      = ( X_test_reshaped, y_test ),\n",
    "#     #                     callbacks=callbacks,\n",
    "#                         epochs              = epochs, \n",
    "#                         verbose             = self.verbose, \n",
    "#                         shuffle             = False, \n",
    "#     #                     batch_size     = 128,\n",
    "#                         use_multiprocessing = True,\n",
    "#                         workers             = 4,\n",
    "#                      )        \n",
    "\n",
    "\n",
    "#             #--- save best model\n",
    "#             !mkdir best_model\n",
    "#     #         callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnetClassifier_from_scratch.tf',  \n",
    "#     #                                                    monitor=\"val_loss\",\n",
    "#     #                                                   save_freq=10,\n",
    "#     #                                                     save_best_only=True)]\n",
    "\n",
    "\n",
    "#             #--- validation loss\n",
    "#             loss       = model.history.history['loss']\n",
    "#             val_loss   = model.history.history['val_loss']\n",
    "#             best_model = model #keras.models.load_model(\"best_model/convnet_from_scratch.tf\")\n",
    "\n",
    "\n",
    "#             !mkdir png\n",
    "#             utl.PltErr(range(len(loss)), loss,\n",
    "#                        yscale='log',\n",
    "#                        xstr='epoch',ystr='loss',\n",
    "#                        title='png/loss_classification.png',\n",
    "#                       )\n",
    "\n",
    "#     #         pdb.set_trace()\n",
    "#             #--- confusion matrix\n",
    "#             cm = confusion_matrix(y_test, model.predict_classes(X_test),\n",
    "#                              labels=[0, 1]\n",
    "#                             )\n",
    "#             print('cm=',cm)\n",
    "#             np.savetxt('png/confusion.txt',np.c_[cm])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628ad42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Combine',\n",
       " 'ConvNetwork',\n",
       " 'ConvNetworkClassifier',\n",
       " 'GetCenterAtom',\n",
       " 'KerasANN',\n",
       " 'PCA',\n",
       " 'Parse',\n",
       " 'PrintDensityMap',\n",
       " 'PrintDescriptors',\n",
       " 'SklearnMLP',\n",
       " 'Spectra',\n",
       " 'TrainClassifier',\n",
       " 'TrainRegressor',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'zscore']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(NeuralNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f8fc70",
   "metadata": {},
   "source": [
    "## main(): classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dfe6a63d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating descriptors ...\n",
      "dim(y)= (7, 3)\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 11, 11, 11, 1)]   0         \n",
      "                                                                 \n",
      " conv3d_16 (Conv3D)          (None, 11, 11, 11, 16)    448       \n",
      "                                                                 \n",
      " average_pooling3d_12 (Avera  (None, 5, 5, 5, 16)      0         \n",
      " gePooling3D)                                                    \n",
      "                                                                 \n",
      " conv3d_17 (Conv3D)          (None, 5, 5, 5, 32)       13856     \n",
      "                                                                 \n",
      " average_pooling3d_13 (Avera  (None, 2, 2, 2, 32)      0         \n",
      " gePooling3D)                                                    \n",
      "                                                                 \n",
      " conv3d_18 (Conv3D)          (None, 2, 2, 2, 64)       55360     \n",
      "                                                                 \n",
      " average_pooling3d_14 (Avera  (None, 1, 1, 1, 64)      0         \n",
      " gePooling3D)                                                    \n",
      "                                                                 \n",
      " conv3d_19 (Conv3D)          (None, 1, 1, 1, 128)      221312    \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 291,363\n",
      "Trainable params: 291,363\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "cnn model summary: None\n",
      "mkdir: best_model: File exists\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f9605f6a5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f9605f6a5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f9605f6a5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/5 [=====>........................] - ETA: 4s - loss: 1.9818 - mse: 1.9818WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f96070e8d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f96070e8d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f96070e8d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "5/5 [==============================] - 1s 57ms/step - loss: 1.9819 - mse: 1.9819 - val_loss: 1.9873 - val_mse: 1.9873\n",
      "Epoch 2/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9807 - mse: 1.9807 - val_loss: 1.9872 - val_mse: 1.9872\n",
      "Epoch 3/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9796 - mse: 1.9796 - val_loss: 1.9872 - val_mse: 1.9872\n",
      "Epoch 4/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.9785 - mse: 1.9785 - val_loss: 1.9871 - val_mse: 1.9871\n",
      "Epoch 5/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9774 - mse: 1.9774 - val_loss: 1.9871 - val_mse: 1.9871\n",
      "Epoch 6/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.9763 - mse: 1.9763 - val_loss: 1.9870 - val_mse: 1.9870\n",
      "Epoch 7/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.9752 - mse: 1.9752 - val_loss: 1.9870 - val_mse: 1.9870\n",
      "Epoch 8/1000\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 1.9741 - mse: 1.9741 - val_loss: 1.9869 - val_mse: 1.9869\n",
      "Epoch 9/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 1.9730 - mse: 1.9730 - val_loss: 1.9869 - val_mse: 1.9869\n",
      "Epoch 10/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.9718 - mse: 1.9718 - val_loss: 1.9868 - val_mse: 1.9868\n",
      "Epoch 11/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.9707 - mse: 1.9707 - val_loss: 1.9868 - val_mse: 1.9868\n",
      "Epoch 12/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.9695 - mse: 1.9695 - val_loss: 1.9867 - val_mse: 1.9867\n",
      "Epoch 13/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9683 - mse: 1.9683 - val_loss: 1.9867 - val_mse: 1.9867\n",
      "Epoch 14/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9671 - mse: 1.9671 - val_loss: 1.9866 - val_mse: 1.9866\n",
      "Epoch 15/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.9659 - mse: 1.9659 - val_loss: 1.9865 - val_mse: 1.9865\n",
      "Epoch 16/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.9646 - mse: 1.9646 - val_loss: 1.9864 - val_mse: 1.9864\n",
      "Epoch 17/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.9634 - mse: 1.9634 - val_loss: 1.9863 - val_mse: 1.9863\n",
      "Epoch 18/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9621 - mse: 1.9621 - val_loss: 1.9863 - val_mse: 1.9863\n",
      "Epoch 19/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9607 - mse: 1.9607 - val_loss: 1.9861 - val_mse: 1.9861\n",
      "Epoch 20/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.9594 - mse: 1.9594 - val_loss: 1.9860 - val_mse: 1.9860\n",
      "Epoch 21/1000\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 1.9580 - mse: 1.9580 - val_loss: 1.9859 - val_mse: 1.9859\n",
      "Epoch 22/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.9565 - mse: 1.9565 - val_loss: 1.9858 - val_mse: 1.9858\n",
      "Epoch 23/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.9551 - mse: 1.9551 - val_loss: 1.9856 - val_mse: 1.9856\n",
      "Epoch 24/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.9536 - mse: 1.9536 - val_loss: 1.9855 - val_mse: 1.9855\n",
      "Epoch 25/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9520 - mse: 1.9520 - val_loss: 1.9853 - val_mse: 1.9853\n",
      "Epoch 26/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9504 - mse: 1.9504 - val_loss: 1.9851 - val_mse: 1.9851\n",
      "Epoch 27/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9488 - mse: 1.9488 - val_loss: 1.9849 - val_mse: 1.9849\n",
      "Epoch 28/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9471 - mse: 1.9471 - val_loss: 1.9847 - val_mse: 1.9847\n",
      "Epoch 29/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.9454 - mse: 1.9454 - val_loss: 1.9845 - val_mse: 1.9845\n",
      "Epoch 30/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9436 - mse: 1.9436 - val_loss: 1.9843 - val_mse: 1.9843\n",
      "Epoch 31/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9418 - mse: 1.9418 - val_loss: 1.9840 - val_mse: 1.9840\n",
      "Epoch 32/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.9399 - mse: 1.9399 - val_loss: 1.9838 - val_mse: 1.9838\n",
      "Epoch 33/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1.9380 - mse: 1.9380 - val_loss: 1.9835 - val_mse: 1.9835\n",
      "Epoch 34/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1.9360 - mse: 1.9360 - val_loss: 1.9832 - val_mse: 1.9832\n",
      "Epoch 35/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1.9339 - mse: 1.9339 - val_loss: 1.9829 - val_mse: 1.9829\n",
      "Epoch 36/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9318 - mse: 1.9318 - val_loss: 1.9825 - val_mse: 1.9825\n",
      "Epoch 37/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9296 - mse: 1.9296 - val_loss: 1.9822 - val_mse: 1.9822\n",
      "Epoch 38/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.9274 - mse: 1.9274 - val_loss: 1.9818 - val_mse: 1.9818\n",
      "Epoch 39/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9250 - mse: 1.9250 - val_loss: 1.9814 - val_mse: 1.9814\n",
      "Epoch 40/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.9226 - mse: 1.9226 - val_loss: 1.9809 - val_mse: 1.9809\n",
      "Epoch 41/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.9201 - mse: 1.9201 - val_loss: 1.9805 - val_mse: 1.9805\n",
      "Epoch 42/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.9175 - mse: 1.9175 - val_loss: 1.9800 - val_mse: 1.9800\n",
      "Epoch 43/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9149 - mse: 1.9149 - val_loss: 1.9795 - val_mse: 1.9795\n",
      "Epoch 44/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9121 - mse: 1.9121 - val_loss: 1.9789 - val_mse: 1.9789\n",
      "Epoch 45/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.9093 - mse: 1.9093 - val_loss: 1.9784 - val_mse: 1.9784\n",
      "Epoch 46/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.9063 - mse: 1.9063 - val_loss: 1.9778 - val_mse: 1.9778\n",
      "Epoch 47/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.9033 - mse: 1.9033 - val_loss: 1.9771 - val_mse: 1.9771\n",
      "Epoch 48/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.9001 - mse: 1.9001 - val_loss: 1.9765 - val_mse: 1.9765\n",
      "Epoch 49/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.8968 - mse: 1.8968 - val_loss: 1.9758 - val_mse: 1.9758\n",
      "Epoch 50/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.8934 - mse: 1.8934 - val_loss: 1.9750 - val_mse: 1.9750\n",
      "Epoch 51/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8899 - mse: 1.8899 - val_loss: 1.9743 - val_mse: 1.9743\n",
      "Epoch 52/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.8863 - mse: 1.8863 - val_loss: 1.9734 - val_mse: 1.9734\n",
      "Epoch 53/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.8826 - mse: 1.8826 - val_loss: 1.9726 - val_mse: 1.9726\n",
      "Epoch 54/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.8787 - mse: 1.8787 - val_loss: 1.9717 - val_mse: 1.9717\n",
      "Epoch 55/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.8747 - mse: 1.8747 - val_loss: 1.9708 - val_mse: 1.9708\n",
      "Epoch 56/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.8705 - mse: 1.8705 - val_loss: 1.9698 - val_mse: 1.9698\n",
      "Epoch 57/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.8662 - mse: 1.8662 - val_loss: 1.9688 - val_mse: 1.9688\n",
      "Epoch 58/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.8617 - mse: 1.8617 - val_loss: 1.9677 - val_mse: 1.9677\n",
      "Epoch 59/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.8571 - mse: 1.8571 - val_loss: 1.9666 - val_mse: 1.9666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8523 - mse: 1.8523 - val_loss: 1.9654 - val_mse: 1.9654\n",
      "Epoch 61/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.8474 - mse: 1.8474 - val_loss: 1.9642 - val_mse: 1.9642\n",
      "Epoch 62/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.8423 - mse: 1.8423 - val_loss: 1.9629 - val_mse: 1.9629\n",
      "Epoch 63/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8370 - mse: 1.8370 - val_loss: 1.9616 - val_mse: 1.9616\n",
      "Epoch 64/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8315 - mse: 1.8315 - val_loss: 1.9602 - val_mse: 1.9602\n",
      "Epoch 65/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8259 - mse: 1.8259 - val_loss: 1.9587 - val_mse: 1.9587\n",
      "Epoch 66/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.8200 - mse: 1.8200 - val_loss: 1.9572 - val_mse: 1.9572\n",
      "Epoch 67/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8140 - mse: 1.8140 - val_loss: 1.9557 - val_mse: 1.9557\n",
      "Epoch 68/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8077 - mse: 1.8077 - val_loss: 1.9540 - val_mse: 1.9540\n",
      "Epoch 69/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8013 - mse: 1.8013 - val_loss: 1.9523 - val_mse: 1.9523\n",
      "Epoch 70/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7946 - mse: 1.7946 - val_loss: 1.9506 - val_mse: 1.9506\n",
      "Epoch 71/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7877 - mse: 1.7877 - val_loss: 1.9488 - val_mse: 1.9488\n",
      "Epoch 72/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7806 - mse: 1.7806 - val_loss: 1.9469 - val_mse: 1.9469\n",
      "Epoch 73/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7733 - mse: 1.7733 - val_loss: 1.9449 - val_mse: 1.9449\n",
      "Epoch 74/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7657 - mse: 1.7657 - val_loss: 1.9429 - val_mse: 1.9429\n",
      "Epoch 75/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7579 - mse: 1.7579 - val_loss: 1.9407 - val_mse: 1.9407\n",
      "Epoch 76/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7499 - mse: 1.7499 - val_loss: 1.9386 - val_mse: 1.9386\n",
      "Epoch 77/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.7416 - mse: 1.7416 - val_loss: 1.9363 - val_mse: 1.9363\n",
      "Epoch 78/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7331 - mse: 1.7331 - val_loss: 1.9340 - val_mse: 1.9340\n",
      "Epoch 79/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7243 - mse: 1.7243 - val_loss: 1.9315 - val_mse: 1.9315\n",
      "Epoch 80/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7153 - mse: 1.7153 - val_loss: 1.9290 - val_mse: 1.9290\n",
      "Epoch 81/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7060 - mse: 1.7060 - val_loss: 1.9265 - val_mse: 1.9265\n",
      "Epoch 82/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.6964 - mse: 1.6964 - val_loss: 1.9238 - val_mse: 1.9238\n",
      "Epoch 83/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.6866 - mse: 1.6866 - val_loss: 1.9211 - val_mse: 1.9211\n",
      "Epoch 84/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6766 - mse: 1.6766 - val_loss: 1.9183 - val_mse: 1.9183\n",
      "Epoch 85/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.6662 - mse: 1.6662 - val_loss: 1.9154 - val_mse: 1.9154\n",
      "Epoch 86/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.6556 - mse: 1.6556 - val_loss: 1.9124 - val_mse: 1.9124\n",
      "Epoch 87/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6448 - mse: 1.6448 - val_loss: 1.9093 - val_mse: 1.9093\n",
      "Epoch 88/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6337 - mse: 1.6337 - val_loss: 1.9061 - val_mse: 1.9061\n",
      "Epoch 89/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6223 - mse: 1.6223 - val_loss: 1.9029 - val_mse: 1.9029\n",
      "Epoch 90/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6106 - mse: 1.6106 - val_loss: 1.8996 - val_mse: 1.8996\n",
      "Epoch 91/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5987 - mse: 1.5987 - val_loss: 1.8962 - val_mse: 1.8962\n",
      "Epoch 92/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5865 - mse: 1.5865 - val_loss: 1.8927 - val_mse: 1.8927\n",
      "Epoch 93/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.5741 - mse: 1.5741 - val_loss: 1.8891 - val_mse: 1.8891\n",
      "Epoch 94/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5615 - mse: 1.5615 - val_loss: 1.8855 - val_mse: 1.8855\n",
      "Epoch 95/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5486 - mse: 1.5486 - val_loss: 1.8818 - val_mse: 1.8818\n",
      "Epoch 96/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5354 - mse: 1.5354 - val_loss: 1.8780 - val_mse: 1.8780\n",
      "Epoch 97/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5221 - mse: 1.5221 - val_loss: 1.8741 - val_mse: 1.8741\n",
      "Epoch 98/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5085 - mse: 1.5085 - val_loss: 1.8701 - val_mse: 1.8701\n",
      "Epoch 99/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.4946 - mse: 1.4946 - val_loss: 1.8661 - val_mse: 1.8661\n",
      "Epoch 100/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.4806 - mse: 1.4806 - val_loss: 1.8620 - val_mse: 1.8620\n",
      "Epoch 101/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.4664 - mse: 1.4664 - val_loss: 1.8578 - val_mse: 1.8578\n",
      "Epoch 102/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.4520 - mse: 1.4520 - val_loss: 1.8536 - val_mse: 1.8536\n",
      "Epoch 103/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.4374 - mse: 1.4374 - val_loss: 1.8493 - val_mse: 1.8493\n",
      "Epoch 104/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.4227 - mse: 1.4227 - val_loss: 1.8450 - val_mse: 1.8450\n",
      "Epoch 105/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.4078 - mse: 1.4078 - val_loss: 1.8405 - val_mse: 1.8405\n",
      "Epoch 106/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.3927 - mse: 1.3927 - val_loss: 1.8361 - val_mse: 1.8361\n",
      "Epoch 107/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3776 - mse: 1.3776 - val_loss: 1.8316 - val_mse: 1.8316\n",
      "Epoch 108/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.3623 - mse: 1.3623 - val_loss: 1.8270 - val_mse: 1.8270\n",
      "Epoch 109/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3470 - mse: 1.3470 - val_loss: 1.8224 - val_mse: 1.8224\n",
      "Epoch 110/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3315 - mse: 1.3315 - val_loss: 1.8178 - val_mse: 1.8178\n",
      "Epoch 111/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3160 - mse: 1.3160 - val_loss: 1.8131 - val_mse: 1.8131\n",
      "Epoch 112/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.3005 - mse: 1.3005 - val_loss: 1.8085 - val_mse: 1.8085\n",
      "Epoch 113/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2849 - mse: 1.2849 - val_loss: 1.8038 - val_mse: 1.8038\n",
      "Epoch 114/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2693 - mse: 1.2693 - val_loss: 1.7990 - val_mse: 1.7990\n",
      "Epoch 115/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2538 - mse: 1.2538 - val_loss: 1.7943 - val_mse: 1.7943\n",
      "Epoch 116/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2383 - mse: 1.2383 - val_loss: 1.7896 - val_mse: 1.7896\n",
      "Epoch 117/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2228 - mse: 1.2228 - val_loss: 1.7848 - val_mse: 1.7848\n",
      "Epoch 118/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2074 - mse: 1.2074 - val_loss: 1.7801 - val_mse: 1.7801\n",
      "Epoch 119/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.1920 - mse: 1.1920 - val_loss: 1.7754 - val_mse: 1.7754\n",
      "Epoch 120/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1768 - mse: 1.1768 - val_loss: 1.7707 - val_mse: 1.7707\n",
      "Epoch 121/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1617 - mse: 1.1617 - val_loss: 1.7660 - val_mse: 1.7660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1467 - mse: 1.1467 - val_loss: 1.7613 - val_mse: 1.7613\n",
      "Epoch 123/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1319 - mse: 1.1319 - val_loss: 1.7567 - val_mse: 1.7567\n",
      "Epoch 124/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1173 - mse: 1.1173 - val_loss: 1.7521 - val_mse: 1.7521\n",
      "Epoch 125/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.1029 - mse: 1.1029 - val_loss: 1.7475 - val_mse: 1.7475\n",
      "Epoch 126/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.0887 - mse: 1.0887 - val_loss: 1.7430 - val_mse: 1.7430\n",
      "Epoch 127/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0747 - mse: 1.0747 - val_loss: 1.7386 - val_mse: 1.7386\n",
      "Epoch 128/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0609 - mse: 1.0609 - val_loss: 1.7342 - val_mse: 1.7342\n",
      "Epoch 129/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0474 - mse: 1.0474 - val_loss: 1.7298 - val_mse: 1.7298\n",
      "Epoch 130/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0341 - mse: 1.0341 - val_loss: 1.7255 - val_mse: 1.7255\n",
      "Epoch 131/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0211 - mse: 1.0211 - val_loss: 1.7212 - val_mse: 1.7212\n",
      "Epoch 132/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0084 - mse: 1.0084 - val_loss: 1.7171 - val_mse: 1.7171\n",
      "Epoch 133/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.9960 - mse: 0.9960 - val_loss: 1.7129 - val_mse: 1.7129\n",
      "Epoch 134/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9838 - mse: 0.9838 - val_loss: 1.7089 - val_mse: 1.7089\n",
      "Epoch 135/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9720 - mse: 0.9720 - val_loss: 1.7049 - val_mse: 1.7049\n",
      "Epoch 136/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9604 - mse: 0.9604 - val_loss: 1.7010 - val_mse: 1.7010\n",
      "Epoch 137/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9492 - mse: 0.9492 - val_loss: 1.6971 - val_mse: 1.6971\n",
      "Epoch 138/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9382 - mse: 0.9382 - val_loss: 1.6933 - val_mse: 1.6933\n",
      "Epoch 139/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.9275 - mse: 0.9275 - val_loss: 1.6896 - val_mse: 1.6896\n",
      "Epoch 140/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.9172 - mse: 0.9172 - val_loss: 1.6859 - val_mse: 1.6859\n",
      "Epoch 141/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9071 - mse: 0.9071 - val_loss: 1.6823 - val_mse: 1.6823\n",
      "Epoch 142/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8973 - mse: 0.8973 - val_loss: 1.6787 - val_mse: 1.6787\n",
      "Epoch 143/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8878 - mse: 0.8878 - val_loss: 1.6752 - val_mse: 1.6752\n",
      "Epoch 144/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8785 - mse: 0.8785 - val_loss: 1.6718 - val_mse: 1.6718\n",
      "Epoch 145/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8695 - mse: 0.8695 - val_loss: 1.6684 - val_mse: 1.6684\n",
      "Epoch 146/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.8608 - mse: 0.8608 - val_loss: 1.6650 - val_mse: 1.6650\n",
      "Epoch 147/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.8523 - mse: 0.8523 - val_loss: 1.6617 - val_mse: 1.6617\n",
      "Epoch 148/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8440 - mse: 0.8440 - val_loss: 1.6584 - val_mse: 1.6584\n",
      "Epoch 149/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8360 - mse: 0.8360 - val_loss: 1.6551 - val_mse: 1.6551\n",
      "Epoch 150/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8281 - mse: 0.8281 - val_loss: 1.6519 - val_mse: 1.6519\n",
      "Epoch 151/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8205 - mse: 0.8205 - val_loss: 1.6487 - val_mse: 1.6487\n",
      "Epoch 152/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8130 - mse: 0.8130 - val_loss: 1.6455 - val_mse: 1.6455\n",
      "Epoch 153/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.8057 - mse: 0.8057 - val_loss: 1.6424 - val_mse: 1.6424\n",
      "Epoch 154/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.7986 - mse: 0.7986 - val_loss: 1.6392 - val_mse: 1.6392\n",
      "Epoch 155/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7916 - mse: 0.7916 - val_loss: 1.6361 - val_mse: 1.6361\n",
      "Epoch 156/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7848 - mse: 0.7848 - val_loss: 1.6330 - val_mse: 1.6330\n",
      "Epoch 157/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7781 - mse: 0.7781 - val_loss: 1.6298 - val_mse: 1.6298\n",
      "Epoch 158/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7715 - mse: 0.7715 - val_loss: 1.6267 - val_mse: 1.6267\n",
      "Epoch 159/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7650 - mse: 0.7650 - val_loss: 1.6236 - val_mse: 1.6236\n",
      "Epoch 160/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.7587 - mse: 0.7587 - val_loss: 1.6205 - val_mse: 1.6205\n",
      "Epoch 161/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7524 - mse: 0.7524 - val_loss: 1.6174 - val_mse: 1.6174\n",
      "Epoch 162/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7462 - mse: 0.7462 - val_loss: 1.6142 - val_mse: 1.6142\n",
      "Epoch 163/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7401 - mse: 0.7401 - val_loss: 1.6111 - val_mse: 1.6111\n",
      "Epoch 164/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7341 - mse: 0.7341 - val_loss: 1.6079 - val_mse: 1.6079\n",
      "Epoch 165/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7281 - mse: 0.7281 - val_loss: 1.6047 - val_mse: 1.6047\n",
      "Epoch 166/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7223 - mse: 0.7223 - val_loss: 1.6015 - val_mse: 1.6015\n",
      "Epoch 167/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.7164 - mse: 0.7164 - val_loss: 1.5983 - val_mse: 1.5983\n",
      "Epoch 168/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.7106 - mse: 0.7106 - val_loss: 1.5951 - val_mse: 1.5951\n",
      "Epoch 169/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7049 - mse: 0.7049 - val_loss: 1.5919 - val_mse: 1.5919\n",
      "Epoch 170/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6992 - mse: 0.6992 - val_loss: 1.5886 - val_mse: 1.5886\n",
      "Epoch 171/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6936 - mse: 0.6936 - val_loss: 1.5853 - val_mse: 1.5853\n",
      "Epoch 172/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6880 - mse: 0.6880 - val_loss: 1.5820 - val_mse: 1.5820\n",
      "Epoch 173/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6824 - mse: 0.6824 - val_loss: 1.5787 - val_mse: 1.5787\n",
      "Epoch 174/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6768 - mse: 0.6768 - val_loss: 1.5753 - val_mse: 1.5753\n",
      "Epoch 175/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6713 - mse: 0.6713 - val_loss: 1.5719 - val_mse: 1.5719\n",
      "Epoch 176/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6658 - mse: 0.6658 - val_loss: 1.5685 - val_mse: 1.5685\n",
      "Epoch 177/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6603 - mse: 0.6603 - val_loss: 1.5651 - val_mse: 1.5651\n",
      "Epoch 178/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6549 - mse: 0.6549 - val_loss: 1.5617 - val_mse: 1.5617\n",
      "Epoch 179/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6495 - mse: 0.6495 - val_loss: 1.5582 - val_mse: 1.5582\n",
      "Epoch 180/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6441 - mse: 0.6441 - val_loss: 1.5547 - val_mse: 1.5547\n",
      "Epoch 181/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6387 - mse: 0.6387 - val_loss: 1.5512 - val_mse: 1.5512\n",
      "Epoch 182/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6333 - mse: 0.6333 - val_loss: 1.5477 - val_mse: 1.5477\n",
      "Epoch 183/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6279 - mse: 0.6279 - val_loss: 1.5441 - val_mse: 1.5441\n",
      "Epoch 184/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6226 - mse: 0.6226 - val_loss: 1.5405 - val_mse: 1.5405\n",
      "Epoch 185/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6173 - mse: 0.6173 - val_loss: 1.5369 - val_mse: 1.5369\n",
      "Epoch 186/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6119 - mse: 0.6119 - val_loss: 1.5332 - val_mse: 1.5332\n",
      "Epoch 187/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6066 - mse: 0.6066 - val_loss: 1.5296 - val_mse: 1.5296\n",
      "Epoch 188/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6013 - mse: 0.6013 - val_loss: 1.5259 - val_mse: 1.5259\n",
      "Epoch 189/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5961 - mse: 0.5961 - val_loss: 1.5222 - val_mse: 1.5222\n",
      "Epoch 190/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5908 - mse: 0.5908 - val_loss: 1.5184 - val_mse: 1.5184\n",
      "Epoch 191/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5855 - mse: 0.5855 - val_loss: 1.5147 - val_mse: 1.5147\n",
      "Epoch 192/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5802 - mse: 0.5802 - val_loss: 1.5109 - val_mse: 1.5109\n",
      "Epoch 193/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5750 - mse: 0.5750 - val_loss: 1.5071 - val_mse: 1.5071\n",
      "Epoch 194/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5697 - mse: 0.5697 - val_loss: 1.5032 - val_mse: 1.5032\n",
      "Epoch 195/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5645 - mse: 0.5645 - val_loss: 1.4994 - val_mse: 1.4994\n",
      "Epoch 196/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5593 - mse: 0.5593 - val_loss: 1.4955 - val_mse: 1.4955\n",
      "Epoch 197/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5541 - mse: 0.5541 - val_loss: 1.4916 - val_mse: 1.4916\n",
      "Epoch 198/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5488 - mse: 0.5488 - val_loss: 1.4877 - val_mse: 1.4877\n",
      "Epoch 199/1000\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.5436 - mse: 0.5436 - val_loss: 1.4837 - val_mse: 1.4837\n",
      "Epoch 200/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5384 - mse: 0.5384 - val_loss: 1.4797 - val_mse: 1.4797\n",
      "Epoch 201/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5332 - mse: 0.5332 - val_loss: 1.4757 - val_mse: 1.4757\n",
      "Epoch 202/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5280 - mse: 0.5280 - val_loss: 1.4717 - val_mse: 1.4717\n",
      "Epoch 203/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 1.4677 - val_mse: 1.4677\n",
      "Epoch 204/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5177 - mse: 0.5177 - val_loss: 1.4636 - val_mse: 1.4636\n",
      "Epoch 205/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5125 - mse: 0.5125 - val_loss: 1.4595 - val_mse: 1.4595\n",
      "Epoch 206/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5073 - mse: 0.5073 - val_loss: 1.4554 - val_mse: 1.4554\n",
      "Epoch 207/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5022 - mse: 0.5022 - val_loss: 1.4512 - val_mse: 1.4512\n",
      "Epoch 208/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4970 - mse: 0.4970 - val_loss: 1.4471 - val_mse: 1.4471\n",
      "Epoch 209/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4919 - mse: 0.4919 - val_loss: 1.4429 - val_mse: 1.4429\n",
      "Epoch 210/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4867 - mse: 0.4867 - val_loss: 1.4387 - val_mse: 1.4387\n",
      "Epoch 211/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4816 - mse: 0.4816 - val_loss: 1.4345 - val_mse: 1.4345\n",
      "Epoch 212/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4765 - mse: 0.4765 - val_loss: 1.4302 - val_mse: 1.4302\n",
      "Epoch 213/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4714 - mse: 0.4714 - val_loss: 1.4260 - val_mse: 1.4260\n",
      "Epoch 214/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4663 - mse: 0.4663 - val_loss: 1.4217 - val_mse: 1.4217\n",
      "Epoch 215/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4612 - mse: 0.4612 - val_loss: 1.4174 - val_mse: 1.4174\n",
      "Epoch 216/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4561 - mse: 0.4561 - val_loss: 1.4131 - val_mse: 1.4131\n",
      "Epoch 217/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4510 - mse: 0.4510 - val_loss: 1.4087 - val_mse: 1.4087\n",
      "Epoch 218/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4460 - mse: 0.4460 - val_loss: 1.4044 - val_mse: 1.4044\n",
      "Epoch 219/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4409 - mse: 0.4409 - val_loss: 1.4000 - val_mse: 1.4000\n",
      "Epoch 220/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4359 - mse: 0.4359 - val_loss: 1.3956 - val_mse: 1.3956\n",
      "Epoch 221/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4308 - mse: 0.4308 - val_loss: 1.3912 - val_mse: 1.3912\n",
      "Epoch 222/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4258 - mse: 0.4258 - val_loss: 1.3867 - val_mse: 1.3867\n",
      "Epoch 223/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4208 - mse: 0.4208 - val_loss: 1.3823 - val_mse: 1.3823\n",
      "Epoch 224/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4158 - mse: 0.4158 - val_loss: 1.3778 - val_mse: 1.3778\n",
      "Epoch 225/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4108 - mse: 0.4108 - val_loss: 1.3733 - val_mse: 1.3733\n",
      "Epoch 226/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4058 - mse: 0.4058 - val_loss: 1.3688 - val_mse: 1.3688\n",
      "Epoch 227/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4009 - mse: 0.4009 - val_loss: 1.3643 - val_mse: 1.3643\n",
      "Epoch 228/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3960 - mse: 0.3960 - val_loss: 1.3597 - val_mse: 1.3597\n",
      "Epoch 229/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3910 - mse: 0.3910 - val_loss: 1.3552 - val_mse: 1.3552\n",
      "Epoch 230/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3861 - mse: 0.3861 - val_loss: 1.3506 - val_mse: 1.3506\n",
      "Epoch 231/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3812 - mse: 0.3812 - val_loss: 1.3461 - val_mse: 1.3461\n",
      "Epoch 232/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3764 - mse: 0.3764 - val_loss: 1.3415 - val_mse: 1.3415\n",
      "Epoch 233/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3715 - mse: 0.3715 - val_loss: 1.3369 - val_mse: 1.3369\n",
      "Epoch 234/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3667 - mse: 0.3667 - val_loss: 1.3322 - val_mse: 1.3322\n",
      "Epoch 235/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3619 - mse: 0.3619 - val_loss: 1.3276 - val_mse: 1.3276\n",
      "Epoch 236/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3571 - mse: 0.3571 - val_loss: 1.3230 - val_mse: 1.3230\n",
      "Epoch 237/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3523 - mse: 0.3523 - val_loss: 1.3183 - val_mse: 1.3183\n",
      "Epoch 238/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3476 - mse: 0.3476 - val_loss: 1.3137 - val_mse: 1.3137\n",
      "Epoch 239/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3429 - mse: 0.3429 - val_loss: 1.3090 - val_mse: 1.3090\n",
      "Epoch 240/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3382 - mse: 0.3382 - val_loss: 1.3043 - val_mse: 1.3043\n",
      "Epoch 241/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3335 - mse: 0.3335 - val_loss: 1.2996 - val_mse: 1.2996\n",
      "Epoch 242/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3288 - mse: 0.3288 - val_loss: 1.2949 - val_mse: 1.2949\n",
      "Epoch 243/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3242 - mse: 0.3242 - val_loss: 1.2902 - val_mse: 1.2902\n",
      "Epoch 244/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3196 - mse: 0.3196 - val_loss: 1.2855 - val_mse: 1.2855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 245/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3151 - mse: 0.3151 - val_loss: 1.2808 - val_mse: 1.2808\n",
      "Epoch 246/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3105 - mse: 0.3105 - val_loss: 1.2761 - val_mse: 1.2761\n",
      "Epoch 247/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3060 - mse: 0.3060 - val_loss: 1.2714 - val_mse: 1.2714\n",
      "Epoch 248/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3015 - mse: 0.3015 - val_loss: 1.2667 - val_mse: 1.2667\n",
      "Epoch 249/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2971 - mse: 0.2971 - val_loss: 1.2619 - val_mse: 1.2619\n",
      "Epoch 250/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2927 - mse: 0.2927 - val_loss: 1.2572 - val_mse: 1.2572\n",
      "Epoch 251/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2883 - mse: 0.2883 - val_loss: 1.2525 - val_mse: 1.2525\n",
      "Epoch 252/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2839 - mse: 0.2839 - val_loss: 1.2478 - val_mse: 1.2478\n",
      "Epoch 253/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2796 - mse: 0.2796 - val_loss: 1.2430 - val_mse: 1.2430\n",
      "Epoch 254/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2754 - mse: 0.2754 - val_loss: 1.2383 - val_mse: 1.2383\n",
      "Epoch 255/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2711 - mse: 0.2711 - val_loss: 1.2336 - val_mse: 1.2336\n",
      "Epoch 256/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2669 - mse: 0.2669 - val_loss: 1.2289 - val_mse: 1.2289\n",
      "Epoch 257/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2627 - mse: 0.2627 - val_loss: 1.2241 - val_mse: 1.2241\n",
      "Epoch 258/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.2586 - mse: 0.2586 - val_loss: 1.2194 - val_mse: 1.2194\n",
      "Epoch 259/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 1.2147 - val_mse: 1.2147\n",
      "Epoch 260/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2505 - mse: 0.2505 - val_loss: 1.2100 - val_mse: 1.2100\n",
      "Epoch 261/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2465 - mse: 0.2465 - val_loss: 1.2053 - val_mse: 1.2053\n",
      "Epoch 262/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2425 - mse: 0.2425 - val_loss: 1.2006 - val_mse: 1.2006\n",
      "Epoch 263/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2386 - mse: 0.2386 - val_loss: 1.1959 - val_mse: 1.1959\n",
      "Epoch 264/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2347 - mse: 0.2347 - val_loss: 1.1913 - val_mse: 1.1913\n",
      "Epoch 265/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2309 - mse: 0.2309 - val_loss: 1.1866 - val_mse: 1.1866\n",
      "Epoch 266/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2271 - mse: 0.2271 - val_loss: 1.1819 - val_mse: 1.1819\n",
      "Epoch 267/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2233 - mse: 0.2233 - val_loss: 1.1773 - val_mse: 1.1773\n",
      "Epoch 268/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2196 - mse: 0.2196 - val_loss: 1.1727 - val_mse: 1.1727\n",
      "Epoch 269/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2159 - mse: 0.2159 - val_loss: 1.1681 - val_mse: 1.1681\n",
      "Epoch 270/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2123 - mse: 0.2123 - val_loss: 1.1635 - val_mse: 1.1635\n",
      "Epoch 271/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2088 - mse: 0.2088 - val_loss: 1.1589 - val_mse: 1.1589\n",
      "Epoch 272/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2052 - mse: 0.2052 - val_loss: 1.1543 - val_mse: 1.1543\n",
      "Epoch 273/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2018 - mse: 0.2018 - val_loss: 1.1498 - val_mse: 1.1498\n",
      "Epoch 274/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1983 - mse: 0.1983 - val_loss: 1.1452 - val_mse: 1.1452\n",
      "Epoch 275/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1950 - mse: 0.1950 - val_loss: 1.1407 - val_mse: 1.1407\n",
      "Epoch 276/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1916 - mse: 0.1916 - val_loss: 1.1362 - val_mse: 1.1362\n",
      "Epoch 277/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1883 - mse: 0.1883 - val_loss: 1.1317 - val_mse: 1.1317\n",
      "Epoch 278/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1851 - mse: 0.1851 - val_loss: 1.1273 - val_mse: 1.1273\n",
      "Epoch 279/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1819 - mse: 0.1819 - val_loss: 1.1228 - val_mse: 1.1228\n",
      "Epoch 280/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1788 - mse: 0.1788 - val_loss: 1.1184 - val_mse: 1.1184\n",
      "Epoch 281/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1757 - mse: 0.1757 - val_loss: 1.1140 - val_mse: 1.1140\n",
      "Epoch 282/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1727 - mse: 0.1727 - val_loss: 1.1096 - val_mse: 1.1096\n",
      "Epoch 283/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1697 - mse: 0.1697 - val_loss: 1.1053 - val_mse: 1.1053\n",
      "Epoch 284/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1667 - mse: 0.1667 - val_loss: 1.1010 - val_mse: 1.1010\n",
      "Epoch 285/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1638 - mse: 0.1638 - val_loss: 1.0967 - val_mse: 1.0967\n",
      "Epoch 286/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1610 - mse: 0.1610 - val_loss: 1.0924 - val_mse: 1.0924\n",
      "Epoch 287/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1582 - mse: 0.1582 - val_loss: 1.0881 - val_mse: 1.0881\n",
      "Epoch 288/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 1.0839 - val_mse: 1.0839\n",
      "Epoch 289/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1528 - mse: 0.1528 - val_loss: 1.0797 - val_mse: 1.0797\n",
      "Epoch 290/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1502 - mse: 0.1502 - val_loss: 1.0756 - val_mse: 1.0756\n",
      "Epoch 291/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1476 - mse: 0.1476 - val_loss: 1.0714 - val_mse: 1.0714\n",
      "Epoch 292/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1450 - mse: 0.1450 - val_loss: 1.0673 - val_mse: 1.0673\n",
      "Epoch 293/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1425 - mse: 0.1425 - val_loss: 1.0632 - val_mse: 1.0632\n",
      "Epoch 294/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1401 - mse: 0.1401 - val_loss: 1.0592 - val_mse: 1.0592\n",
      "Epoch 295/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1377 - mse: 0.1377 - val_loss: 1.0552 - val_mse: 1.0552\n",
      "Epoch 296/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1353 - mse: 0.1353 - val_loss: 1.0512 - val_mse: 1.0512\n",
      "Epoch 297/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1330 - mse: 0.1330 - val_loss: 1.0472 - val_mse: 1.0472\n",
      "Epoch 298/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1308 - mse: 0.1308 - val_loss: 1.0433 - val_mse: 1.0433\n",
      "Epoch 299/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1286 - mse: 0.1286 - val_loss: 1.0394 - val_mse: 1.0394\n",
      "Epoch 300/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1264 - mse: 0.1264 - val_loss: 1.0356 - val_mse: 1.0356\n",
      "Epoch 301/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1243 - mse: 0.1243 - val_loss: 1.0317 - val_mse: 1.0317\n",
      "Epoch 302/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1223 - mse: 0.1223 - val_loss: 1.0279 - val_mse: 1.0279\n",
      "Epoch 303/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1202 - mse: 0.1202 - val_loss: 1.0242 - val_mse: 1.0242\n",
      "Epoch 304/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1183 - mse: 0.1183 - val_loss: 1.0205 - val_mse: 1.0205\n",
      "Epoch 305/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1163 - mse: 0.1163 - val_loss: 1.0168 - val_mse: 1.0168\n",
      "Epoch 306/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.1144 - mse: 0.1144 - val_loss: 1.0131 - val_mse: 1.0131\n",
      "Epoch 307/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.1126 - mse: 0.1126 - val_loss: 1.0095 - val_mse: 1.0095\n",
      "Epoch 308/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.1108 - mse: 0.1108 - val_loss: 1.0059 - val_mse: 1.0059\n",
      "Epoch 309/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.1090 - mse: 0.1090 - val_loss: 1.0024 - val_mse: 1.0024\n",
      "Epoch 310/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.1073 - mse: 0.1073 - val_loss: 0.9988 - val_mse: 0.9988\n",
      "Epoch 311/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.1056 - mse: 0.1056 - val_loss: 0.9953 - val_mse: 0.9953\n",
      "Epoch 312/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.1040 - mse: 0.1040 - val_loss: 0.9919 - val_mse: 0.9919\n",
      "Epoch 313/1000\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.1024 - mse: 0.1024 - val_loss: 0.9885 - val_mse: 0.9885\n",
      "Epoch 314/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.1008 - mse: 0.1008 - val_loss: 0.9851 - val_mse: 0.9851\n",
      "Epoch 315/1000\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0993 - mse: 0.0993 - val_loss: 0.9818 - val_mse: 0.9818\n",
      "Epoch 316/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.9785 - val_mse: 0.9785\n",
      "Epoch 317/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0963 - mse: 0.0963 - val_loss: 0.9752 - val_mse: 0.9752\n",
      "Epoch 318/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0949 - mse: 0.0949 - val_loss: 0.9719 - val_mse: 0.9719\n",
      "Epoch 319/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0935 - mse: 0.0935 - val_loss: 0.9687 - val_mse: 0.9687\n",
      "Epoch 320/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0922 - mse: 0.0922 - val_loss: 0.9656 - val_mse: 0.9656\n",
      "Epoch 321/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.9624 - val_mse: 0.9624\n",
      "Epoch 322/1000\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.9593 - val_mse: 0.9593\n",
      "Epoch 323/1000\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.9563 - val_mse: 0.9563\n",
      "Epoch 324/1000\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0871 - mse: 0.0871 - val_loss: 0.9533 - val_mse: 0.9533\n",
      "Epoch 325/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.9503 - val_mse: 0.9503\n",
      "Epoch 326/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0848 - mse: 0.0848 - val_loss: 0.9473 - val_mse: 0.9473\n",
      "Epoch 327/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0836 - mse: 0.0836 - val_loss: 0.9444 - val_mse: 0.9444\n",
      "Epoch 328/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0825 - mse: 0.0825 - val_loss: 0.9415 - val_mse: 0.9415\n",
      "Epoch 329/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0814 - mse: 0.0814 - val_loss: 0.9386 - val_mse: 0.9386\n",
      "Epoch 330/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0804 - mse: 0.0804 - val_loss: 0.9358 - val_mse: 0.9358\n",
      "Epoch 331/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0794 - mse: 0.0794 - val_loss: 0.9330 - val_mse: 0.9330\n",
      "Epoch 332/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0784 - mse: 0.0784 - val_loss: 0.9302 - val_mse: 0.9302\n",
      "Epoch 333/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0774 - mse: 0.0774 - val_loss: 0.9275 - val_mse: 0.9275\n",
      "Epoch 334/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0764 - mse: 0.0764 - val_loss: 0.9248 - val_mse: 0.9248\n",
      "Epoch 335/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0755 - mse: 0.0755 - val_loss: 0.9221 - val_mse: 0.9221\n",
      "Epoch 336/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0746 - mse: 0.0746 - val_loss: 0.9195 - val_mse: 0.9195\n",
      "Epoch 337/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0737 - mse: 0.0737 - val_loss: 0.9169 - val_mse: 0.9169\n",
      "Epoch 338/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0728 - mse: 0.0728 - val_loss: 0.9143 - val_mse: 0.9143\n",
      "Epoch 339/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.9118 - val_mse: 0.9118\n",
      "Epoch 340/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0712 - mse: 0.0712 - val_loss: 0.9093 - val_mse: 0.9093\n",
      "Epoch 341/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0704 - mse: 0.0704 - val_loss: 0.9068 - val_mse: 0.9068\n",
      "Epoch 342/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0696 - mse: 0.0696 - val_loss: 0.9043 - val_mse: 0.9043\n",
      "Epoch 343/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0688 - mse: 0.0688 - val_loss: 0.9019 - val_mse: 0.9019\n",
      "Epoch 344/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0681 - mse: 0.0681 - val_loss: 0.8995 - val_mse: 0.8995\n",
      "Epoch 345/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0673 - mse: 0.0673 - val_loss: 0.8972 - val_mse: 0.8972\n",
      "Epoch 346/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0666 - mse: 0.0666 - val_loss: 0.8948 - val_mse: 0.8948\n",
      "Epoch 347/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0659 - mse: 0.0659 - val_loss: 0.8925 - val_mse: 0.8925\n",
      "Epoch 348/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0652 - mse: 0.0652 - val_loss: 0.8902 - val_mse: 0.8902\n",
      "Epoch 349/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0645 - mse: 0.0645 - val_loss: 0.8880 - val_mse: 0.8880\n",
      "Epoch 350/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0639 - mse: 0.0639 - val_loss: 0.8857 - val_mse: 0.8857\n",
      "Epoch 351/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0632 - mse: 0.0632 - val_loss: 0.8835 - val_mse: 0.8835\n",
      "Epoch 352/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0626 - mse: 0.0626 - val_loss: 0.8813 - val_mse: 0.8813\n",
      "Epoch 353/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0620 - mse: 0.0620 - val_loss: 0.8792 - val_mse: 0.8792\n",
      "Epoch 354/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0614 - mse: 0.0614 - val_loss: 0.8771 - val_mse: 0.8771\n",
      "Epoch 355/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0608 - mse: 0.0608 - val_loss: 0.8749 - val_mse: 0.8749\n",
      "Epoch 356/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0602 - mse: 0.0602 - val_loss: 0.8729 - val_mse: 0.8729\n",
      "Epoch 357/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0596 - mse: 0.0596 - val_loss: 0.8708 - val_mse: 0.8708\n",
      "Epoch 358/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0590 - mse: 0.0590 - val_loss: 0.8688 - val_mse: 0.8688\n",
      "Epoch 359/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0585 - mse: 0.0585 - val_loss: 0.8668 - val_mse: 0.8668\n",
      "Epoch 360/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0579 - mse: 0.0579 - val_loss: 0.8648 - val_mse: 0.8648\n",
      "Epoch 361/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0574 - mse: 0.0574 - val_loss: 0.8628 - val_mse: 0.8628\n",
      "Epoch 362/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0569 - mse: 0.0569 - val_loss: 0.8608 - val_mse: 0.8608\n",
      "Epoch 363/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0564 - mse: 0.0564 - val_loss: 0.8589 - val_mse: 0.8589\n",
      "Epoch 364/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0559 - mse: 0.0559 - val_loss: 0.8570 - val_mse: 0.8570\n",
      "Epoch 365/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0554 - mse: 0.0554 - val_loss: 0.8551 - val_mse: 0.8551\n",
      "Epoch 366/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0549 - mse: 0.0549 - val_loss: 0.8532 - val_mse: 0.8532\n",
      "Epoch 367/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0544 - mse: 0.0544 - val_loss: 0.8514 - val_mse: 0.8514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0539 - mse: 0.0539 - val_loss: 0.8496 - val_mse: 0.8496\n",
      "Epoch 369/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0534 - mse: 0.0534 - val_loss: 0.8478 - val_mse: 0.8478\n",
      "Epoch 370/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0530 - mse: 0.0530 - val_loss: 0.8460 - val_mse: 0.8460\n",
      "Epoch 371/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0525 - mse: 0.0525 - val_loss: 0.8442 - val_mse: 0.8442\n",
      "Epoch 372/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0521 - mse: 0.0521 - val_loss: 0.8424 - val_mse: 0.8424\n",
      "Epoch 373/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0516 - mse: 0.0516 - val_loss: 0.8407 - val_mse: 0.8407\n",
      "Epoch 374/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0512 - mse: 0.0512 - val_loss: 0.8390 - val_mse: 0.8390\n",
      "Epoch 375/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0507 - mse: 0.0507 - val_loss: 0.8373 - val_mse: 0.8373\n",
      "Epoch 376/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0503 - mse: 0.0503 - val_loss: 0.8356 - val_mse: 0.8356\n",
      "Epoch 377/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0499 - mse: 0.0499 - val_loss: 0.8339 - val_mse: 0.8339\n",
      "Epoch 378/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0495 - mse: 0.0495 - val_loss: 0.8322 - val_mse: 0.8322\n",
      "Epoch 379/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0491 - mse: 0.0491 - val_loss: 0.8306 - val_mse: 0.8306\n",
      "Epoch 380/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0487 - mse: 0.0487 - val_loss: 0.8290 - val_mse: 0.8290\n",
      "Epoch 381/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0483 - mse: 0.0483 - val_loss: 0.8274 - val_mse: 0.8274\n",
      "Epoch 382/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0479 - mse: 0.0479 - val_loss: 0.8258 - val_mse: 0.8258\n",
      "Epoch 383/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0475 - mse: 0.0475 - val_loss: 0.8242 - val_mse: 0.8242\n",
      "Epoch 384/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0471 - mse: 0.0471 - val_loss: 0.8226 - val_mse: 0.8226\n",
      "Epoch 385/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0467 - mse: 0.0467 - val_loss: 0.8210 - val_mse: 0.8210\n",
      "Epoch 386/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0463 - mse: 0.0463 - val_loss: 0.8195 - val_mse: 0.8195\n",
      "Epoch 387/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0459 - mse: 0.0459 - val_loss: 0.8180 - val_mse: 0.8180\n",
      "Epoch 388/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0455 - mse: 0.0455 - val_loss: 0.8164 - val_mse: 0.8164\n",
      "Epoch 389/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0452 - mse: 0.0452 - val_loss: 0.8149 - val_mse: 0.8149\n",
      "Epoch 390/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0448 - mse: 0.0448 - val_loss: 0.8134 - val_mse: 0.8134\n",
      "Epoch 391/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0444 - mse: 0.0444 - val_loss: 0.8119 - val_mse: 0.8119\n",
      "Epoch 392/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0441 - mse: 0.0441 - val_loss: 0.8105 - val_mse: 0.8105\n",
      "Epoch 393/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0437 - mse: 0.0437 - val_loss: 0.8090 - val_mse: 0.8090\n",
      "Epoch 394/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0434 - mse: 0.0434 - val_loss: 0.8076 - val_mse: 0.8076\n",
      "Epoch 395/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.8061 - val_mse: 0.8061\n",
      "Epoch 396/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.8047 - val_mse: 0.8047\n",
      "Epoch 397/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0423 - mse: 0.0423 - val_loss: 0.8033 - val_mse: 0.8033\n",
      "Epoch 398/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0420 - mse: 0.0420 - val_loss: 0.8019 - val_mse: 0.8019\n",
      "Epoch 399/1000\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0416 - mse: 0.0416 - val_loss: 0.8005 - val_mse: 0.8005\n",
      "Epoch 400/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.7991 - val_mse: 0.7991\n",
      "Epoch 401/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.7977 - val_mse: 0.7977\n",
      "Epoch 402/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0406 - mse: 0.0406 - val_loss: 0.7963 - val_mse: 0.7963\n",
      "Epoch 403/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0403 - mse: 0.0403 - val_loss: 0.7949 - val_mse: 0.7949\n",
      "Epoch 404/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.7936 - val_mse: 0.7936\n",
      "Epoch 405/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.7922 - val_mse: 0.7922\n",
      "Epoch 406/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.7909 - val_mse: 0.7909\n",
      "Epoch 407/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.7896 - val_mse: 0.7896\n",
      "Epoch 408/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.7882 - val_mse: 0.7882\n",
      "Epoch 409/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.7869 - val_mse: 0.7869\n",
      "Epoch 410/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.7856 - val_mse: 0.7856\n",
      "Epoch 411/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.7843 - val_mse: 0.7843\n",
      "Epoch 412/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.7830 - val_mse: 0.7830\n",
      "Epoch 413/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0371 - mse: 0.0371 - val_loss: 0.7817 - val_mse: 0.7817\n",
      "Epoch 414/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.7805 - val_mse: 0.7805\n",
      "Epoch 415/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.7792 - val_mse: 0.7792\n",
      "Epoch 416/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.7779 - val_mse: 0.7779\n",
      "Epoch 417/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0359 - mse: 0.0359 - val_loss: 0.7767 - val_mse: 0.7767\n",
      "Epoch 418/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.7754 - val_mse: 0.7754\n",
      "Epoch 419/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.7742 - val_mse: 0.7742\n",
      "Epoch 420/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0350 - mse: 0.0350 - val_loss: 0.7729 - val_mse: 0.7729\n",
      "Epoch 421/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0347 - mse: 0.0347 - val_loss: 0.7717 - val_mse: 0.7717\n",
      "Epoch 422/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0345 - mse: 0.0345 - val_loss: 0.7705 - val_mse: 0.7705\n",
      "Epoch 423/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.7692 - val_mse: 0.7692\n",
      "Epoch 424/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0339 - mse: 0.0339 - val_loss: 0.7680 - val_mse: 0.7680\n",
      "Epoch 425/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.7668 - val_mse: 0.7668\n",
      "Epoch 426/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.7656 - val_mse: 0.7656\n",
      "Epoch 427/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.7644 - val_mse: 0.7644\n",
      "Epoch 428/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0328 - mse: 0.0328 - val_loss: 0.7632 - val_mse: 0.7632\n",
      "Epoch 429/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0325 - mse: 0.0325 - val_loss: 0.7620 - val_mse: 0.7620\n",
      "Epoch 430/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.7608 - val_mse: 0.7608\n",
      "Epoch 431/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.7596 - val_mse: 0.7596\n",
      "Epoch 432/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.7585 - val_mse: 0.7585\n",
      "Epoch 433/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.7573 - val_mse: 0.7573\n",
      "Epoch 434/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0311 - mse: 0.0311 - val_loss: 0.7561 - val_mse: 0.7561\n",
      "Epoch 435/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.7549 - val_mse: 0.7549\n",
      "Epoch 436/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.7538 - val_mse: 0.7538\n",
      "Epoch 437/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.7526 - val_mse: 0.7526\n",
      "Epoch 438/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.7515 - val_mse: 0.7515\n",
      "Epoch 439/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.7503 - val_mse: 0.7503\n",
      "Epoch 440/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.7492 - val_mse: 0.7492\n",
      "Epoch 441/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.7480 - val_mse: 0.7480\n",
      "Epoch 442/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.7469 - val_mse: 0.7469\n",
      "Epoch 443/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.7458 - val_mse: 0.7458\n",
      "Epoch 444/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.7446 - val_mse: 0.7446\n",
      "Epoch 445/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.7435 - val_mse: 0.7435\n",
      "Epoch 446/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.7424 - val_mse: 0.7424\n",
      "Epoch 447/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.7413 - val_mse: 0.7413\n",
      "Epoch 448/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.7402 - val_mse: 0.7402\n",
      "Epoch 449/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.7391 - val_mse: 0.7391\n",
      "Epoch 450/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.7380 - val_mse: 0.7380\n",
      "Epoch 451/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.7368 - val_mse: 0.7368\n",
      "Epoch 452/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.7357 - val_mse: 0.7357\n",
      "Epoch 453/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.7346 - val_mse: 0.7346\n",
      "Epoch 454/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.7336 - val_mse: 0.7336\n",
      "Epoch 455/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.7325 - val_mse: 0.7325\n",
      "Epoch 456/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.7314 - val_mse: 0.7314\n",
      "Epoch 457/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.7303 - val_mse: 0.7303\n",
      "Epoch 458/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.7292 - val_mse: 0.7292\n",
      "Epoch 459/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.7281 - val_mse: 0.7281\n",
      "Epoch 460/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.7271 - val_mse: 0.7271\n",
      "Epoch 461/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.7260 - val_mse: 0.7260\n",
      "Epoch 462/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0242 - mse: 0.0242 - val_loss: 0.7249 - val_mse: 0.7249\n",
      "Epoch 463/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0240 - mse: 0.0240 - val_loss: 0.7238 - val_mse: 0.7238\n",
      "Epoch 464/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0238 - mse: 0.0238 - val_loss: 0.7228 - val_mse: 0.7228\n",
      "Epoch 465/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.7217 - val_mse: 0.7217\n",
      "Epoch 466/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0233 - mse: 0.0233 - val_loss: 0.7207 - val_mse: 0.7207\n",
      "Epoch 467/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0231 - mse: 0.0231 - val_loss: 0.7196 - val_mse: 0.7196\n",
      "Epoch 468/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.7185 - val_mse: 0.7185\n",
      "Epoch 469/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0227 - mse: 0.0227 - val_loss: 0.7175 - val_mse: 0.7175\n",
      "Epoch 470/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 0.7164 - val_mse: 0.7164\n",
      "Epoch 471/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0222 - mse: 0.0222 - val_loss: 0.7154 - val_mse: 0.7154\n",
      "Epoch 472/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.7144 - val_mse: 0.7144\n",
      "Epoch 473/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.7133 - val_mse: 0.7133\n",
      "Epoch 474/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.7123 - val_mse: 0.7123\n",
      "Epoch 475/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.7112 - val_mse: 0.7112\n",
      "Epoch 476/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0212 - mse: 0.0212 - val_loss: 0.7102 - val_mse: 0.7102\n",
      "Epoch 477/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.7092 - val_mse: 0.7092\n",
      "Epoch 478/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.7082 - val_mse: 0.7082\n",
      "Epoch 479/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.7071 - val_mse: 0.7071\n",
      "Epoch 480/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.7061 - val_mse: 0.7061\n",
      "Epoch 481/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.7051 - val_mse: 0.7051\n",
      "Epoch 482/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.7041 - val_mse: 0.7041\n",
      "Epoch 483/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.7030 - val_mse: 0.7030\n",
      "Epoch 484/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.7020 - val_mse: 0.7020\n",
      "Epoch 485/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0194 - mse: 0.0194 - val_loss: 0.7010 - val_mse: 0.7010\n",
      "Epoch 486/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.7000 - val_mse: 0.7000\n",
      "Epoch 487/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.6990 - val_mse: 0.6990\n",
      "Epoch 488/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.6980 - val_mse: 0.6980\n",
      "Epoch 489/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0186 - mse: 0.0186 - val_loss: 0.6970 - val_mse: 0.6970\n",
      "Epoch 490/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.6960 - val_mse: 0.6960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.6950 - val_mse: 0.6950\n",
      "Epoch 492/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.6940 - val_mse: 0.6940\n",
      "Epoch 493/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.6930 - val_mse: 0.6930\n",
      "Epoch 494/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.6920 - val_mse: 0.6920\n",
      "Epoch 495/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.6910 - val_mse: 0.6910\n",
      "Epoch 496/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.6901 - val_mse: 0.6901\n",
      "Epoch 497/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.6891 - val_mse: 0.6891\n",
      "Epoch 498/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.6881 - val_mse: 0.6881\n",
      "Epoch 499/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.6871 - val_mse: 0.6871\n",
      "Epoch 500/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.6861 - val_mse: 0.6861\n",
      "Epoch 501/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.6852 - val_mse: 0.6852\n",
      "Epoch 502/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.6842 - val_mse: 0.6842\n",
      "Epoch 503/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.6832 - val_mse: 0.6832\n",
      "Epoch 504/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.6823 - val_mse: 0.6823\n",
      "Epoch 505/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.6813 - val_mse: 0.6813\n",
      "Epoch 506/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.6803 - val_mse: 0.6803\n",
      "Epoch 507/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.6794 - val_mse: 0.6794\n",
      "Epoch 508/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.6784 - val_mse: 0.6784\n",
      "Epoch 509/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.6775 - val_mse: 0.6775\n",
      "Epoch 510/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.6765 - val_mse: 0.6765\n",
      "Epoch 511/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.6755 - val_mse: 0.6755\n",
      "Epoch 512/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.6746 - val_mse: 0.6746\n",
      "Epoch 513/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.6737 - val_mse: 0.6737\n",
      "Epoch 514/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.6727 - val_mse: 0.6727\n",
      "Epoch 515/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.6718 - val_mse: 0.6718\n",
      "Epoch 516/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.6708 - val_mse: 0.6708\n",
      "Epoch 517/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.6699 - val_mse: 0.6699\n",
      "Epoch 518/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.6690 - val_mse: 0.6690\n",
      "Epoch 519/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.6680 - val_mse: 0.6680\n",
      "Epoch 520/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.6671 - val_mse: 0.6671\n",
      "Epoch 521/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.6662 - val_mse: 0.6662\n",
      "Epoch 522/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.6652 - val_mse: 0.6652\n",
      "Epoch 523/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.6643 - val_mse: 0.6643\n",
      "Epoch 524/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.6634 - val_mse: 0.6634\n",
      "Epoch 525/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.6625 - val_mse: 0.6625\n",
      "Epoch 526/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.6616 - val_mse: 0.6616\n",
      "Epoch 527/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.6606 - val_mse: 0.6606\n",
      "Epoch 528/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.6597 - val_mse: 0.6597\n",
      "Epoch 529/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.6588 - val_mse: 0.6588\n",
      "Epoch 530/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.6579 - val_mse: 0.6579\n",
      "Epoch 531/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.6570 - val_mse: 0.6570\n",
      "Epoch 532/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.6561 - val_mse: 0.6561\n",
      "Epoch 533/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.6552 - val_mse: 0.6552\n",
      "Epoch 534/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.6543 - val_mse: 0.6543\n",
      "Epoch 535/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.6534 - val_mse: 0.6534\n",
      "Epoch 536/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.6525 - val_mse: 0.6525\n",
      "Epoch 537/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.6516 - val_mse: 0.6516\n",
      "Epoch 538/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.6508 - val_mse: 0.6508\n",
      "Epoch 539/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.6499 - val_mse: 0.6499\n",
      "Epoch 540/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.6490 - val_mse: 0.6490\n",
      "Epoch 541/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.6481 - val_mse: 0.6481\n",
      "Epoch 542/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.6472 - val_mse: 0.6472\n",
      "Epoch 543/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.6464 - val_mse: 0.6464\n",
      "Epoch 544/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.6455 - val_mse: 0.6455\n",
      "Epoch 545/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 546/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.6438 - val_mse: 0.6438\n",
      "Epoch 547/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.6429 - val_mse: 0.6429\n",
      "Epoch 548/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.6420 - val_mse: 0.6420\n",
      "Epoch 549/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.6412 - val_mse: 0.6412\n",
      "Epoch 550/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.6403 - val_mse: 0.6403\n",
      "Epoch 551/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.6395 - val_mse: 0.6395\n",
      "Epoch 552/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.6386 - val_mse: 0.6386\n",
      "Epoch 553/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.6378 - val_mse: 0.6378\n",
      "Epoch 554/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.6369 - val_mse: 0.6369\n",
      "Epoch 555/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.6361 - val_mse: 0.6361\n",
      "Epoch 556/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.6352 - val_mse: 0.6352\n",
      "Epoch 557/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.6344 - val_mse: 0.6344\n",
      "Epoch 558/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.6336 - val_mse: 0.6336\n",
      "Epoch 559/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.6327 - val_mse: 0.6327\n",
      "Epoch 560/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.6319 - val_mse: 0.6319\n",
      "Epoch 561/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.6311 - val_mse: 0.6311\n",
      "Epoch 562/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.6303 - val_mse: 0.6303\n",
      "Epoch 563/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.6294 - val_mse: 0.6294\n",
      "Epoch 564/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.6286 - val_mse: 0.6286\n",
      "Epoch 565/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.6278 - val_mse: 0.6278\n",
      "Epoch 566/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.6270 - val_mse: 0.6270\n",
      "Epoch 567/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.6262 - val_mse: 0.6262\n",
      "Epoch 568/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.6254 - val_mse: 0.6254\n",
      "Epoch 569/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.6246 - val_mse: 0.6246\n",
      "Epoch 570/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.6238 - val_mse: 0.6238\n",
      "Epoch 571/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.6230 - val_mse: 0.6230\n",
      "Epoch 572/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.6222 - val_mse: 0.6222\n",
      "Epoch 573/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.6214 - val_mse: 0.6214\n",
      "Epoch 574/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.6206 - val_mse: 0.6206\n",
      "Epoch 575/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.6198 - val_mse: 0.6198\n",
      "Epoch 576/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.6190 - val_mse: 0.6190\n",
      "Epoch 577/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.6183 - val_mse: 0.6183\n",
      "Epoch 578/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.6175 - val_mse: 0.6175\n",
      "Epoch 579/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.6167 - val_mse: 0.6167\n",
      "Epoch 580/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.6159 - val_mse: 0.6159\n",
      "Epoch 581/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.6152 - val_mse: 0.6152\n",
      "Epoch 582/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.6144 - val_mse: 0.6144\n",
      "Epoch 583/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.6136 - val_mse: 0.6136\n",
      "Epoch 584/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.6129 - val_mse: 0.6129\n",
      "Epoch 585/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.6121 - val_mse: 0.6121\n",
      "Epoch 586/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.6114 - val_mse: 0.6114\n",
      "Epoch 587/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.6106 - val_mse: 0.6106\n",
      "Epoch 588/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.6099 - val_mse: 0.6099\n",
      "Epoch 589/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.6091 - val_mse: 0.6091\n",
      "Epoch 590/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.6084 - val_mse: 0.6084\n",
      "Epoch 591/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.6076 - val_mse: 0.6076\n",
      "Epoch 592/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.6069 - val_mse: 0.6069\n",
      "Epoch 593/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.6062 - val_mse: 0.6062\n",
      "Epoch 594/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.6054 - val_mse: 0.6054\n",
      "Epoch 595/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.6047 - val_mse: 0.6047\n",
      "Epoch 596/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.6040 - val_mse: 0.6040\n",
      "Epoch 597/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.6033 - val_mse: 0.6033\n",
      "Epoch 598/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.6026 - val_mse: 0.6026\n",
      "Epoch 599/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.6018 - val_mse: 0.6018\n",
      "Epoch 600/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.6011 - val_mse: 0.6011\n",
      "Epoch 601/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.6004 - val_mse: 0.6004\n",
      "Epoch 602/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.5997 - val_mse: 0.5997\n",
      "Epoch 603/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.5990 - val_mse: 0.5990\n",
      "Epoch 604/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.5983 - val_mse: 0.5983\n",
      "Epoch 605/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.5976 - val_mse: 0.5976\n",
      "Epoch 606/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.5969 - val_mse: 0.5969\n",
      "Epoch 607/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.5962 - val_mse: 0.5962\n",
      "Epoch 608/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.5955 - val_mse: 0.5955\n",
      "Epoch 609/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.5949 - val_mse: 0.5949\n",
      "Epoch 610/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.5942 - val_mse: 0.5942\n",
      "Epoch 611/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.5935 - val_mse: 0.5935\n",
      "Epoch 612/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.5928 - val_mse: 0.5928\n",
      "Epoch 613/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.5922 - val_mse: 0.5922\n",
      "Epoch 614/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.5915 - val_mse: 0.5915\n",
      "Epoch 615/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.5908 - val_mse: 0.5908\n",
      "Epoch 616/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.5902 - val_mse: 0.5902\n",
      "Epoch 617/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.5895 - val_mse: 0.5895\n",
      "Epoch 618/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.5889 - val_mse: 0.5889\n",
      "Epoch 619/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.5882 - val_mse: 0.5882\n",
      "Epoch 620/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.5876 - val_mse: 0.5876\n",
      "Epoch 621/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.5869 - val_mse: 0.5869\n",
      "Epoch 622/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.5863 - val_mse: 0.5863\n",
      "Epoch 623/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.5856 - val_mse: 0.5856\n",
      "Epoch 624/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.5850 - val_mse: 0.5850\n",
      "Epoch 625/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.5844 - val_mse: 0.5844\n",
      "Epoch 626/1000\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.5837 - val_mse: 0.5837\n",
      "Epoch 627/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.5831 - val_mse: 0.5831\n",
      "Epoch 628/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.5825 - val_mse: 0.5825\n",
      "Epoch 629/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.5819 - val_mse: 0.5819\n",
      "Epoch 630/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.5813 - val_mse: 0.5813\n",
      "Epoch 631/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.5806 - val_mse: 0.5806\n",
      "Epoch 632/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.5800 - val_mse: 0.5800\n",
      "Epoch 633/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.5794 - val_mse: 0.5794\n",
      "Epoch 634/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.5788 - val_mse: 0.5788\n",
      "Epoch 635/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.5782 - val_mse: 0.5782\n",
      "Epoch 636/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.5776 - val_mse: 0.5776\n",
      "Epoch 637/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.5770 - val_mse: 0.5770\n",
      "Epoch 638/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.5765 - val_mse: 0.5765\n",
      "Epoch 639/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.5759 - val_mse: 0.5759\n",
      "Epoch 640/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.5753 - val_mse: 0.5753\n",
      "Epoch 641/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.5747 - val_mse: 0.5747\n",
      "Epoch 642/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.5741 - val_mse: 0.5741\n",
      "Epoch 643/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.5736 - val_mse: 0.5736\n",
      "Epoch 644/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.5730 - val_mse: 0.5730\n",
      "Epoch 645/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.5724 - val_mse: 0.5724\n",
      "Epoch 646/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.5719 - val_mse: 0.5719\n",
      "Epoch 647/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.5713 - val_mse: 0.5713\n",
      "Epoch 648/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.5707 - val_mse: 0.5707\n",
      "Epoch 649/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.5702 - val_mse: 0.5702\n",
      "Epoch 650/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.5696 - val_mse: 0.5696\n",
      "Epoch 651/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.5691 - val_mse: 0.5691\n",
      "Epoch 652/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.5685 - val_mse: 0.5685\n",
      "Epoch 653/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.5680 - val_mse: 0.5680\n",
      "Epoch 654/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.5675 - val_mse: 0.5675\n",
      "Epoch 655/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.5669 - val_mse: 0.5669\n",
      "Epoch 656/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.5664 - val_mse: 0.5664\n",
      "Epoch 657/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.5659 - val_mse: 0.5659\n",
      "Epoch 658/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.5654 - val_mse: 0.5654\n",
      "Epoch 659/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.5648 - val_mse: 0.5648\n",
      "Epoch 660/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.5643 - val_mse: 0.5643\n",
      "Epoch 661/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.5638 - val_mse: 0.5638\n",
      "Epoch 662/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.5633 - val_mse: 0.5633\n",
      "Epoch 663/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.5628 - val_mse: 0.5628\n",
      "Epoch 664/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.5623 - val_mse: 0.5623\n",
      "Epoch 665/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.5618 - val_mse: 0.5618\n",
      "Epoch 666/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.5613 - val_mse: 0.5613\n",
      "Epoch 667/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.5608 - val_mse: 0.5608\n",
      "Epoch 668/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.5603 - val_mse: 0.5603\n",
      "Epoch 669/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.5598 - val_mse: 0.5598\n",
      "Epoch 670/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.5593 - val_mse: 0.5593\n",
      "Epoch 671/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.5588 - val_mse: 0.5588\n",
      "Epoch 672/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.5583 - val_mse: 0.5583\n",
      "Epoch 673/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.5579 - val_mse: 0.5579\n",
      "Epoch 674/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.5574 - val_mse: 0.5574\n",
      "Epoch 675/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.5569 - val_mse: 0.5569\n",
      "Epoch 676/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.5565 - val_mse: 0.5565\n",
      "Epoch 677/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.5560 - val_mse: 0.5560\n",
      "Epoch 678/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.5555 - val_mse: 0.5555\n",
      "Epoch 679/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.5551 - val_mse: 0.5551\n",
      "Epoch 680/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.5546 - val_mse: 0.5546\n",
      "Epoch 681/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.5542 - val_mse: 0.5542\n",
      "Epoch 682/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.5537 - val_mse: 0.5537\n",
      "Epoch 683/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.5533 - val_mse: 0.5533\n",
      "Epoch 684/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.5528 - val_mse: 0.5528\n",
      "Epoch 685/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.5524 - val_mse: 0.5524\n",
      "Epoch 686/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.5520 - val_mse: 0.5520\n",
      "Epoch 687/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.5515 - val_mse: 0.5515\n",
      "Epoch 688/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.5511 - val_mse: 0.5511\n",
      "Epoch 689/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.8095e-04 - mse: 9.8095e-04 - val_loss: 0.5507 - val_mse: 0.5507\n",
      "Epoch 690/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 9.6174e-04 - mse: 9.6174e-04 - val_loss: 0.5503 - val_mse: 0.5503\n",
      "Epoch 691/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.4285e-04 - mse: 9.4285e-04 - val_loss: 0.5498 - val_mse: 0.5498\n",
      "Epoch 692/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.2430e-04 - mse: 9.2430e-04 - val_loss: 0.5494 - val_mse: 0.5494\n",
      "Epoch 693/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.0606e-04 - mse: 9.0606e-04 - val_loss: 0.5490 - val_mse: 0.5490\n",
      "Epoch 694/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.8814e-04 - mse: 8.8814e-04 - val_loss: 0.5486 - val_mse: 0.5486\n",
      "Epoch 695/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.7054e-04 - mse: 8.7054e-04 - val_loss: 0.5482 - val_mse: 0.5482\n",
      "Epoch 696/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 8.5324e-04 - mse: 8.5324e-04 - val_loss: 0.5478 - val_mse: 0.5478\n",
      "Epoch 697/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.3625e-04 - mse: 8.3625e-04 - val_loss: 0.5474 - val_mse: 0.5474\n",
      "Epoch 698/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 8.1956e-04 - mse: 8.1956e-04 - val_loss: 0.5470 - val_mse: 0.5470\n",
      "Epoch 699/1000\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 8.0316e-04 - mse: 8.0316e-04 - val_loss: 0.5466 - val_mse: 0.5466\n",
      "Epoch 700/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.8706e-04 - mse: 7.8706e-04 - val_loss: 0.5462 - val_mse: 0.5462\n",
      "Epoch 701/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.7124e-04 - mse: 7.7124e-04 - val_loss: 0.5458 - val_mse: 0.5458\n",
      "Epoch 702/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.5570e-04 - mse: 7.5570e-04 - val_loss: 0.5454 - val_mse: 0.5454\n",
      "Epoch 703/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.4045e-04 - mse: 7.4045e-04 - val_loss: 0.5450 - val_mse: 0.5450\n",
      "Epoch 704/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.2547e-04 - mse: 7.2547e-04 - val_loss: 0.5446 - val_mse: 0.5446\n",
      "Epoch 705/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.1076e-04 - mse: 7.1076e-04 - val_loss: 0.5443 - val_mse: 0.5443\n",
      "Epoch 706/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.9631e-04 - mse: 6.9631e-04 - val_loss: 0.5439 - val_mse: 0.5439\n",
      "Epoch 707/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.8213e-04 - mse: 6.8213e-04 - val_loss: 0.5435 - val_mse: 0.5435\n",
      "Epoch 708/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.6821e-04 - mse: 6.6821e-04 - val_loss: 0.5432 - val_mse: 0.5432\n",
      "Epoch 709/1000\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 6.5454e-04 - mse: 6.5454e-04 - val_loss: 0.5428 - val_mse: 0.5428\n",
      "Epoch 710/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.4112e-04 - mse: 6.4112e-04 - val_loss: 0.5424 - val_mse: 0.5424\n",
      "Epoch 711/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.2796e-04 - mse: 6.2796e-04 - val_loss: 0.5421 - val_mse: 0.5421\n",
      "Epoch 712/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.1503e-04 - mse: 6.1503e-04 - val_loss: 0.5417 - val_mse: 0.5417\n",
      "Epoch 713/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.0235e-04 - mse: 6.0235e-04 - val_loss: 0.5413 - val_mse: 0.5413\n",
      "Epoch 714/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.8990e-04 - mse: 5.8990e-04 - val_loss: 0.5410 - val_mse: 0.5410\n",
      "Epoch 715/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.7768e-04 - mse: 5.7768e-04 - val_loss: 0.5406 - val_mse: 0.5406\n",
      "Epoch 716/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.6569e-04 - mse: 5.6569e-04 - val_loss: 0.5403 - val_mse: 0.5403\n",
      "Epoch 717/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.5393e-04 - mse: 5.5393e-04 - val_loss: 0.5399 - val_mse: 0.5399\n",
      "Epoch 718/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.4239e-04 - mse: 5.4239e-04 - val_loss: 0.5396 - val_mse: 0.5396\n",
      "Epoch 719/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.3107e-04 - mse: 5.3107e-04 - val_loss: 0.5393 - val_mse: 0.5393\n",
      "Epoch 720/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.1996e-04 - mse: 5.1996e-04 - val_loss: 0.5389 - val_mse: 0.5389\n",
      "Epoch 721/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.0906e-04 - mse: 5.0906e-04 - val_loss: 0.5386 - val_mse: 0.5386\n",
      "Epoch 722/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.9837e-04 - mse: 4.9837e-04 - val_loss: 0.5383 - val_mse: 0.5383\n",
      "Epoch 723/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.8789e-04 - mse: 4.8789e-04 - val_loss: 0.5379 - val_mse: 0.5379\n",
      "Epoch 724/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.7760e-04 - mse: 4.7760e-04 - val_loss: 0.5376 - val_mse: 0.5376\n",
      "Epoch 725/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.6751e-04 - mse: 4.6751e-04 - val_loss: 0.5373 - val_mse: 0.5373\n",
      "Epoch 726/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.5763e-04 - mse: 4.5763e-04 - val_loss: 0.5370 - val_mse: 0.5370\n",
      "Epoch 727/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.4792e-04 - mse: 4.4792e-04 - val_loss: 0.5367 - val_mse: 0.5367\n",
      "Epoch 728/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.3841e-04 - mse: 4.3841e-04 - val_loss: 0.5363 - val_mse: 0.5363\n",
      "Epoch 729/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.2908e-04 - mse: 4.2908e-04 - val_loss: 0.5360 - val_mse: 0.5360\n",
      "Epoch 730/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.1994e-04 - mse: 4.1994e-04 - val_loss: 0.5357 - val_mse: 0.5357\n",
      "Epoch 731/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.1097e-04 - mse: 4.1097e-04 - val_loss: 0.5354 - val_mse: 0.5354\n",
      "Epoch 732/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 4.0218e-04 - mse: 4.0218e-04 - val_loss: 0.5351 - val_mse: 0.5351\n",
      "Epoch 733/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 25ms/step - loss: 3.9357e-04 - mse: 3.9357e-04 - val_loss: 0.5348 - val_mse: 0.5348\n",
      "Epoch 734/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 3.8512e-04 - mse: 3.8512e-04 - val_loss: 0.5345 - val_mse: 0.5345\n",
      "Epoch 735/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.7684e-04 - mse: 3.7684e-04 - val_loss: 0.5342 - val_mse: 0.5342\n",
      "Epoch 736/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.6872e-04 - mse: 3.6872e-04 - val_loss: 0.5339 - val_mse: 0.5339\n",
      "Epoch 737/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.6077e-04 - mse: 3.6077e-04 - val_loss: 0.5336 - val_mse: 0.5336\n",
      "Epoch 738/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.5297e-04 - mse: 3.5297e-04 - val_loss: 0.5333 - val_mse: 0.5333\n",
      "Epoch 739/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.4533e-04 - mse: 3.4533e-04 - val_loss: 0.5331 - val_mse: 0.5331\n",
      "Epoch 740/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 3.3785e-04 - mse: 3.3785e-04 - val_loss: 0.5328 - val_mse: 0.5328\n",
      "Epoch 741/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.3051e-04 - mse: 3.3051e-04 - val_loss: 0.5325 - val_mse: 0.5325\n",
      "Epoch 742/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.2332e-04 - mse: 3.2332e-04 - val_loss: 0.5322 - val_mse: 0.5322\n",
      "Epoch 743/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 3.1628e-04 - mse: 3.1628e-04 - val_loss: 0.5319 - val_mse: 0.5319\n",
      "Epoch 744/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.0938e-04 - mse: 3.0938e-04 - val_loss: 0.5317 - val_mse: 0.5317\n",
      "Epoch 745/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.0262e-04 - mse: 3.0262e-04 - val_loss: 0.5314 - val_mse: 0.5314\n",
      "Epoch 746/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2.9599e-04 - mse: 2.9599e-04 - val_loss: 0.5311 - val_mse: 0.5311\n",
      "Epoch 747/1000\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 2.8950e-04 - mse: 2.8950e-04 - val_loss: 0.5308 - val_mse: 0.5308\n",
      "Epoch 748/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2.8315e-04 - mse: 2.8315e-04 - val_loss: 0.5306 - val_mse: 0.5306\n",
      "Epoch 749/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.7692e-04 - mse: 2.7692e-04 - val_loss: 0.5303 - val_mse: 0.5303\n",
      "Epoch 750/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2.7082e-04 - mse: 2.7082e-04 - val_loss: 0.5301 - val_mse: 0.5301\n",
      "Epoch 751/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2.6485e-04 - mse: 2.6485e-04 - val_loss: 0.5298 - val_mse: 0.5298\n",
      "Epoch 752/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2.5900e-04 - mse: 2.5900e-04 - val_loss: 0.5296 - val_mse: 0.5296\n",
      "Epoch 753/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 2.5327e-04 - mse: 2.5327e-04 - val_loss: 0.5293 - val_mse: 0.5293\n",
      "Epoch 754/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2.4767e-04 - mse: 2.4767e-04 - val_loss: 0.5290 - val_mse: 0.5290\n",
      "Epoch 755/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2.4217e-04 - mse: 2.4217e-04 - val_loss: 0.5288 - val_mse: 0.5288\n",
      "Epoch 756/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2.3679e-04 - mse: 2.3679e-04 - val_loss: 0.5286 - val_mse: 0.5286\n",
      "Epoch 757/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2.3153e-04 - mse: 2.3153e-04 - val_loss: 0.5283 - val_mse: 0.5283\n",
      "Epoch 758/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2.2637e-04 - mse: 2.2637e-04 - val_loss: 0.5281 - val_mse: 0.5281\n",
      "Epoch 759/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2.2132e-04 - mse: 2.2132e-04 - val_loss: 0.5278 - val_mse: 0.5278\n",
      "Epoch 760/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2.1638e-04 - mse: 2.1638e-04 - val_loss: 0.5276 - val_mse: 0.5276\n",
      "Epoch 761/1000\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 2.1154e-04 - mse: 2.1154e-04 - val_loss: 0.5274 - val_mse: 0.5274\n",
      "Epoch 762/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2.0681e-04 - mse: 2.0681e-04 - val_loss: 0.5271 - val_mse: 0.5271\n",
      "Epoch 763/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2.0217e-04 - mse: 2.0217e-04 - val_loss: 0.5269 - val_mse: 0.5269\n",
      "Epoch 764/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.9763e-04 - mse: 1.9763e-04 - val_loss: 0.5267 - val_mse: 0.5267\n",
      "Epoch 765/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1.9319e-04 - mse: 1.9319e-04 - val_loss: 0.5264 - val_mse: 0.5264\n",
      "Epoch 766/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1.8884e-04 - mse: 1.8884e-04 - val_loss: 0.5262 - val_mse: 0.5262\n",
      "Epoch 767/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 1.8459e-04 - mse: 1.8459e-04 - val_loss: 0.5260 - val_mse: 0.5260\n",
      "Epoch 768/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 1.8042e-04 - mse: 1.8042e-04 - val_loss: 0.5258 - val_mse: 0.5258\n",
      "Epoch 769/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.7635e-04 - mse: 1.7635e-04 - val_loss: 0.5256 - val_mse: 0.5256\n",
      "Epoch 770/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.7236e-04 - mse: 1.7236e-04 - val_loss: 0.5253 - val_mse: 0.5253\n",
      "Epoch 771/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.6846e-04 - mse: 1.6846e-04 - val_loss: 0.5251 - val_mse: 0.5251\n",
      "Epoch 772/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 1.6464e-04 - mse: 1.6464e-04 - val_loss: 0.5249 - val_mse: 0.5249\n",
      "Epoch 773/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 1.6090e-04 - mse: 1.6090e-04 - val_loss: 0.5247 - val_mse: 0.5247\n",
      "Epoch 774/1000\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 1.5725e-04 - mse: 1.5725e-04 - val_loss: 0.5245 - val_mse: 0.5245\n",
      "Epoch 775/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 1.5367e-04 - mse: 1.5367e-04 - val_loss: 0.5243 - val_mse: 0.5243\n",
      "Epoch 776/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1.5017e-04 - mse: 1.5017e-04 - val_loss: 0.5241 - val_mse: 0.5241\n",
      "Epoch 777/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 1.4675e-04 - mse: 1.4675e-04 - val_loss: 0.5239 - val_mse: 0.5239\n",
      "Epoch 778/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 1.4340e-04 - mse: 1.4340e-04 - val_loss: 0.5237 - val_mse: 0.5237\n",
      "Epoch 779/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1.4012e-04 - mse: 1.4012e-04 - val_loss: 0.5235 - val_mse: 0.5235\n",
      "Epoch 780/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.3692e-04 - mse: 1.3692e-04 - val_loss: 0.5233 - val_mse: 0.5233\n",
      "Epoch 781/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 1.3379e-04 - mse: 1.3379e-04 - val_loss: 0.5231 - val_mse: 0.5231\n",
      "Epoch 782/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1.3072e-04 - mse: 1.3072e-04 - val_loss: 0.5229 - val_mse: 0.5229\n",
      "Epoch 783/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.2772e-04 - mse: 1.2772e-04 - val_loss: 0.5227 - val_mse: 0.5227\n",
      "Epoch 784/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.2479e-04 - mse: 1.2479e-04 - val_loss: 0.5225 - val_mse: 0.5225\n",
      "Epoch 785/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.2192e-04 - mse: 1.2192e-04 - val_loss: 0.5224 - val_mse: 0.5224\n",
      "Epoch 786/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.1911e-04 - mse: 1.1911e-04 - val_loss: 0.5222 - val_mse: 0.5222\n",
      "Epoch 787/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1.1637e-04 - mse: 1.1637e-04 - val_loss: 0.5220 - val_mse: 0.5220\n",
      "Epoch 788/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.1369e-04 - mse: 1.1369e-04 - val_loss: 0.5218 - val_mse: 0.5218\n",
      "Epoch 789/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1.1106e-04 - mse: 1.1106e-04 - val_loss: 0.5216 - val_mse: 0.5216\n",
      "Epoch 790/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.0850e-04 - mse: 1.0850e-04 - val_loss: 0.5215 - val_mse: 0.5215\n",
      "Epoch 791/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1.0599e-04 - mse: 1.0599e-04 - val_loss: 0.5213 - val_mse: 0.5213\n",
      "Epoch 792/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.0354e-04 - mse: 1.0354e-04 - val_loss: 0.5211 - val_mse: 0.5211\n",
      "Epoch 793/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.0114e-04 - mse: 1.0114e-04 - val_loss: 0.5209 - val_mse: 0.5209\n",
      "Epoch 794/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.8791e-05 - mse: 9.8791e-05 - val_loss: 0.5208 - val_mse: 0.5208\n",
      "Epoch 795/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.6498e-05 - mse: 9.6498e-05 - val_loss: 0.5206 - val_mse: 0.5206\n",
      "Epoch 796/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.4257e-05 - mse: 9.4257e-05 - val_loss: 0.5204 - val_mse: 0.5204\n",
      "Epoch 797/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.2065e-05 - mse: 9.2065e-05 - val_loss: 0.5203 - val_mse: 0.5203\n",
      "Epoch 798/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.9923e-05 - mse: 8.9923e-05 - val_loss: 0.5201 - val_mse: 0.5201\n",
      "Epoch 799/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.7827e-05 - mse: 8.7827e-05 - val_loss: 0.5199 - val_mse: 0.5199\n",
      "Epoch 800/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.5780e-05 - mse: 8.5780e-05 - val_loss: 0.5198 - val_mse: 0.5198\n",
      "Epoch 801/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.3779e-05 - mse: 8.3779e-05 - val_loss: 0.5196 - val_mse: 0.5196\n",
      "Epoch 802/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.1822e-05 - mse: 8.1822e-05 - val_loss: 0.5195 - val_mse: 0.5195\n",
      "Epoch 803/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.9909e-05 - mse: 7.9909e-05 - val_loss: 0.5193 - val_mse: 0.5193\n",
      "Epoch 804/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.8040e-05 - mse: 7.8040e-05 - val_loss: 0.5192 - val_mse: 0.5192\n",
      "Epoch 805/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.6213e-05 - mse: 7.6213e-05 - val_loss: 0.5190 - val_mse: 0.5190\n",
      "Epoch 806/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.4427e-05 - mse: 7.4427e-05 - val_loss: 0.5189 - val_mse: 0.5189\n",
      "Epoch 807/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.2681e-05 - mse: 7.2681e-05 - val_loss: 0.5187 - val_mse: 0.5187\n",
      "Epoch 808/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.0975e-05 - mse: 7.0975e-05 - val_loss: 0.5186 - val_mse: 0.5186\n",
      "Epoch 809/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.9307e-05 - mse: 6.9307e-05 - val_loss: 0.5184 - val_mse: 0.5184\n",
      "Epoch 810/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.7678e-05 - mse: 6.7678e-05 - val_loss: 0.5183 - val_mse: 0.5183\n",
      "Epoch 811/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.6085e-05 - mse: 6.6085e-05 - val_loss: 0.5181 - val_mse: 0.5181\n",
      "Epoch 812/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.4529e-05 - mse: 6.4529e-05 - val_loss: 0.5180 - val_mse: 0.5180\n",
      "Epoch 813/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.3008e-05 - mse: 6.3008e-05 - val_loss: 0.5179 - val_mse: 0.5179\n",
      "Epoch 814/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.1523e-05 - mse: 6.1523e-05 - val_loss: 0.5177 - val_mse: 0.5177\n",
      "Epoch 815/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.0070e-05 - mse: 6.0070e-05 - val_loss: 0.5176 - val_mse: 0.5176\n",
      "Epoch 816/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.8651e-05 - mse: 5.8651e-05 - val_loss: 0.5175 - val_mse: 0.5175\n",
      "Epoch 817/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.7264e-05 - mse: 5.7264e-05 - val_loss: 0.5173 - val_mse: 0.5173\n",
      "Epoch 818/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.5909e-05 - mse: 5.5909e-05 - val_loss: 0.5172 - val_mse: 0.5172\n",
      "Epoch 819/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.4585e-05 - mse: 5.4585e-05 - val_loss: 0.5171 - val_mse: 0.5171\n",
      "Epoch 820/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.3292e-05 - mse: 5.3292e-05 - val_loss: 0.5169 - val_mse: 0.5169\n",
      "Epoch 821/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.2027e-05 - mse: 5.2027e-05 - val_loss: 0.5168 - val_mse: 0.5168\n",
      "Epoch 822/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.0792e-05 - mse: 5.0792e-05 - val_loss: 0.5167 - val_mse: 0.5167\n",
      "Epoch 823/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.9586e-05 - mse: 4.9586e-05 - val_loss: 0.5166 - val_mse: 0.5166\n",
      "Epoch 824/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.8408e-05 - mse: 4.8408e-05 - val_loss: 0.5164 - val_mse: 0.5164\n",
      "Epoch 825/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.7255e-05 - mse: 4.7255e-05 - val_loss: 0.5163 - val_mse: 0.5163\n",
      "Epoch 826/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.6130e-05 - mse: 4.6130e-05 - val_loss: 0.5162 - val_mse: 0.5162\n",
      "Epoch 827/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.5030e-05 - mse: 4.5030e-05 - val_loss: 0.5161 - val_mse: 0.5161\n",
      "Epoch 828/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.3957e-05 - mse: 4.3957e-05 - val_loss: 0.5160 - val_mse: 0.5160\n",
      "Epoch 829/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.2908e-05 - mse: 4.2908e-05 - val_loss: 0.5158 - val_mse: 0.5158\n",
      "Epoch 830/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.1883e-05 - mse: 4.1883e-05 - val_loss: 0.5157 - val_mse: 0.5157\n",
      "Epoch 831/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.0882e-05 - mse: 4.0882e-05 - val_loss: 0.5156 - val_mse: 0.5156\n",
      "Epoch 832/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.9904e-05 - mse: 3.9904e-05 - val_loss: 0.5155 - val_mse: 0.5155\n",
      "Epoch 833/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.8949e-05 - mse: 3.8949e-05 - val_loss: 0.5154 - val_mse: 0.5154\n",
      "Epoch 834/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.8016e-05 - mse: 3.8016e-05 - val_loss: 0.5153 - val_mse: 0.5153\n",
      "Epoch 835/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.7104e-05 - mse: 3.7104e-05 - val_loss: 0.5152 - val_mse: 0.5152\n",
      "Epoch 836/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.6213e-05 - mse: 3.6213e-05 - val_loss: 0.5151 - val_mse: 0.5151\n",
      "Epoch 837/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.5344e-05 - mse: 3.5344e-05 - val_loss: 0.5150 - val_mse: 0.5150\n",
      "Epoch 838/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.4494e-05 - mse: 3.4494e-05 - val_loss: 0.5149 - val_mse: 0.5149\n",
      "Epoch 839/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.3665e-05 - mse: 3.3665e-05 - val_loss: 0.5148 - val_mse: 0.5148\n",
      "Epoch 840/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.2855e-05 - mse: 3.2855e-05 - val_loss: 0.5146 - val_mse: 0.5146\n",
      "Epoch 841/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.2063e-05 - mse: 3.2063e-05 - val_loss: 0.5145 - val_mse: 0.5145\n",
      "Epoch 842/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.1290e-05 - mse: 3.1290e-05 - val_loss: 0.5144 - val_mse: 0.5144\n",
      "Epoch 843/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.0535e-05 - mse: 3.0535e-05 - val_loss: 0.5143 - val_mse: 0.5143\n",
      "Epoch 844/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.9798e-05 - mse: 2.9798e-05 - val_loss: 0.5143 - val_mse: 0.5143\n",
      "Epoch 845/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2.9077e-05 - mse: 2.9077e-05 - val_loss: 0.5142 - val_mse: 0.5142\n",
      "Epoch 846/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.8373e-05 - mse: 2.8373e-05 - val_loss: 0.5141 - val_mse: 0.5141\n",
      "Epoch 847/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2.7687e-05 - mse: 2.7687e-05 - val_loss: 0.5140 - val_mse: 0.5140\n",
      "Epoch 848/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.7015e-05 - mse: 2.7015e-05 - val_loss: 0.5139 - val_mse: 0.5139\n",
      "Epoch 849/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 30ms/step - loss: 2.6361e-05 - mse: 2.6361e-05 - val_loss: 0.5138 - val_mse: 0.5138\n",
      "Epoch 850/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.5721e-05 - mse: 2.5721e-05 - val_loss: 0.5137 - val_mse: 0.5137\n",
      "Epoch 851/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.5096e-05 - mse: 2.5096e-05 - val_loss: 0.5136 - val_mse: 0.5136\n",
      "Epoch 852/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.4486e-05 - mse: 2.4486e-05 - val_loss: 0.5135 - val_mse: 0.5135\n",
      "Epoch 853/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2.3890e-05 - mse: 2.3890e-05 - val_loss: 0.5134 - val_mse: 0.5134\n",
      "Epoch 854/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 2.3308e-05 - mse: 2.3308e-05 - val_loss: 0.5133 - val_mse: 0.5133\n",
      "Epoch 855/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2.2740e-05 - mse: 2.2740e-05 - val_loss: 0.5132 - val_mse: 0.5132\n",
      "Epoch 856/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.2186e-05 - mse: 2.2186e-05 - val_loss: 0.5132 - val_mse: 0.5132\n",
      "Epoch 857/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2.1644e-05 - mse: 2.1644e-05 - val_loss: 0.5131 - val_mse: 0.5131\n",
      "Epoch 858/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.1115e-05 - mse: 2.1115e-05 - val_loss: 0.5130 - val_mse: 0.5130\n",
      "Epoch 859/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.0598e-05 - mse: 2.0598e-05 - val_loss: 0.5129 - val_mse: 0.5129\n",
      "Epoch 860/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.0094e-05 - mse: 2.0094e-05 - val_loss: 0.5128 - val_mse: 0.5128\n",
      "Epoch 861/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.9601e-05 - mse: 1.9601e-05 - val_loss: 0.5128 - val_mse: 0.5128\n",
      "Epoch 862/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.9121e-05 - mse: 1.9121e-05 - val_loss: 0.5127 - val_mse: 0.5127\n",
      "Epoch 863/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.8651e-05 - mse: 1.8651e-05 - val_loss: 0.5126 - val_mse: 0.5126\n",
      "Epoch 864/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.8193e-05 - mse: 1.8193e-05 - val_loss: 0.5125 - val_mse: 0.5125\n",
      "Epoch 865/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.7745e-05 - mse: 1.7745e-05 - val_loss: 0.5124 - val_mse: 0.5124\n",
      "Epoch 866/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.7308e-05 - mse: 1.7308e-05 - val_loss: 0.5124 - val_mse: 0.5124\n",
      "Epoch 867/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.6881e-05 - mse: 1.6881e-05 - val_loss: 0.5123 - val_mse: 0.5123\n",
      "Epoch 868/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.6465e-05 - mse: 1.6465e-05 - val_loss: 0.5122 - val_mse: 0.5122\n",
      "Epoch 869/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.6058e-05 - mse: 1.6058e-05 - val_loss: 0.5121 - val_mse: 0.5121\n",
      "Epoch 870/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.5661e-05 - mse: 1.5661e-05 - val_loss: 0.5121 - val_mse: 0.5121\n",
      "Epoch 871/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.5274e-05 - mse: 1.5274e-05 - val_loss: 0.5120 - val_mse: 0.5120\n",
      "Epoch 872/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.4895e-05 - mse: 1.4895e-05 - val_loss: 0.5119 - val_mse: 0.5119\n",
      "Epoch 873/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.4526e-05 - mse: 1.4526e-05 - val_loss: 0.5119 - val_mse: 0.5119\n",
      "Epoch 874/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.4166e-05 - mse: 1.4166e-05 - val_loss: 0.5118 - val_mse: 0.5118\n",
      "Epoch 875/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.3813e-05 - mse: 1.3813e-05 - val_loss: 0.5117 - val_mse: 0.5117\n",
      "Epoch 876/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.3470e-05 - mse: 1.3470e-05 - val_loss: 0.5117 - val_mse: 0.5117\n",
      "Epoch 877/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1.3134e-05 - mse: 1.3134e-05 - val_loss: 0.5116 - val_mse: 0.5116\n",
      "Epoch 878/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1.2807e-05 - mse: 1.2807e-05 - val_loss: 0.5115 - val_mse: 0.5115\n",
      "Epoch 879/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.2487e-05 - mse: 1.2487e-05 - val_loss: 0.5115 - val_mse: 0.5115\n",
      "Epoch 880/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.2175e-05 - mse: 1.2175e-05 - val_loss: 0.5114 - val_mse: 0.5114\n",
      "Epoch 881/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.1871e-05 - mse: 1.1871e-05 - val_loss: 0.5113 - val_mse: 0.5113\n",
      "Epoch 882/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.1574e-05 - mse: 1.1574e-05 - val_loss: 0.5113 - val_mse: 0.5113\n",
      "Epoch 883/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.1283e-05 - mse: 1.1283e-05 - val_loss: 0.5112 - val_mse: 0.5112\n",
      "Epoch 884/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.1000e-05 - mse: 1.1000e-05 - val_loss: 0.5112 - val_mse: 0.5112\n",
      "Epoch 885/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.0724e-05 - mse: 1.0724e-05 - val_loss: 0.5111 - val_mse: 0.5111\n",
      "Epoch 886/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.0454e-05 - mse: 1.0454e-05 - val_loss: 0.5110 - val_mse: 0.5110\n",
      "Epoch 887/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.0191e-05 - mse: 1.0191e-05 - val_loss: 0.5110 - val_mse: 0.5110\n",
      "Epoch 888/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.9336e-06 - mse: 9.9336e-06 - val_loss: 0.5109 - val_mse: 0.5109\n",
      "Epoch 889/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.6827e-06 - mse: 9.6827e-06 - val_loss: 0.5109 - val_mse: 0.5109\n",
      "Epoch 890/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.4381e-06 - mse: 9.4381e-06 - val_loss: 0.5108 - val_mse: 0.5108\n",
      "Epoch 891/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.1993e-06 - mse: 9.1993e-06 - val_loss: 0.5108 - val_mse: 0.5108\n",
      "Epoch 892/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.9660e-06 - mse: 8.9660e-06 - val_loss: 0.5107 - val_mse: 0.5107\n",
      "Epoch 893/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.7385e-06 - mse: 8.7385e-06 - val_loss: 0.5106 - val_mse: 0.5106\n",
      "Epoch 894/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.5165e-06 - mse: 8.5165e-06 - val_loss: 0.5106 - val_mse: 0.5106\n",
      "Epoch 895/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.2997e-06 - mse: 8.2997e-06 - val_loss: 0.5105 - val_mse: 0.5105\n",
      "Epoch 896/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.0881e-06 - mse: 8.0881e-06 - val_loss: 0.5105 - val_mse: 0.5105\n",
      "Epoch 897/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.8821e-06 - mse: 7.8821e-06 - val_loss: 0.5104 - val_mse: 0.5104\n",
      "Epoch 898/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.6806e-06 - mse: 7.6806e-06 - val_loss: 0.5104 - val_mse: 0.5104\n",
      "Epoch 899/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.4841e-06 - mse: 7.4841e-06 - val_loss: 0.5103 - val_mse: 0.5103\n",
      "Epoch 900/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.2922e-06 - mse: 7.2922e-06 - val_loss: 0.5103 - val_mse: 0.5103\n",
      "Epoch 901/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.1055e-06 - mse: 7.1055e-06 - val_loss: 0.5102 - val_mse: 0.5102\n",
      "Epoch 902/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.9231e-06 - mse: 6.9231e-06 - val_loss: 0.5102 - val_mse: 0.5102\n",
      "Epoch 903/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.7449e-06 - mse: 6.7449e-06 - val_loss: 0.5101 - val_mse: 0.5101\n",
      "Epoch 904/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.5713e-06 - mse: 6.5713e-06 - val_loss: 0.5101 - val_mse: 0.5101\n",
      "Epoch 905/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.4016e-06 - mse: 6.4016e-06 - val_loss: 0.5100 - val_mse: 0.5100\n",
      "Epoch 906/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.2365e-06 - mse: 6.2365e-06 - val_loss: 0.5100 - val_mse: 0.5100\n",
      "Epoch 907/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.0751e-06 - mse: 6.0751e-06 - val_loss: 0.5100 - val_mse: 0.5100\n",
      "Epoch 908/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.9178e-06 - mse: 5.9178e-06 - val_loss: 0.5099 - val_mse: 0.5099\n",
      "Epoch 909/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.7641e-06 - mse: 5.7641e-06 - val_loss: 0.5099 - val_mse: 0.5099\n",
      "Epoch 910/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.6142e-06 - mse: 5.6142e-06 - val_loss: 0.5098 - val_mse: 0.5098\n",
      "Epoch 911/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.4682e-06 - mse: 5.4682e-06 - val_loss: 0.5098 - val_mse: 0.5098\n",
      "Epoch 912/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.3259e-06 - mse: 5.3259e-06 - val_loss: 0.5097 - val_mse: 0.5097\n",
      "Epoch 913/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.1868e-06 - mse: 5.1868e-06 - val_loss: 0.5097 - val_mse: 0.5097\n",
      "Epoch 914/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.0512e-06 - mse: 5.0512e-06 - val_loss: 0.5097 - val_mse: 0.5097\n",
      "Epoch 915/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.9191e-06 - mse: 4.9191e-06 - val_loss: 0.5096 - val_mse: 0.5096\n",
      "Epoch 916/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.7899e-06 - mse: 4.7899e-06 - val_loss: 0.5096 - val_mse: 0.5096\n",
      "Epoch 917/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.6642e-06 - mse: 4.6642e-06 - val_loss: 0.5095 - val_mse: 0.5095\n",
      "Epoch 918/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.5416e-06 - mse: 4.5416e-06 - val_loss: 0.5095 - val_mse: 0.5095\n",
      "Epoch 919/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.4220e-06 - mse: 4.4220e-06 - val_loss: 0.5095 - val_mse: 0.5095\n",
      "Epoch 920/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.3053e-06 - mse: 4.3053e-06 - val_loss: 0.5094 - val_mse: 0.5094\n",
      "Epoch 921/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.1913e-06 - mse: 4.1913e-06 - val_loss: 0.5094 - val_mse: 0.5094\n",
      "Epoch 922/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.0803e-06 - mse: 4.0803e-06 - val_loss: 0.5093 - val_mse: 0.5093\n",
      "Epoch 923/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.9722e-06 - mse: 3.9722e-06 - val_loss: 0.5093 - val_mse: 0.5093\n",
      "Epoch 924/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.8667e-06 - mse: 3.8667e-06 - val_loss: 0.5093 - val_mse: 0.5093\n",
      "Epoch 925/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.7639e-06 - mse: 3.7639e-06 - val_loss: 0.5092 - val_mse: 0.5092\n",
      "Epoch 926/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.6635e-06 - mse: 3.6635e-06 - val_loss: 0.5092 - val_mse: 0.5092\n",
      "Epoch 927/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.5657e-06 - mse: 3.5657e-06 - val_loss: 0.5092 - val_mse: 0.5092\n",
      "Epoch 928/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.4704e-06 - mse: 3.4704e-06 - val_loss: 0.5091 - val_mse: 0.5091\n",
      "Epoch 929/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.3774e-06 - mse: 3.3774e-06 - val_loss: 0.5091 - val_mse: 0.5091\n",
      "Epoch 930/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.2868e-06 - mse: 3.2868e-06 - val_loss: 0.5091 - val_mse: 0.5091\n",
      "Epoch 931/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.1984e-06 - mse: 3.1984e-06 - val_loss: 0.5090 - val_mse: 0.5090\n",
      "Epoch 932/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.1122e-06 - mse: 3.1122e-06 - val_loss: 0.5090 - val_mse: 0.5090\n",
      "Epoch 933/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.0284e-06 - mse: 3.0284e-06 - val_loss: 0.5090 - val_mse: 0.5090\n",
      "Epoch 934/1000\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 2.9464e-06 - mse: 2.9464e-06 - val_loss: 0.5089 - val_mse: 0.5089\n",
      "Epoch 935/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.8668e-06 - mse: 2.8668e-06 - val_loss: 0.5089 - val_mse: 0.5089\n",
      "Epoch 936/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.7889e-06 - mse: 2.7889e-06 - val_loss: 0.5089 - val_mse: 0.5089\n",
      "Epoch 937/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.7132e-06 - mse: 2.7132e-06 - val_loss: 0.5088 - val_mse: 0.5088\n",
      "Epoch 938/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2.6393e-06 - mse: 2.6393e-06 - val_loss: 0.5088 - val_mse: 0.5088\n",
      "Epoch 939/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.5673e-06 - mse: 2.5673e-06 - val_loss: 0.5088 - val_mse: 0.5088\n",
      "Epoch 940/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2.4972e-06 - mse: 2.4972e-06 - val_loss: 0.5088 - val_mse: 0.5088\n",
      "Epoch 941/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.4288e-06 - mse: 2.4288e-06 - val_loss: 0.5087 - val_mse: 0.5087\n",
      "Epoch 942/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.3621e-06 - mse: 2.3621e-06 - val_loss: 0.5087 - val_mse: 0.5087\n",
      "Epoch 943/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.2972e-06 - mse: 2.2972e-06 - val_loss: 0.5087 - val_mse: 0.5087\n",
      "Epoch 944/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2.2340e-06 - mse: 2.2340e-06 - val_loss: 0.5086 - val_mse: 0.5086\n",
      "Epoch 945/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 2.1724e-06 - mse: 2.1724e-06 - val_loss: 0.5086 - val_mse: 0.5086\n",
      "Epoch 946/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2.1124e-06 - mse: 2.1124e-06 - val_loss: 0.5086 - val_mse: 0.5086\n",
      "Epoch 947/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.0540e-06 - mse: 2.0540e-06 - val_loss: 0.5086 - val_mse: 0.5086\n",
      "Epoch 948/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.9969e-06 - mse: 1.9969e-06 - val_loss: 0.5085 - val_mse: 0.5085\n",
      "Epoch 949/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.9414e-06 - mse: 1.9414e-06 - val_loss: 0.5085 - val_mse: 0.5085\n",
      "Epoch 950/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.8874e-06 - mse: 1.8874e-06 - val_loss: 0.5085 - val_mse: 0.5085\n",
      "Epoch 951/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.8347e-06 - mse: 1.8347e-06 - val_loss: 0.5085 - val_mse: 0.5085\n",
      "Epoch 952/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.7835e-06 - mse: 1.7835e-06 - val_loss: 0.5084 - val_mse: 0.5084\n",
      "Epoch 953/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.7334e-06 - mse: 1.7334e-06 - val_loss: 0.5084 - val_mse: 0.5084\n",
      "Epoch 954/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.6848e-06 - mse: 1.6848e-06 - val_loss: 0.5084 - val_mse: 0.5084\n",
      "Epoch 955/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.6373e-06 - mse: 1.6373e-06 - val_loss: 0.5084 - val_mse: 0.5084\n",
      "Epoch 956/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.5912e-06 - mse: 1.5912e-06 - val_loss: 0.5083 - val_mse: 0.5083\n",
      "Epoch 957/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.5463e-06 - mse: 1.5463e-06 - val_loss: 0.5083 - val_mse: 0.5083\n",
      "Epoch 958/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.5026e-06 - mse: 1.5026e-06 - val_loss: 0.5083 - val_mse: 0.5083\n",
      "Epoch 959/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.4600e-06 - mse: 1.4600e-06 - val_loss: 0.5083 - val_mse: 0.5083\n",
      "Epoch 960/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.4185e-06 - mse: 1.4185e-06 - val_loss: 0.5083 - val_mse: 0.5083\n",
      "Epoch 961/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.3781e-06 - mse: 1.3781e-06 - val_loss: 0.5082 - val_mse: 0.5082\n",
      "Epoch 962/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.3389e-06 - mse: 1.3389e-06 - val_loss: 0.5082 - val_mse: 0.5082\n",
      "Epoch 963/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.3004e-06 - mse: 1.3004e-06 - val_loss: 0.5082 - val_mse: 0.5082\n",
      "Epoch 964/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.2632e-06 - mse: 1.2632e-06 - val_loss: 0.5082 - val_mse: 0.5082\n",
      "Epoch 965/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 19ms/step - loss: 1.2269e-06 - mse: 1.2269e-06 - val_loss: 0.5082 - val_mse: 0.5082\n",
      "Epoch 966/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.1916e-06 - mse: 1.1916e-06 - val_loss: 0.5081 - val_mse: 0.5081\n",
      "Epoch 967/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.1573e-06 - mse: 1.1573e-06 - val_loss: 0.5081 - val_mse: 0.5081\n",
      "Epoch 968/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.1239e-06 - mse: 1.1239e-06 - val_loss: 0.5081 - val_mse: 0.5081\n",
      "Epoch 969/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.0913e-06 - mse: 1.0913e-06 - val_loss: 0.5081 - val_mse: 0.5081\n",
      "Epoch 970/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.0597e-06 - mse: 1.0597e-06 - val_loss: 0.5081 - val_mse: 0.5081\n",
      "Epoch 971/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.0288e-06 - mse: 1.0288e-06 - val_loss: 0.5080 - val_mse: 0.5080\n",
      "Epoch 972/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.9883e-07 - mse: 9.9883e-07 - val_loss: 0.5080 - val_mse: 0.5080\n",
      "Epoch 973/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.6981e-07 - mse: 9.6981e-07 - val_loss: 0.5080 - val_mse: 0.5080\n",
      "Epoch 974/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.4121e-07 - mse: 9.4121e-07 - val_loss: 0.5080 - val_mse: 0.5080\n",
      "Epoch 975/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.1365e-07 - mse: 9.1365e-07 - val_loss: 0.5080 - val_mse: 0.5080\n",
      "Epoch 976/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.8686e-07 - mse: 8.8686e-07 - val_loss: 0.5079 - val_mse: 0.5079\n",
      "Epoch 977/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.6077e-07 - mse: 8.6077e-07 - val_loss: 0.5079 - val_mse: 0.5079\n",
      "Epoch 978/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.3533e-07 - mse: 8.3533e-07 - val_loss: 0.5079 - val_mse: 0.5079\n",
      "Epoch 979/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.1061e-07 - mse: 8.1061e-07 - val_loss: 0.5079 - val_mse: 0.5079\n",
      "Epoch 980/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.8655e-07 - mse: 7.8655e-07 - val_loss: 0.5079 - val_mse: 0.5079\n",
      "Epoch 981/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.6319e-07 - mse: 7.6319e-07 - val_loss: 0.5079 - val_mse: 0.5079\n",
      "Epoch 982/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.4048e-07 - mse: 7.4048e-07 - val_loss: 0.5078 - val_mse: 0.5078\n",
      "Epoch 983/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.1830e-07 - mse: 7.1830e-07 - val_loss: 0.5078 - val_mse: 0.5078\n",
      "Epoch 984/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.9681e-07 - mse: 6.9681e-07 - val_loss: 0.5078 - val_mse: 0.5078\n",
      "Epoch 985/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.7591e-07 - mse: 6.7591e-07 - val_loss: 0.5078 - val_mse: 0.5078\n",
      "Epoch 986/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.5563e-07 - mse: 6.5563e-07 - val_loss: 0.5078 - val_mse: 0.5078\n",
      "Epoch 987/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.3592e-07 - mse: 6.3592e-07 - val_loss: 0.5078 - val_mse: 0.5078\n",
      "Epoch 988/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.1668e-07 - mse: 6.1668e-07 - val_loss: 0.5078 - val_mse: 0.5078\n",
      "Epoch 989/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.9807e-07 - mse: 5.9807e-07 - val_loss: 0.5077 - val_mse: 0.5077\n",
      "Epoch 990/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.7988e-07 - mse: 5.7988e-07 - val_loss: 0.5077 - val_mse: 0.5077\n",
      "Epoch 991/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.6226e-07 - mse: 5.6226e-07 - val_loss: 0.5077 - val_mse: 0.5077\n",
      "Epoch 992/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.4517e-07 - mse: 5.4517e-07 - val_loss: 0.5077 - val_mse: 0.5077\n",
      "Epoch 993/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.2848e-07 - mse: 5.2848e-07 - val_loss: 0.5077 - val_mse: 0.5077\n",
      "Epoch 994/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.1230e-07 - mse: 5.1230e-07 - val_loss: 0.5077 - val_mse: 0.5077\n",
      "Epoch 995/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.9657e-07 - mse: 4.9657e-07 - val_loss: 0.5077 - val_mse: 0.5077\n",
      "Epoch 996/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.8130e-07 - mse: 4.8130e-07 - val_loss: 0.5076 - val_mse: 0.5076\n",
      "Epoch 997/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.6646e-07 - mse: 4.6646e-07 - val_loss: 0.5076 - val_mse: 0.5076\n",
      "Epoch 998/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.5203e-07 - mse: 4.5203e-07 - val_loss: 0.5076 - val_mse: 0.5076\n",
      "Epoch 999/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.3804e-07 - mse: 4.3804e-07 - val_loss: 0.5076 - val_mse: 0.5076\n",
      "Epoch 1000/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.2447e-07 - mse: 4.2447e-07 - val_loss: 0.5076 - val_mse: 0.5076\n",
      "mkdir: png: File exists\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9616282440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9616282440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9616282440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " def main():\n",
    " \n",
    "    if not eval(confParser['neural net classification']['classification']):\n",
    "        return\n",
    "    \n",
    "    nn = NeuralNetwork(\n",
    "                        hidden_layer_sizes = eval(confParser['neural net classification']['hidden_layer_sizes']),\n",
    "                        learning_rate_init = eval(confParser['neural net classification']['learning_rate_init']),\n",
    "                        n_iter_no_change   = eval(confParser['neural net classification']['n_iter_no_change']),\n",
    "                        tol                = eval(confParser['neural net classification']['tol']),\n",
    "                        max_iter           = eval(confParser['neural net classification']['max_iter']),\n",
    "                        alpha              = eval(confParser['neural net classification']['alpha']),\n",
    "                        hidden_layer_size  = eval(confParser['neural net classification']['hidden_layer_size']),\n",
    "                        fully_connected    = eval(confParser['neural net classification']['fully_connected']),\n",
    "                        implementation     = eval(confParser['neural net classification']['implementation']),\n",
    "                        cnn                = eval(confParser['neural net classification']['cnn']),\n",
    "                        n_channels         = eval(confParser['neural net classification']['n_channels']),\n",
    "                        kernel_size        = eval(confParser['neural net classification']['kernel_size']),\n",
    "                        activation         = eval(confParser['neural net classification']['activation']),\n",
    "                        number_hidden_layers = eval(confParser['neural net classification']['number_hidden_layers']),\n",
    "                        verbose            = True \n",
    "                    )\n",
    "    \n",
    "    nn.Parse( path  = confParser['neural net']['input_path'],\n",
    "              nruns = eval(confParser['neural net']['nruns']))\n",
    "\n",
    "    nn.Combine() #--- concat. descriptors\n",
    "    \n",
    "#     tmp = nn.df_combined.set_index('id')\n",
    "#    display(nn.perAtomData.iloc[:13])\n",
    "#    nn.PrintDensityMap(6,'density6.xyz')\n",
    "#    nn.PCA()\n",
    "#    nn.Spectra()\n",
    "\n",
    "\n",
    "    #--- classifier\n",
    "    nn.TrainClassifier(np.c_[nn.perAtomData.defect_label].astype(int))\n",
    "#        nn.Test(np.c_[nn.perAtomData.defect_label].astype(int))\n",
    "    \n",
    "    \n",
    "    return nn\n",
    "\n",
    "#!rm -r png;mkdir png\n",
    "model_clf = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0db348",
   "metadata": {},
   "source": [
    "## main(): regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e01971ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating descriptors ...\n",
      "dim(y)= (7, 3)\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 11, 11, 11, 1)]   0         \n",
      "                                                                 \n",
      " conv3d_16 (Conv3D)          (None, 11, 11, 11, 16)    448       \n",
      "                                                                 \n",
      " average_pooling3d_12 (Avera  (None, 5, 5, 5, 16)      0         \n",
      " gePooling3D)                                                    \n",
      "                                                                 \n",
      " conv3d_17 (Conv3D)          (None, 5, 5, 5, 32)       13856     \n",
      "                                                                 \n",
      " average_pooling3d_13 (Avera  (None, 2, 2, 2, 32)      0         \n",
      " gePooling3D)                                                    \n",
      "                                                                 \n",
      " conv3d_18 (Conv3D)          (None, 2, 2, 2, 64)       55360     \n",
      "                                                                 \n",
      " average_pooling3d_14 (Avera  (None, 1, 1, 1, 64)      0         \n",
      " gePooling3D)                                                    \n",
      "                                                                 \n",
      " conv3d_19 (Conv3D)          (None, 1, 1, 1, 128)      221312    \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 291,363\n",
      "Trainable params: 291,363\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "cnn model summary: None\n",
      "mkdir: best_model: File exists\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f9605f6a5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f9605f6a5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f9605f6a5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/5 [=====>........................] - ETA: 4s - loss: 1.9818 - mse: 1.9818WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f96070e8d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f96070e8d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f96070e8d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "5/5 [==============================] - 1s 57ms/step - loss: 1.9819 - mse: 1.9819 - val_loss: 1.9873 - val_mse: 1.9873\n",
      "Epoch 2/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9807 - mse: 1.9807 - val_loss: 1.9872 - val_mse: 1.9872\n",
      "Epoch 3/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9796 - mse: 1.9796 - val_loss: 1.9872 - val_mse: 1.9872\n",
      "Epoch 4/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.9785 - mse: 1.9785 - val_loss: 1.9871 - val_mse: 1.9871\n",
      "Epoch 5/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9774 - mse: 1.9774 - val_loss: 1.9871 - val_mse: 1.9871\n",
      "Epoch 6/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.9763 - mse: 1.9763 - val_loss: 1.9870 - val_mse: 1.9870\n",
      "Epoch 7/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.9752 - mse: 1.9752 - val_loss: 1.9870 - val_mse: 1.9870\n",
      "Epoch 8/1000\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 1.9741 - mse: 1.9741 - val_loss: 1.9869 - val_mse: 1.9869\n",
      "Epoch 9/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 1.9730 - mse: 1.9730 - val_loss: 1.9869 - val_mse: 1.9869\n",
      "Epoch 10/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.9718 - mse: 1.9718 - val_loss: 1.9868 - val_mse: 1.9868\n",
      "Epoch 11/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.9707 - mse: 1.9707 - val_loss: 1.9868 - val_mse: 1.9868\n",
      "Epoch 12/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.9695 - mse: 1.9695 - val_loss: 1.9867 - val_mse: 1.9867\n",
      "Epoch 13/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9683 - mse: 1.9683 - val_loss: 1.9867 - val_mse: 1.9867\n",
      "Epoch 14/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9671 - mse: 1.9671 - val_loss: 1.9866 - val_mse: 1.9866\n",
      "Epoch 15/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.9659 - mse: 1.9659 - val_loss: 1.9865 - val_mse: 1.9865\n",
      "Epoch 16/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.9646 - mse: 1.9646 - val_loss: 1.9864 - val_mse: 1.9864\n",
      "Epoch 17/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.9634 - mse: 1.9634 - val_loss: 1.9863 - val_mse: 1.9863\n",
      "Epoch 18/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9621 - mse: 1.9621 - val_loss: 1.9863 - val_mse: 1.9863\n",
      "Epoch 19/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9607 - mse: 1.9607 - val_loss: 1.9861 - val_mse: 1.9861\n",
      "Epoch 20/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.9594 - mse: 1.9594 - val_loss: 1.9860 - val_mse: 1.9860\n",
      "Epoch 21/1000\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 1.9580 - mse: 1.9580 - val_loss: 1.9859 - val_mse: 1.9859\n",
      "Epoch 22/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.9565 - mse: 1.9565 - val_loss: 1.9858 - val_mse: 1.9858\n",
      "Epoch 23/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.9551 - mse: 1.9551 - val_loss: 1.9856 - val_mse: 1.9856\n",
      "Epoch 24/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.9536 - mse: 1.9536 - val_loss: 1.9855 - val_mse: 1.9855\n",
      "Epoch 25/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9520 - mse: 1.9520 - val_loss: 1.9853 - val_mse: 1.9853\n",
      "Epoch 26/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9504 - mse: 1.9504 - val_loss: 1.9851 - val_mse: 1.9851\n",
      "Epoch 27/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9488 - mse: 1.9488 - val_loss: 1.9849 - val_mse: 1.9849\n",
      "Epoch 28/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9471 - mse: 1.9471 - val_loss: 1.9847 - val_mse: 1.9847\n",
      "Epoch 29/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.9454 - mse: 1.9454 - val_loss: 1.9845 - val_mse: 1.9845\n",
      "Epoch 30/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9436 - mse: 1.9436 - val_loss: 1.9843 - val_mse: 1.9843\n",
      "Epoch 31/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9418 - mse: 1.9418 - val_loss: 1.9840 - val_mse: 1.9840\n",
      "Epoch 32/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.9399 - mse: 1.9399 - val_loss: 1.9838 - val_mse: 1.9838\n",
      "Epoch 33/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1.9380 - mse: 1.9380 - val_loss: 1.9835 - val_mse: 1.9835\n",
      "Epoch 34/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1.9360 - mse: 1.9360 - val_loss: 1.9832 - val_mse: 1.9832\n",
      "Epoch 35/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1.9339 - mse: 1.9339 - val_loss: 1.9829 - val_mse: 1.9829\n",
      "Epoch 36/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9318 - mse: 1.9318 - val_loss: 1.9825 - val_mse: 1.9825\n",
      "Epoch 37/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9296 - mse: 1.9296 - val_loss: 1.9822 - val_mse: 1.9822\n",
      "Epoch 38/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.9274 - mse: 1.9274 - val_loss: 1.9818 - val_mse: 1.9818\n",
      "Epoch 39/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9250 - mse: 1.9250 - val_loss: 1.9814 - val_mse: 1.9814\n",
      "Epoch 40/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.9226 - mse: 1.9226 - val_loss: 1.9809 - val_mse: 1.9809\n",
      "Epoch 41/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.9201 - mse: 1.9201 - val_loss: 1.9805 - val_mse: 1.9805\n",
      "Epoch 42/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.9175 - mse: 1.9175 - val_loss: 1.9800 - val_mse: 1.9800\n",
      "Epoch 43/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9149 - mse: 1.9149 - val_loss: 1.9795 - val_mse: 1.9795\n",
      "Epoch 44/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.9121 - mse: 1.9121 - val_loss: 1.9789 - val_mse: 1.9789\n",
      "Epoch 45/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.9093 - mse: 1.9093 - val_loss: 1.9784 - val_mse: 1.9784\n",
      "Epoch 46/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.9063 - mse: 1.9063 - val_loss: 1.9778 - val_mse: 1.9778\n",
      "Epoch 47/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.9033 - mse: 1.9033 - val_loss: 1.9771 - val_mse: 1.9771\n",
      "Epoch 48/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.9001 - mse: 1.9001 - val_loss: 1.9765 - val_mse: 1.9765\n",
      "Epoch 49/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.8968 - mse: 1.8968 - val_loss: 1.9758 - val_mse: 1.9758\n",
      "Epoch 50/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.8934 - mse: 1.8934 - val_loss: 1.9750 - val_mse: 1.9750\n",
      "Epoch 51/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8899 - mse: 1.8899 - val_loss: 1.9743 - val_mse: 1.9743\n",
      "Epoch 52/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.8863 - mse: 1.8863 - val_loss: 1.9734 - val_mse: 1.9734\n",
      "Epoch 53/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.8826 - mse: 1.8826 - val_loss: 1.9726 - val_mse: 1.9726\n",
      "Epoch 54/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.8787 - mse: 1.8787 - val_loss: 1.9717 - val_mse: 1.9717\n",
      "Epoch 55/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.8747 - mse: 1.8747 - val_loss: 1.9708 - val_mse: 1.9708\n",
      "Epoch 56/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.8705 - mse: 1.8705 - val_loss: 1.9698 - val_mse: 1.9698\n",
      "Epoch 57/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.8662 - mse: 1.8662 - val_loss: 1.9688 - val_mse: 1.9688\n",
      "Epoch 58/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.8617 - mse: 1.8617 - val_loss: 1.9677 - val_mse: 1.9677\n",
      "Epoch 59/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.8571 - mse: 1.8571 - val_loss: 1.9666 - val_mse: 1.9666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8523 - mse: 1.8523 - val_loss: 1.9654 - val_mse: 1.9654\n",
      "Epoch 61/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.8474 - mse: 1.8474 - val_loss: 1.9642 - val_mse: 1.9642\n",
      "Epoch 62/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.8423 - mse: 1.8423 - val_loss: 1.9629 - val_mse: 1.9629\n",
      "Epoch 63/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8370 - mse: 1.8370 - val_loss: 1.9616 - val_mse: 1.9616\n",
      "Epoch 64/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8315 - mse: 1.8315 - val_loss: 1.9602 - val_mse: 1.9602\n",
      "Epoch 65/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8259 - mse: 1.8259 - val_loss: 1.9587 - val_mse: 1.9587\n",
      "Epoch 66/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.8200 - mse: 1.8200 - val_loss: 1.9572 - val_mse: 1.9572\n",
      "Epoch 67/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8140 - mse: 1.8140 - val_loss: 1.9557 - val_mse: 1.9557\n",
      "Epoch 68/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8077 - mse: 1.8077 - val_loss: 1.9540 - val_mse: 1.9540\n",
      "Epoch 69/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.8013 - mse: 1.8013 - val_loss: 1.9523 - val_mse: 1.9523\n",
      "Epoch 70/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7946 - mse: 1.7946 - val_loss: 1.9506 - val_mse: 1.9506\n",
      "Epoch 71/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7877 - mse: 1.7877 - val_loss: 1.9488 - val_mse: 1.9488\n",
      "Epoch 72/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7806 - mse: 1.7806 - val_loss: 1.9469 - val_mse: 1.9469\n",
      "Epoch 73/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7733 - mse: 1.7733 - val_loss: 1.9449 - val_mse: 1.9449\n",
      "Epoch 74/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7657 - mse: 1.7657 - val_loss: 1.9429 - val_mse: 1.9429\n",
      "Epoch 75/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7579 - mse: 1.7579 - val_loss: 1.9407 - val_mse: 1.9407\n",
      "Epoch 76/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7499 - mse: 1.7499 - val_loss: 1.9386 - val_mse: 1.9386\n",
      "Epoch 77/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.7416 - mse: 1.7416 - val_loss: 1.9363 - val_mse: 1.9363\n",
      "Epoch 78/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7331 - mse: 1.7331 - val_loss: 1.9340 - val_mse: 1.9340\n",
      "Epoch 79/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7243 - mse: 1.7243 - val_loss: 1.9315 - val_mse: 1.9315\n",
      "Epoch 80/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7153 - mse: 1.7153 - val_loss: 1.9290 - val_mse: 1.9290\n",
      "Epoch 81/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.7060 - mse: 1.7060 - val_loss: 1.9265 - val_mse: 1.9265\n",
      "Epoch 82/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.6964 - mse: 1.6964 - val_loss: 1.9238 - val_mse: 1.9238\n",
      "Epoch 83/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.6866 - mse: 1.6866 - val_loss: 1.9211 - val_mse: 1.9211\n",
      "Epoch 84/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6766 - mse: 1.6766 - val_loss: 1.9183 - val_mse: 1.9183\n",
      "Epoch 85/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.6662 - mse: 1.6662 - val_loss: 1.9154 - val_mse: 1.9154\n",
      "Epoch 86/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.6556 - mse: 1.6556 - val_loss: 1.9124 - val_mse: 1.9124\n",
      "Epoch 87/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6448 - mse: 1.6448 - val_loss: 1.9093 - val_mse: 1.9093\n",
      "Epoch 88/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6337 - mse: 1.6337 - val_loss: 1.9061 - val_mse: 1.9061\n",
      "Epoch 89/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6223 - mse: 1.6223 - val_loss: 1.9029 - val_mse: 1.9029\n",
      "Epoch 90/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.6106 - mse: 1.6106 - val_loss: 1.8996 - val_mse: 1.8996\n",
      "Epoch 91/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5987 - mse: 1.5987 - val_loss: 1.8962 - val_mse: 1.8962\n",
      "Epoch 92/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5865 - mse: 1.5865 - val_loss: 1.8927 - val_mse: 1.8927\n",
      "Epoch 93/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.5741 - mse: 1.5741 - val_loss: 1.8891 - val_mse: 1.8891\n",
      "Epoch 94/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5615 - mse: 1.5615 - val_loss: 1.8855 - val_mse: 1.8855\n",
      "Epoch 95/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5486 - mse: 1.5486 - val_loss: 1.8818 - val_mse: 1.8818\n",
      "Epoch 96/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5354 - mse: 1.5354 - val_loss: 1.8780 - val_mse: 1.8780\n",
      "Epoch 97/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5221 - mse: 1.5221 - val_loss: 1.8741 - val_mse: 1.8741\n",
      "Epoch 98/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.5085 - mse: 1.5085 - val_loss: 1.8701 - val_mse: 1.8701\n",
      "Epoch 99/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.4946 - mse: 1.4946 - val_loss: 1.8661 - val_mse: 1.8661\n",
      "Epoch 100/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.4806 - mse: 1.4806 - val_loss: 1.8620 - val_mse: 1.8620\n",
      "Epoch 101/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.4664 - mse: 1.4664 - val_loss: 1.8578 - val_mse: 1.8578\n",
      "Epoch 102/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.4520 - mse: 1.4520 - val_loss: 1.8536 - val_mse: 1.8536\n",
      "Epoch 103/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.4374 - mse: 1.4374 - val_loss: 1.8493 - val_mse: 1.8493\n",
      "Epoch 104/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.4227 - mse: 1.4227 - val_loss: 1.8450 - val_mse: 1.8450\n",
      "Epoch 105/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.4078 - mse: 1.4078 - val_loss: 1.8405 - val_mse: 1.8405\n",
      "Epoch 106/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.3927 - mse: 1.3927 - val_loss: 1.8361 - val_mse: 1.8361\n",
      "Epoch 107/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3776 - mse: 1.3776 - val_loss: 1.8316 - val_mse: 1.8316\n",
      "Epoch 108/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.3623 - mse: 1.3623 - val_loss: 1.8270 - val_mse: 1.8270\n",
      "Epoch 109/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3470 - mse: 1.3470 - val_loss: 1.8224 - val_mse: 1.8224\n",
      "Epoch 110/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3315 - mse: 1.3315 - val_loss: 1.8178 - val_mse: 1.8178\n",
      "Epoch 111/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3160 - mse: 1.3160 - val_loss: 1.8131 - val_mse: 1.8131\n",
      "Epoch 112/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.3005 - mse: 1.3005 - val_loss: 1.8085 - val_mse: 1.8085\n",
      "Epoch 113/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2849 - mse: 1.2849 - val_loss: 1.8038 - val_mse: 1.8038\n",
      "Epoch 114/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2693 - mse: 1.2693 - val_loss: 1.7990 - val_mse: 1.7990\n",
      "Epoch 115/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2538 - mse: 1.2538 - val_loss: 1.7943 - val_mse: 1.7943\n",
      "Epoch 116/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2383 - mse: 1.2383 - val_loss: 1.7896 - val_mse: 1.7896\n",
      "Epoch 117/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2228 - mse: 1.2228 - val_loss: 1.7848 - val_mse: 1.7848\n",
      "Epoch 118/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.2074 - mse: 1.2074 - val_loss: 1.7801 - val_mse: 1.7801\n",
      "Epoch 119/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.1920 - mse: 1.1920 - val_loss: 1.7754 - val_mse: 1.7754\n",
      "Epoch 120/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1768 - mse: 1.1768 - val_loss: 1.7707 - val_mse: 1.7707\n",
      "Epoch 121/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1617 - mse: 1.1617 - val_loss: 1.7660 - val_mse: 1.7660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1467 - mse: 1.1467 - val_loss: 1.7613 - val_mse: 1.7613\n",
      "Epoch 123/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1319 - mse: 1.1319 - val_loss: 1.7567 - val_mse: 1.7567\n",
      "Epoch 124/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1173 - mse: 1.1173 - val_loss: 1.7521 - val_mse: 1.7521\n",
      "Epoch 125/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.1029 - mse: 1.1029 - val_loss: 1.7475 - val_mse: 1.7475\n",
      "Epoch 126/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.0887 - mse: 1.0887 - val_loss: 1.7430 - val_mse: 1.7430\n",
      "Epoch 127/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0747 - mse: 1.0747 - val_loss: 1.7386 - val_mse: 1.7386\n",
      "Epoch 128/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0609 - mse: 1.0609 - val_loss: 1.7342 - val_mse: 1.7342\n",
      "Epoch 129/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0474 - mse: 1.0474 - val_loss: 1.7298 - val_mse: 1.7298\n",
      "Epoch 130/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0341 - mse: 1.0341 - val_loss: 1.7255 - val_mse: 1.7255\n",
      "Epoch 131/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0211 - mse: 1.0211 - val_loss: 1.7212 - val_mse: 1.7212\n",
      "Epoch 132/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.0084 - mse: 1.0084 - val_loss: 1.7171 - val_mse: 1.7171\n",
      "Epoch 133/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.9960 - mse: 0.9960 - val_loss: 1.7129 - val_mse: 1.7129\n",
      "Epoch 134/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9838 - mse: 0.9838 - val_loss: 1.7089 - val_mse: 1.7089\n",
      "Epoch 135/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9720 - mse: 0.9720 - val_loss: 1.7049 - val_mse: 1.7049\n",
      "Epoch 136/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9604 - mse: 0.9604 - val_loss: 1.7010 - val_mse: 1.7010\n",
      "Epoch 137/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9492 - mse: 0.9492 - val_loss: 1.6971 - val_mse: 1.6971\n",
      "Epoch 138/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9382 - mse: 0.9382 - val_loss: 1.6933 - val_mse: 1.6933\n",
      "Epoch 139/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.9275 - mse: 0.9275 - val_loss: 1.6896 - val_mse: 1.6896\n",
      "Epoch 140/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.9172 - mse: 0.9172 - val_loss: 1.6859 - val_mse: 1.6859\n",
      "Epoch 141/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9071 - mse: 0.9071 - val_loss: 1.6823 - val_mse: 1.6823\n",
      "Epoch 142/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8973 - mse: 0.8973 - val_loss: 1.6787 - val_mse: 1.6787\n",
      "Epoch 143/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8878 - mse: 0.8878 - val_loss: 1.6752 - val_mse: 1.6752\n",
      "Epoch 144/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8785 - mse: 0.8785 - val_loss: 1.6718 - val_mse: 1.6718\n",
      "Epoch 145/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8695 - mse: 0.8695 - val_loss: 1.6684 - val_mse: 1.6684\n",
      "Epoch 146/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.8608 - mse: 0.8608 - val_loss: 1.6650 - val_mse: 1.6650\n",
      "Epoch 147/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.8523 - mse: 0.8523 - val_loss: 1.6617 - val_mse: 1.6617\n",
      "Epoch 148/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8440 - mse: 0.8440 - val_loss: 1.6584 - val_mse: 1.6584\n",
      "Epoch 149/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8360 - mse: 0.8360 - val_loss: 1.6551 - val_mse: 1.6551\n",
      "Epoch 150/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8281 - mse: 0.8281 - val_loss: 1.6519 - val_mse: 1.6519\n",
      "Epoch 151/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8205 - mse: 0.8205 - val_loss: 1.6487 - val_mse: 1.6487\n",
      "Epoch 152/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.8130 - mse: 0.8130 - val_loss: 1.6455 - val_mse: 1.6455\n",
      "Epoch 153/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.8057 - mse: 0.8057 - val_loss: 1.6424 - val_mse: 1.6424\n",
      "Epoch 154/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.7986 - mse: 0.7986 - val_loss: 1.6392 - val_mse: 1.6392\n",
      "Epoch 155/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7916 - mse: 0.7916 - val_loss: 1.6361 - val_mse: 1.6361\n",
      "Epoch 156/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7848 - mse: 0.7848 - val_loss: 1.6330 - val_mse: 1.6330\n",
      "Epoch 157/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7781 - mse: 0.7781 - val_loss: 1.6298 - val_mse: 1.6298\n",
      "Epoch 158/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7715 - mse: 0.7715 - val_loss: 1.6267 - val_mse: 1.6267\n",
      "Epoch 159/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7650 - mse: 0.7650 - val_loss: 1.6236 - val_mse: 1.6236\n",
      "Epoch 160/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.7587 - mse: 0.7587 - val_loss: 1.6205 - val_mse: 1.6205\n",
      "Epoch 161/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7524 - mse: 0.7524 - val_loss: 1.6174 - val_mse: 1.6174\n",
      "Epoch 162/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7462 - mse: 0.7462 - val_loss: 1.6142 - val_mse: 1.6142\n",
      "Epoch 163/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7401 - mse: 0.7401 - val_loss: 1.6111 - val_mse: 1.6111\n",
      "Epoch 164/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7341 - mse: 0.7341 - val_loss: 1.6079 - val_mse: 1.6079\n",
      "Epoch 165/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7281 - mse: 0.7281 - val_loss: 1.6047 - val_mse: 1.6047\n",
      "Epoch 166/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7223 - mse: 0.7223 - val_loss: 1.6015 - val_mse: 1.6015\n",
      "Epoch 167/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.7164 - mse: 0.7164 - val_loss: 1.5983 - val_mse: 1.5983\n",
      "Epoch 168/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.7106 - mse: 0.7106 - val_loss: 1.5951 - val_mse: 1.5951\n",
      "Epoch 169/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7049 - mse: 0.7049 - val_loss: 1.5919 - val_mse: 1.5919\n",
      "Epoch 170/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6992 - mse: 0.6992 - val_loss: 1.5886 - val_mse: 1.5886\n",
      "Epoch 171/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6936 - mse: 0.6936 - val_loss: 1.5853 - val_mse: 1.5853\n",
      "Epoch 172/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6880 - mse: 0.6880 - val_loss: 1.5820 - val_mse: 1.5820\n",
      "Epoch 173/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6824 - mse: 0.6824 - val_loss: 1.5787 - val_mse: 1.5787\n",
      "Epoch 174/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6768 - mse: 0.6768 - val_loss: 1.5753 - val_mse: 1.5753\n",
      "Epoch 175/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6713 - mse: 0.6713 - val_loss: 1.5719 - val_mse: 1.5719\n",
      "Epoch 176/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6658 - mse: 0.6658 - val_loss: 1.5685 - val_mse: 1.5685\n",
      "Epoch 177/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6603 - mse: 0.6603 - val_loss: 1.5651 - val_mse: 1.5651\n",
      "Epoch 178/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6549 - mse: 0.6549 - val_loss: 1.5617 - val_mse: 1.5617\n",
      "Epoch 179/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6495 - mse: 0.6495 - val_loss: 1.5582 - val_mse: 1.5582\n",
      "Epoch 180/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6441 - mse: 0.6441 - val_loss: 1.5547 - val_mse: 1.5547\n",
      "Epoch 181/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6387 - mse: 0.6387 - val_loss: 1.5512 - val_mse: 1.5512\n",
      "Epoch 182/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6333 - mse: 0.6333 - val_loss: 1.5477 - val_mse: 1.5477\n",
      "Epoch 183/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6279 - mse: 0.6279 - val_loss: 1.5441 - val_mse: 1.5441\n",
      "Epoch 184/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6226 - mse: 0.6226 - val_loss: 1.5405 - val_mse: 1.5405\n",
      "Epoch 185/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6173 - mse: 0.6173 - val_loss: 1.5369 - val_mse: 1.5369\n",
      "Epoch 186/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6119 - mse: 0.6119 - val_loss: 1.5332 - val_mse: 1.5332\n",
      "Epoch 187/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6066 - mse: 0.6066 - val_loss: 1.5296 - val_mse: 1.5296\n",
      "Epoch 188/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6013 - mse: 0.6013 - val_loss: 1.5259 - val_mse: 1.5259\n",
      "Epoch 189/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5961 - mse: 0.5961 - val_loss: 1.5222 - val_mse: 1.5222\n",
      "Epoch 190/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5908 - mse: 0.5908 - val_loss: 1.5184 - val_mse: 1.5184\n",
      "Epoch 191/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5855 - mse: 0.5855 - val_loss: 1.5147 - val_mse: 1.5147\n",
      "Epoch 192/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5802 - mse: 0.5802 - val_loss: 1.5109 - val_mse: 1.5109\n",
      "Epoch 193/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5750 - mse: 0.5750 - val_loss: 1.5071 - val_mse: 1.5071\n",
      "Epoch 194/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5697 - mse: 0.5697 - val_loss: 1.5032 - val_mse: 1.5032\n",
      "Epoch 195/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5645 - mse: 0.5645 - val_loss: 1.4994 - val_mse: 1.4994\n",
      "Epoch 196/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5593 - mse: 0.5593 - val_loss: 1.4955 - val_mse: 1.4955\n",
      "Epoch 197/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5541 - mse: 0.5541 - val_loss: 1.4916 - val_mse: 1.4916\n",
      "Epoch 198/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5488 - mse: 0.5488 - val_loss: 1.4877 - val_mse: 1.4877\n",
      "Epoch 199/1000\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.5436 - mse: 0.5436 - val_loss: 1.4837 - val_mse: 1.4837\n",
      "Epoch 200/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5384 - mse: 0.5384 - val_loss: 1.4797 - val_mse: 1.4797\n",
      "Epoch 201/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5332 - mse: 0.5332 - val_loss: 1.4757 - val_mse: 1.4757\n",
      "Epoch 202/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5280 - mse: 0.5280 - val_loss: 1.4717 - val_mse: 1.4717\n",
      "Epoch 203/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 1.4677 - val_mse: 1.4677\n",
      "Epoch 204/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5177 - mse: 0.5177 - val_loss: 1.4636 - val_mse: 1.4636\n",
      "Epoch 205/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5125 - mse: 0.5125 - val_loss: 1.4595 - val_mse: 1.4595\n",
      "Epoch 206/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5073 - mse: 0.5073 - val_loss: 1.4554 - val_mse: 1.4554\n",
      "Epoch 207/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5022 - mse: 0.5022 - val_loss: 1.4512 - val_mse: 1.4512\n",
      "Epoch 208/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4970 - mse: 0.4970 - val_loss: 1.4471 - val_mse: 1.4471\n",
      "Epoch 209/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4919 - mse: 0.4919 - val_loss: 1.4429 - val_mse: 1.4429\n",
      "Epoch 210/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4867 - mse: 0.4867 - val_loss: 1.4387 - val_mse: 1.4387\n",
      "Epoch 211/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4816 - mse: 0.4816 - val_loss: 1.4345 - val_mse: 1.4345\n",
      "Epoch 212/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4765 - mse: 0.4765 - val_loss: 1.4302 - val_mse: 1.4302\n",
      "Epoch 213/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4714 - mse: 0.4714 - val_loss: 1.4260 - val_mse: 1.4260\n",
      "Epoch 214/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4663 - mse: 0.4663 - val_loss: 1.4217 - val_mse: 1.4217\n",
      "Epoch 215/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4612 - mse: 0.4612 - val_loss: 1.4174 - val_mse: 1.4174\n",
      "Epoch 216/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4561 - mse: 0.4561 - val_loss: 1.4131 - val_mse: 1.4131\n",
      "Epoch 217/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4510 - mse: 0.4510 - val_loss: 1.4087 - val_mse: 1.4087\n",
      "Epoch 218/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4460 - mse: 0.4460 - val_loss: 1.4044 - val_mse: 1.4044\n",
      "Epoch 219/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4409 - mse: 0.4409 - val_loss: 1.4000 - val_mse: 1.4000\n",
      "Epoch 220/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4359 - mse: 0.4359 - val_loss: 1.3956 - val_mse: 1.3956\n",
      "Epoch 221/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4308 - mse: 0.4308 - val_loss: 1.3912 - val_mse: 1.3912\n",
      "Epoch 222/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4258 - mse: 0.4258 - val_loss: 1.3867 - val_mse: 1.3867\n",
      "Epoch 223/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4208 - mse: 0.4208 - val_loss: 1.3823 - val_mse: 1.3823\n",
      "Epoch 224/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4158 - mse: 0.4158 - val_loss: 1.3778 - val_mse: 1.3778\n",
      "Epoch 225/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4108 - mse: 0.4108 - val_loss: 1.3733 - val_mse: 1.3733\n",
      "Epoch 226/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4058 - mse: 0.4058 - val_loss: 1.3688 - val_mse: 1.3688\n",
      "Epoch 227/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.4009 - mse: 0.4009 - val_loss: 1.3643 - val_mse: 1.3643\n",
      "Epoch 228/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3960 - mse: 0.3960 - val_loss: 1.3597 - val_mse: 1.3597\n",
      "Epoch 229/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3910 - mse: 0.3910 - val_loss: 1.3552 - val_mse: 1.3552\n",
      "Epoch 230/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3861 - mse: 0.3861 - val_loss: 1.3506 - val_mse: 1.3506\n",
      "Epoch 231/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3812 - mse: 0.3812 - val_loss: 1.3461 - val_mse: 1.3461\n",
      "Epoch 232/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3764 - mse: 0.3764 - val_loss: 1.3415 - val_mse: 1.3415\n",
      "Epoch 233/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3715 - mse: 0.3715 - val_loss: 1.3369 - val_mse: 1.3369\n",
      "Epoch 234/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3667 - mse: 0.3667 - val_loss: 1.3322 - val_mse: 1.3322\n",
      "Epoch 235/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3619 - mse: 0.3619 - val_loss: 1.3276 - val_mse: 1.3276\n",
      "Epoch 236/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3571 - mse: 0.3571 - val_loss: 1.3230 - val_mse: 1.3230\n",
      "Epoch 237/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3523 - mse: 0.3523 - val_loss: 1.3183 - val_mse: 1.3183\n",
      "Epoch 238/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3476 - mse: 0.3476 - val_loss: 1.3137 - val_mse: 1.3137\n",
      "Epoch 239/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3429 - mse: 0.3429 - val_loss: 1.3090 - val_mse: 1.3090\n",
      "Epoch 240/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3382 - mse: 0.3382 - val_loss: 1.3043 - val_mse: 1.3043\n",
      "Epoch 241/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3335 - mse: 0.3335 - val_loss: 1.2996 - val_mse: 1.2996\n",
      "Epoch 242/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3288 - mse: 0.3288 - val_loss: 1.2949 - val_mse: 1.2949\n",
      "Epoch 243/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3242 - mse: 0.3242 - val_loss: 1.2902 - val_mse: 1.2902\n",
      "Epoch 244/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3196 - mse: 0.3196 - val_loss: 1.2855 - val_mse: 1.2855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 245/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3151 - mse: 0.3151 - val_loss: 1.2808 - val_mse: 1.2808\n",
      "Epoch 246/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3105 - mse: 0.3105 - val_loss: 1.2761 - val_mse: 1.2761\n",
      "Epoch 247/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3060 - mse: 0.3060 - val_loss: 1.2714 - val_mse: 1.2714\n",
      "Epoch 248/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3015 - mse: 0.3015 - val_loss: 1.2667 - val_mse: 1.2667\n",
      "Epoch 249/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2971 - mse: 0.2971 - val_loss: 1.2619 - val_mse: 1.2619\n",
      "Epoch 250/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2927 - mse: 0.2927 - val_loss: 1.2572 - val_mse: 1.2572\n",
      "Epoch 251/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2883 - mse: 0.2883 - val_loss: 1.2525 - val_mse: 1.2525\n",
      "Epoch 252/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2839 - mse: 0.2839 - val_loss: 1.2478 - val_mse: 1.2478\n",
      "Epoch 253/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2796 - mse: 0.2796 - val_loss: 1.2430 - val_mse: 1.2430\n",
      "Epoch 254/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2754 - mse: 0.2754 - val_loss: 1.2383 - val_mse: 1.2383\n",
      "Epoch 255/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2711 - mse: 0.2711 - val_loss: 1.2336 - val_mse: 1.2336\n",
      "Epoch 256/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2669 - mse: 0.2669 - val_loss: 1.2289 - val_mse: 1.2289\n",
      "Epoch 257/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2627 - mse: 0.2627 - val_loss: 1.2241 - val_mse: 1.2241\n",
      "Epoch 258/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.2586 - mse: 0.2586 - val_loss: 1.2194 - val_mse: 1.2194\n",
      "Epoch 259/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2545 - mse: 0.2545 - val_loss: 1.2147 - val_mse: 1.2147\n",
      "Epoch 260/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2505 - mse: 0.2505 - val_loss: 1.2100 - val_mse: 1.2100\n",
      "Epoch 261/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2465 - mse: 0.2465 - val_loss: 1.2053 - val_mse: 1.2053\n",
      "Epoch 262/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2425 - mse: 0.2425 - val_loss: 1.2006 - val_mse: 1.2006\n",
      "Epoch 263/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2386 - mse: 0.2386 - val_loss: 1.1959 - val_mse: 1.1959\n",
      "Epoch 264/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2347 - mse: 0.2347 - val_loss: 1.1913 - val_mse: 1.1913\n",
      "Epoch 265/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2309 - mse: 0.2309 - val_loss: 1.1866 - val_mse: 1.1866\n",
      "Epoch 266/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2271 - mse: 0.2271 - val_loss: 1.1819 - val_mse: 1.1819\n",
      "Epoch 267/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2233 - mse: 0.2233 - val_loss: 1.1773 - val_mse: 1.1773\n",
      "Epoch 268/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2196 - mse: 0.2196 - val_loss: 1.1727 - val_mse: 1.1727\n",
      "Epoch 269/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2159 - mse: 0.2159 - val_loss: 1.1681 - val_mse: 1.1681\n",
      "Epoch 270/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2123 - mse: 0.2123 - val_loss: 1.1635 - val_mse: 1.1635\n",
      "Epoch 271/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2088 - mse: 0.2088 - val_loss: 1.1589 - val_mse: 1.1589\n",
      "Epoch 272/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2052 - mse: 0.2052 - val_loss: 1.1543 - val_mse: 1.1543\n",
      "Epoch 273/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2018 - mse: 0.2018 - val_loss: 1.1498 - val_mse: 1.1498\n",
      "Epoch 274/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1983 - mse: 0.1983 - val_loss: 1.1452 - val_mse: 1.1452\n",
      "Epoch 275/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1950 - mse: 0.1950 - val_loss: 1.1407 - val_mse: 1.1407\n",
      "Epoch 276/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1916 - mse: 0.1916 - val_loss: 1.1362 - val_mse: 1.1362\n",
      "Epoch 277/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1883 - mse: 0.1883 - val_loss: 1.1317 - val_mse: 1.1317\n",
      "Epoch 278/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1851 - mse: 0.1851 - val_loss: 1.1273 - val_mse: 1.1273\n",
      "Epoch 279/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1819 - mse: 0.1819 - val_loss: 1.1228 - val_mse: 1.1228\n",
      "Epoch 280/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1788 - mse: 0.1788 - val_loss: 1.1184 - val_mse: 1.1184\n",
      "Epoch 281/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1757 - mse: 0.1757 - val_loss: 1.1140 - val_mse: 1.1140\n",
      "Epoch 282/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1727 - mse: 0.1727 - val_loss: 1.1096 - val_mse: 1.1096\n",
      "Epoch 283/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1697 - mse: 0.1697 - val_loss: 1.1053 - val_mse: 1.1053\n",
      "Epoch 284/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1667 - mse: 0.1667 - val_loss: 1.1010 - val_mse: 1.1010\n",
      "Epoch 285/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1638 - mse: 0.1638 - val_loss: 1.0967 - val_mse: 1.0967\n",
      "Epoch 286/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1610 - mse: 0.1610 - val_loss: 1.0924 - val_mse: 1.0924\n",
      "Epoch 287/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1582 - mse: 0.1582 - val_loss: 1.0881 - val_mse: 1.0881\n",
      "Epoch 288/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1555 - mse: 0.1555 - val_loss: 1.0839 - val_mse: 1.0839\n",
      "Epoch 289/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1528 - mse: 0.1528 - val_loss: 1.0797 - val_mse: 1.0797\n",
      "Epoch 290/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1502 - mse: 0.1502 - val_loss: 1.0756 - val_mse: 1.0756\n",
      "Epoch 291/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1476 - mse: 0.1476 - val_loss: 1.0714 - val_mse: 1.0714\n",
      "Epoch 292/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1450 - mse: 0.1450 - val_loss: 1.0673 - val_mse: 1.0673\n",
      "Epoch 293/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1425 - mse: 0.1425 - val_loss: 1.0632 - val_mse: 1.0632\n",
      "Epoch 294/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1401 - mse: 0.1401 - val_loss: 1.0592 - val_mse: 1.0592\n",
      "Epoch 295/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1377 - mse: 0.1377 - val_loss: 1.0552 - val_mse: 1.0552\n",
      "Epoch 296/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1353 - mse: 0.1353 - val_loss: 1.0512 - val_mse: 1.0512\n",
      "Epoch 297/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1330 - mse: 0.1330 - val_loss: 1.0472 - val_mse: 1.0472\n",
      "Epoch 298/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1308 - mse: 0.1308 - val_loss: 1.0433 - val_mse: 1.0433\n",
      "Epoch 299/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1286 - mse: 0.1286 - val_loss: 1.0394 - val_mse: 1.0394\n",
      "Epoch 300/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1264 - mse: 0.1264 - val_loss: 1.0356 - val_mse: 1.0356\n",
      "Epoch 301/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1243 - mse: 0.1243 - val_loss: 1.0317 - val_mse: 1.0317\n",
      "Epoch 302/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1223 - mse: 0.1223 - val_loss: 1.0279 - val_mse: 1.0279\n",
      "Epoch 303/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1202 - mse: 0.1202 - val_loss: 1.0242 - val_mse: 1.0242\n",
      "Epoch 304/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1183 - mse: 0.1183 - val_loss: 1.0205 - val_mse: 1.0205\n",
      "Epoch 305/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1163 - mse: 0.1163 - val_loss: 1.0168 - val_mse: 1.0168\n",
      "Epoch 306/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.1144 - mse: 0.1144 - val_loss: 1.0131 - val_mse: 1.0131\n",
      "Epoch 307/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.1126 - mse: 0.1126 - val_loss: 1.0095 - val_mse: 1.0095\n",
      "Epoch 308/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.1108 - mse: 0.1108 - val_loss: 1.0059 - val_mse: 1.0059\n",
      "Epoch 309/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.1090 - mse: 0.1090 - val_loss: 1.0024 - val_mse: 1.0024\n",
      "Epoch 310/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.1073 - mse: 0.1073 - val_loss: 0.9988 - val_mse: 0.9988\n",
      "Epoch 311/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.1056 - mse: 0.1056 - val_loss: 0.9953 - val_mse: 0.9953\n",
      "Epoch 312/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.1040 - mse: 0.1040 - val_loss: 0.9919 - val_mse: 0.9919\n",
      "Epoch 313/1000\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.1024 - mse: 0.1024 - val_loss: 0.9885 - val_mse: 0.9885\n",
      "Epoch 314/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.1008 - mse: 0.1008 - val_loss: 0.9851 - val_mse: 0.9851\n",
      "Epoch 315/1000\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0993 - mse: 0.0993 - val_loss: 0.9818 - val_mse: 0.9818\n",
      "Epoch 316/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.9785 - val_mse: 0.9785\n",
      "Epoch 317/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0963 - mse: 0.0963 - val_loss: 0.9752 - val_mse: 0.9752\n",
      "Epoch 318/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0949 - mse: 0.0949 - val_loss: 0.9719 - val_mse: 0.9719\n",
      "Epoch 319/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0935 - mse: 0.0935 - val_loss: 0.9687 - val_mse: 0.9687\n",
      "Epoch 320/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0922 - mse: 0.0922 - val_loss: 0.9656 - val_mse: 0.9656\n",
      "Epoch 321/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0909 - mse: 0.0909 - val_loss: 0.9624 - val_mse: 0.9624\n",
      "Epoch 322/1000\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.0896 - mse: 0.0896 - val_loss: 0.9593 - val_mse: 0.9593\n",
      "Epoch 323/1000\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.0883 - mse: 0.0883 - val_loss: 0.9563 - val_mse: 0.9563\n",
      "Epoch 324/1000\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0871 - mse: 0.0871 - val_loss: 0.9533 - val_mse: 0.9533\n",
      "Epoch 325/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0859 - mse: 0.0859 - val_loss: 0.9503 - val_mse: 0.9503\n",
      "Epoch 326/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0848 - mse: 0.0848 - val_loss: 0.9473 - val_mse: 0.9473\n",
      "Epoch 327/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0836 - mse: 0.0836 - val_loss: 0.9444 - val_mse: 0.9444\n",
      "Epoch 328/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0825 - mse: 0.0825 - val_loss: 0.9415 - val_mse: 0.9415\n",
      "Epoch 329/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0814 - mse: 0.0814 - val_loss: 0.9386 - val_mse: 0.9386\n",
      "Epoch 330/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0804 - mse: 0.0804 - val_loss: 0.9358 - val_mse: 0.9358\n",
      "Epoch 331/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0794 - mse: 0.0794 - val_loss: 0.9330 - val_mse: 0.9330\n",
      "Epoch 332/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0784 - mse: 0.0784 - val_loss: 0.9302 - val_mse: 0.9302\n",
      "Epoch 333/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0774 - mse: 0.0774 - val_loss: 0.9275 - val_mse: 0.9275\n",
      "Epoch 334/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0764 - mse: 0.0764 - val_loss: 0.9248 - val_mse: 0.9248\n",
      "Epoch 335/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0755 - mse: 0.0755 - val_loss: 0.9221 - val_mse: 0.9221\n",
      "Epoch 336/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0746 - mse: 0.0746 - val_loss: 0.9195 - val_mse: 0.9195\n",
      "Epoch 337/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0737 - mse: 0.0737 - val_loss: 0.9169 - val_mse: 0.9169\n",
      "Epoch 338/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0728 - mse: 0.0728 - val_loss: 0.9143 - val_mse: 0.9143\n",
      "Epoch 339/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0720 - mse: 0.0720 - val_loss: 0.9118 - val_mse: 0.9118\n",
      "Epoch 340/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0712 - mse: 0.0712 - val_loss: 0.9093 - val_mse: 0.9093\n",
      "Epoch 341/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0704 - mse: 0.0704 - val_loss: 0.9068 - val_mse: 0.9068\n",
      "Epoch 342/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0696 - mse: 0.0696 - val_loss: 0.9043 - val_mse: 0.9043\n",
      "Epoch 343/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0688 - mse: 0.0688 - val_loss: 0.9019 - val_mse: 0.9019\n",
      "Epoch 344/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0681 - mse: 0.0681 - val_loss: 0.8995 - val_mse: 0.8995\n",
      "Epoch 345/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0673 - mse: 0.0673 - val_loss: 0.8972 - val_mse: 0.8972\n",
      "Epoch 346/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0666 - mse: 0.0666 - val_loss: 0.8948 - val_mse: 0.8948\n",
      "Epoch 347/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0659 - mse: 0.0659 - val_loss: 0.8925 - val_mse: 0.8925\n",
      "Epoch 348/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0652 - mse: 0.0652 - val_loss: 0.8902 - val_mse: 0.8902\n",
      "Epoch 349/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0645 - mse: 0.0645 - val_loss: 0.8880 - val_mse: 0.8880\n",
      "Epoch 350/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0639 - mse: 0.0639 - val_loss: 0.8857 - val_mse: 0.8857\n",
      "Epoch 351/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0632 - mse: 0.0632 - val_loss: 0.8835 - val_mse: 0.8835\n",
      "Epoch 352/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0626 - mse: 0.0626 - val_loss: 0.8813 - val_mse: 0.8813\n",
      "Epoch 353/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0620 - mse: 0.0620 - val_loss: 0.8792 - val_mse: 0.8792\n",
      "Epoch 354/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0614 - mse: 0.0614 - val_loss: 0.8771 - val_mse: 0.8771\n",
      "Epoch 355/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0608 - mse: 0.0608 - val_loss: 0.8749 - val_mse: 0.8749\n",
      "Epoch 356/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0602 - mse: 0.0602 - val_loss: 0.8729 - val_mse: 0.8729\n",
      "Epoch 357/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0596 - mse: 0.0596 - val_loss: 0.8708 - val_mse: 0.8708\n",
      "Epoch 358/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0590 - mse: 0.0590 - val_loss: 0.8688 - val_mse: 0.8688\n",
      "Epoch 359/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0585 - mse: 0.0585 - val_loss: 0.8668 - val_mse: 0.8668\n",
      "Epoch 360/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0579 - mse: 0.0579 - val_loss: 0.8648 - val_mse: 0.8648\n",
      "Epoch 361/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0574 - mse: 0.0574 - val_loss: 0.8628 - val_mse: 0.8628\n",
      "Epoch 362/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0569 - mse: 0.0569 - val_loss: 0.8608 - val_mse: 0.8608\n",
      "Epoch 363/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0564 - mse: 0.0564 - val_loss: 0.8589 - val_mse: 0.8589\n",
      "Epoch 364/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0559 - mse: 0.0559 - val_loss: 0.8570 - val_mse: 0.8570\n",
      "Epoch 365/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0554 - mse: 0.0554 - val_loss: 0.8551 - val_mse: 0.8551\n",
      "Epoch 366/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0549 - mse: 0.0549 - val_loss: 0.8532 - val_mse: 0.8532\n",
      "Epoch 367/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0544 - mse: 0.0544 - val_loss: 0.8514 - val_mse: 0.8514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0539 - mse: 0.0539 - val_loss: 0.8496 - val_mse: 0.8496\n",
      "Epoch 369/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0534 - mse: 0.0534 - val_loss: 0.8478 - val_mse: 0.8478\n",
      "Epoch 370/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0530 - mse: 0.0530 - val_loss: 0.8460 - val_mse: 0.8460\n",
      "Epoch 371/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0525 - mse: 0.0525 - val_loss: 0.8442 - val_mse: 0.8442\n",
      "Epoch 372/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0521 - mse: 0.0521 - val_loss: 0.8424 - val_mse: 0.8424\n",
      "Epoch 373/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0516 - mse: 0.0516 - val_loss: 0.8407 - val_mse: 0.8407\n",
      "Epoch 374/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0512 - mse: 0.0512 - val_loss: 0.8390 - val_mse: 0.8390\n",
      "Epoch 375/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0507 - mse: 0.0507 - val_loss: 0.8373 - val_mse: 0.8373\n",
      "Epoch 376/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0503 - mse: 0.0503 - val_loss: 0.8356 - val_mse: 0.8356\n",
      "Epoch 377/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0499 - mse: 0.0499 - val_loss: 0.8339 - val_mse: 0.8339\n",
      "Epoch 378/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0495 - mse: 0.0495 - val_loss: 0.8322 - val_mse: 0.8322\n",
      "Epoch 379/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0491 - mse: 0.0491 - val_loss: 0.8306 - val_mse: 0.8306\n",
      "Epoch 380/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0487 - mse: 0.0487 - val_loss: 0.8290 - val_mse: 0.8290\n",
      "Epoch 381/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0483 - mse: 0.0483 - val_loss: 0.8274 - val_mse: 0.8274\n",
      "Epoch 382/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0479 - mse: 0.0479 - val_loss: 0.8258 - val_mse: 0.8258\n",
      "Epoch 383/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0475 - mse: 0.0475 - val_loss: 0.8242 - val_mse: 0.8242\n",
      "Epoch 384/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0471 - mse: 0.0471 - val_loss: 0.8226 - val_mse: 0.8226\n",
      "Epoch 385/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0467 - mse: 0.0467 - val_loss: 0.8210 - val_mse: 0.8210\n",
      "Epoch 386/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0463 - mse: 0.0463 - val_loss: 0.8195 - val_mse: 0.8195\n",
      "Epoch 387/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0459 - mse: 0.0459 - val_loss: 0.8180 - val_mse: 0.8180\n",
      "Epoch 388/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0455 - mse: 0.0455 - val_loss: 0.8164 - val_mse: 0.8164\n",
      "Epoch 389/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0452 - mse: 0.0452 - val_loss: 0.8149 - val_mse: 0.8149\n",
      "Epoch 390/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0448 - mse: 0.0448 - val_loss: 0.8134 - val_mse: 0.8134\n",
      "Epoch 391/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0444 - mse: 0.0444 - val_loss: 0.8119 - val_mse: 0.8119\n",
      "Epoch 392/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0441 - mse: 0.0441 - val_loss: 0.8105 - val_mse: 0.8105\n",
      "Epoch 393/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0437 - mse: 0.0437 - val_loss: 0.8090 - val_mse: 0.8090\n",
      "Epoch 394/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0434 - mse: 0.0434 - val_loss: 0.8076 - val_mse: 0.8076\n",
      "Epoch 395/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.8061 - val_mse: 0.8061\n",
      "Epoch 396/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.8047 - val_mse: 0.8047\n",
      "Epoch 397/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0423 - mse: 0.0423 - val_loss: 0.8033 - val_mse: 0.8033\n",
      "Epoch 398/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0420 - mse: 0.0420 - val_loss: 0.8019 - val_mse: 0.8019\n",
      "Epoch 399/1000\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0416 - mse: 0.0416 - val_loss: 0.8005 - val_mse: 0.8005\n",
      "Epoch 400/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.7991 - val_mse: 0.7991\n",
      "Epoch 401/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.7977 - val_mse: 0.7977\n",
      "Epoch 402/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0406 - mse: 0.0406 - val_loss: 0.7963 - val_mse: 0.7963\n",
      "Epoch 403/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0403 - mse: 0.0403 - val_loss: 0.7949 - val_mse: 0.7949\n",
      "Epoch 404/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.7936 - val_mse: 0.7936\n",
      "Epoch 405/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.7922 - val_mse: 0.7922\n",
      "Epoch 406/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.7909 - val_mse: 0.7909\n",
      "Epoch 407/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.7896 - val_mse: 0.7896\n",
      "Epoch 408/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.7882 - val_mse: 0.7882\n",
      "Epoch 409/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.7869 - val_mse: 0.7869\n",
      "Epoch 410/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.7856 - val_mse: 0.7856\n",
      "Epoch 411/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.7843 - val_mse: 0.7843\n",
      "Epoch 412/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.7830 - val_mse: 0.7830\n",
      "Epoch 413/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0371 - mse: 0.0371 - val_loss: 0.7817 - val_mse: 0.7817\n",
      "Epoch 414/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.7805 - val_mse: 0.7805\n",
      "Epoch 415/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.7792 - val_mse: 0.7792\n",
      "Epoch 416/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.7779 - val_mse: 0.7779\n",
      "Epoch 417/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0359 - mse: 0.0359 - val_loss: 0.7767 - val_mse: 0.7767\n",
      "Epoch 418/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.7754 - val_mse: 0.7754\n",
      "Epoch 419/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.7742 - val_mse: 0.7742\n",
      "Epoch 420/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0350 - mse: 0.0350 - val_loss: 0.7729 - val_mse: 0.7729\n",
      "Epoch 421/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0347 - mse: 0.0347 - val_loss: 0.7717 - val_mse: 0.7717\n",
      "Epoch 422/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0345 - mse: 0.0345 - val_loss: 0.7705 - val_mse: 0.7705\n",
      "Epoch 423/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.7692 - val_mse: 0.7692\n",
      "Epoch 424/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0339 - mse: 0.0339 - val_loss: 0.7680 - val_mse: 0.7680\n",
      "Epoch 425/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.7668 - val_mse: 0.7668\n",
      "Epoch 426/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.7656 - val_mse: 0.7656\n",
      "Epoch 427/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.7644 - val_mse: 0.7644\n",
      "Epoch 428/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0328 - mse: 0.0328 - val_loss: 0.7632 - val_mse: 0.7632\n",
      "Epoch 429/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0325 - mse: 0.0325 - val_loss: 0.7620 - val_mse: 0.7620\n",
      "Epoch 430/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.7608 - val_mse: 0.7608\n",
      "Epoch 431/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.7596 - val_mse: 0.7596\n",
      "Epoch 432/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.7585 - val_mse: 0.7585\n",
      "Epoch 433/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.7573 - val_mse: 0.7573\n",
      "Epoch 434/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0311 - mse: 0.0311 - val_loss: 0.7561 - val_mse: 0.7561\n",
      "Epoch 435/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.7549 - val_mse: 0.7549\n",
      "Epoch 436/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.7538 - val_mse: 0.7538\n",
      "Epoch 437/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.7526 - val_mse: 0.7526\n",
      "Epoch 438/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.7515 - val_mse: 0.7515\n",
      "Epoch 439/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.7503 - val_mse: 0.7503\n",
      "Epoch 440/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.7492 - val_mse: 0.7492\n",
      "Epoch 441/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.7480 - val_mse: 0.7480\n",
      "Epoch 442/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.7469 - val_mse: 0.7469\n",
      "Epoch 443/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.7458 - val_mse: 0.7458\n",
      "Epoch 444/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.7446 - val_mse: 0.7446\n",
      "Epoch 445/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.7435 - val_mse: 0.7435\n",
      "Epoch 446/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.7424 - val_mse: 0.7424\n",
      "Epoch 447/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.7413 - val_mse: 0.7413\n",
      "Epoch 448/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.7402 - val_mse: 0.7402\n",
      "Epoch 449/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.7391 - val_mse: 0.7391\n",
      "Epoch 450/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.7380 - val_mse: 0.7380\n",
      "Epoch 451/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.7368 - val_mse: 0.7368\n",
      "Epoch 452/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.7357 - val_mse: 0.7357\n",
      "Epoch 453/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.7346 - val_mse: 0.7346\n",
      "Epoch 454/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.7336 - val_mse: 0.7336\n",
      "Epoch 455/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.7325 - val_mse: 0.7325\n",
      "Epoch 456/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.7314 - val_mse: 0.7314\n",
      "Epoch 457/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.7303 - val_mse: 0.7303\n",
      "Epoch 458/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.7292 - val_mse: 0.7292\n",
      "Epoch 459/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.7281 - val_mse: 0.7281\n",
      "Epoch 460/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.7271 - val_mse: 0.7271\n",
      "Epoch 461/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.7260 - val_mse: 0.7260\n",
      "Epoch 462/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0242 - mse: 0.0242 - val_loss: 0.7249 - val_mse: 0.7249\n",
      "Epoch 463/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0240 - mse: 0.0240 - val_loss: 0.7238 - val_mse: 0.7238\n",
      "Epoch 464/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0238 - mse: 0.0238 - val_loss: 0.7228 - val_mse: 0.7228\n",
      "Epoch 465/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.7217 - val_mse: 0.7217\n",
      "Epoch 466/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0233 - mse: 0.0233 - val_loss: 0.7207 - val_mse: 0.7207\n",
      "Epoch 467/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0231 - mse: 0.0231 - val_loss: 0.7196 - val_mse: 0.7196\n",
      "Epoch 468/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.7185 - val_mse: 0.7185\n",
      "Epoch 469/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0227 - mse: 0.0227 - val_loss: 0.7175 - val_mse: 0.7175\n",
      "Epoch 470/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0224 - mse: 0.0224 - val_loss: 0.7164 - val_mse: 0.7164\n",
      "Epoch 471/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0222 - mse: 0.0222 - val_loss: 0.7154 - val_mse: 0.7154\n",
      "Epoch 472/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.7144 - val_mse: 0.7144\n",
      "Epoch 473/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.7133 - val_mse: 0.7133\n",
      "Epoch 474/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.7123 - val_mse: 0.7123\n",
      "Epoch 475/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.7112 - val_mse: 0.7112\n",
      "Epoch 476/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0212 - mse: 0.0212 - val_loss: 0.7102 - val_mse: 0.7102\n",
      "Epoch 477/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.7092 - val_mse: 0.7092\n",
      "Epoch 478/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.7082 - val_mse: 0.7082\n",
      "Epoch 479/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.7071 - val_mse: 0.7071\n",
      "Epoch 480/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.7061 - val_mse: 0.7061\n",
      "Epoch 481/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.7051 - val_mse: 0.7051\n",
      "Epoch 482/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.7041 - val_mse: 0.7041\n",
      "Epoch 483/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.7030 - val_mse: 0.7030\n",
      "Epoch 484/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.7020 - val_mse: 0.7020\n",
      "Epoch 485/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0194 - mse: 0.0194 - val_loss: 0.7010 - val_mse: 0.7010\n",
      "Epoch 486/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.7000 - val_mse: 0.7000\n",
      "Epoch 487/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.6990 - val_mse: 0.6990\n",
      "Epoch 488/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.6980 - val_mse: 0.6980\n",
      "Epoch 489/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0186 - mse: 0.0186 - val_loss: 0.6970 - val_mse: 0.6970\n",
      "Epoch 490/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.6960 - val_mse: 0.6960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.6950 - val_mse: 0.6950\n",
      "Epoch 492/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.6940 - val_mse: 0.6940\n",
      "Epoch 493/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.6930 - val_mse: 0.6930\n",
      "Epoch 494/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.6920 - val_mse: 0.6920\n",
      "Epoch 495/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.6910 - val_mse: 0.6910\n",
      "Epoch 496/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.6901 - val_mse: 0.6901\n",
      "Epoch 497/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.6891 - val_mse: 0.6891\n",
      "Epoch 498/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.6881 - val_mse: 0.6881\n",
      "Epoch 499/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.6871 - val_mse: 0.6871\n",
      "Epoch 500/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.6861 - val_mse: 0.6861\n",
      "Epoch 501/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.6852 - val_mse: 0.6852\n",
      "Epoch 502/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.6842 - val_mse: 0.6842\n",
      "Epoch 503/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.6832 - val_mse: 0.6832\n",
      "Epoch 504/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.6823 - val_mse: 0.6823\n",
      "Epoch 505/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.6813 - val_mse: 0.6813\n",
      "Epoch 506/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.6803 - val_mse: 0.6803\n",
      "Epoch 507/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.6794 - val_mse: 0.6794\n",
      "Epoch 508/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.6784 - val_mse: 0.6784\n",
      "Epoch 509/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.6775 - val_mse: 0.6775\n",
      "Epoch 510/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.6765 - val_mse: 0.6765\n",
      "Epoch 511/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.6755 - val_mse: 0.6755\n",
      "Epoch 512/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.6746 - val_mse: 0.6746\n",
      "Epoch 513/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.6737 - val_mse: 0.6737\n",
      "Epoch 514/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.6727 - val_mse: 0.6727\n",
      "Epoch 515/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.6718 - val_mse: 0.6718\n",
      "Epoch 516/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.6708 - val_mse: 0.6708\n",
      "Epoch 517/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.6699 - val_mse: 0.6699\n",
      "Epoch 518/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.6690 - val_mse: 0.6690\n",
      "Epoch 519/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.6680 - val_mse: 0.6680\n",
      "Epoch 520/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.6671 - val_mse: 0.6671\n",
      "Epoch 521/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.6662 - val_mse: 0.6662\n",
      "Epoch 522/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.6652 - val_mse: 0.6652\n",
      "Epoch 523/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.6643 - val_mse: 0.6643\n",
      "Epoch 524/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.6634 - val_mse: 0.6634\n",
      "Epoch 525/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.6625 - val_mse: 0.6625\n",
      "Epoch 526/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.6616 - val_mse: 0.6616\n",
      "Epoch 527/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.6606 - val_mse: 0.6606\n",
      "Epoch 528/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.6597 - val_mse: 0.6597\n",
      "Epoch 529/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.6588 - val_mse: 0.6588\n",
      "Epoch 530/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.6579 - val_mse: 0.6579\n",
      "Epoch 531/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.6570 - val_mse: 0.6570\n",
      "Epoch 532/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.6561 - val_mse: 0.6561\n",
      "Epoch 533/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.6552 - val_mse: 0.6552\n",
      "Epoch 534/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.6543 - val_mse: 0.6543\n",
      "Epoch 535/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.6534 - val_mse: 0.6534\n",
      "Epoch 536/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.6525 - val_mse: 0.6525\n",
      "Epoch 537/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.6516 - val_mse: 0.6516\n",
      "Epoch 538/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.6508 - val_mse: 0.6508\n",
      "Epoch 539/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.6499 - val_mse: 0.6499\n",
      "Epoch 540/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.6490 - val_mse: 0.6490\n",
      "Epoch 541/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.6481 - val_mse: 0.6481\n",
      "Epoch 542/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.6472 - val_mse: 0.6472\n",
      "Epoch 543/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.6464 - val_mse: 0.6464\n",
      "Epoch 544/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.6455 - val_mse: 0.6455\n",
      "Epoch 545/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.6446 - val_mse: 0.6446\n",
      "Epoch 546/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.6438 - val_mse: 0.6438\n",
      "Epoch 547/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.6429 - val_mse: 0.6429\n",
      "Epoch 548/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.6420 - val_mse: 0.6420\n",
      "Epoch 549/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.6412 - val_mse: 0.6412\n",
      "Epoch 550/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.6403 - val_mse: 0.6403\n",
      "Epoch 551/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.6395 - val_mse: 0.6395\n",
      "Epoch 552/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.6386 - val_mse: 0.6386\n",
      "Epoch 553/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.6378 - val_mse: 0.6378\n",
      "Epoch 554/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.6369 - val_mse: 0.6369\n",
      "Epoch 555/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.6361 - val_mse: 0.6361\n",
      "Epoch 556/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.6352 - val_mse: 0.6352\n",
      "Epoch 557/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.6344 - val_mse: 0.6344\n",
      "Epoch 558/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.6336 - val_mse: 0.6336\n",
      "Epoch 559/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.6327 - val_mse: 0.6327\n",
      "Epoch 560/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.6319 - val_mse: 0.6319\n",
      "Epoch 561/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.6311 - val_mse: 0.6311\n",
      "Epoch 562/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.6303 - val_mse: 0.6303\n",
      "Epoch 563/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.6294 - val_mse: 0.6294\n",
      "Epoch 564/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.6286 - val_mse: 0.6286\n",
      "Epoch 565/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.6278 - val_mse: 0.6278\n",
      "Epoch 566/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.6270 - val_mse: 0.6270\n",
      "Epoch 567/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.6262 - val_mse: 0.6262\n",
      "Epoch 568/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.6254 - val_mse: 0.6254\n",
      "Epoch 569/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.6246 - val_mse: 0.6246\n",
      "Epoch 570/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.6238 - val_mse: 0.6238\n",
      "Epoch 571/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.6230 - val_mse: 0.6230\n",
      "Epoch 572/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.6222 - val_mse: 0.6222\n",
      "Epoch 573/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.6214 - val_mse: 0.6214\n",
      "Epoch 574/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.6206 - val_mse: 0.6206\n",
      "Epoch 575/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.6198 - val_mse: 0.6198\n",
      "Epoch 576/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.6190 - val_mse: 0.6190\n",
      "Epoch 577/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.6183 - val_mse: 0.6183\n",
      "Epoch 578/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.6175 - val_mse: 0.6175\n",
      "Epoch 579/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.6167 - val_mse: 0.6167\n",
      "Epoch 580/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.6159 - val_mse: 0.6159\n",
      "Epoch 581/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.6152 - val_mse: 0.6152\n",
      "Epoch 582/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.6144 - val_mse: 0.6144\n",
      "Epoch 583/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.6136 - val_mse: 0.6136\n",
      "Epoch 584/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.6129 - val_mse: 0.6129\n",
      "Epoch 585/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.6121 - val_mse: 0.6121\n",
      "Epoch 586/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.6114 - val_mse: 0.6114\n",
      "Epoch 587/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.6106 - val_mse: 0.6106\n",
      "Epoch 588/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.6099 - val_mse: 0.6099\n",
      "Epoch 589/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.6091 - val_mse: 0.6091\n",
      "Epoch 590/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.6084 - val_mse: 0.6084\n",
      "Epoch 591/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.6076 - val_mse: 0.6076\n",
      "Epoch 592/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.6069 - val_mse: 0.6069\n",
      "Epoch 593/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.6062 - val_mse: 0.6062\n",
      "Epoch 594/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.6054 - val_mse: 0.6054\n",
      "Epoch 595/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.6047 - val_mse: 0.6047\n",
      "Epoch 596/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.6040 - val_mse: 0.6040\n",
      "Epoch 597/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.6033 - val_mse: 0.6033\n",
      "Epoch 598/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.6026 - val_mse: 0.6026\n",
      "Epoch 599/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.6018 - val_mse: 0.6018\n",
      "Epoch 600/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.6011 - val_mse: 0.6011\n",
      "Epoch 601/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.6004 - val_mse: 0.6004\n",
      "Epoch 602/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.5997 - val_mse: 0.5997\n",
      "Epoch 603/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.5990 - val_mse: 0.5990\n",
      "Epoch 604/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.5983 - val_mse: 0.5983\n",
      "Epoch 605/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.5976 - val_mse: 0.5976\n",
      "Epoch 606/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.5969 - val_mse: 0.5969\n",
      "Epoch 607/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.5962 - val_mse: 0.5962\n",
      "Epoch 608/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.5955 - val_mse: 0.5955\n",
      "Epoch 609/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.5949 - val_mse: 0.5949\n",
      "Epoch 610/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.5942 - val_mse: 0.5942\n",
      "Epoch 611/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.5935 - val_mse: 0.5935\n",
      "Epoch 612/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.5928 - val_mse: 0.5928\n",
      "Epoch 613/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.5922 - val_mse: 0.5922\n",
      "Epoch 614/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.5915 - val_mse: 0.5915\n",
      "Epoch 615/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.5908 - val_mse: 0.5908\n",
      "Epoch 616/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.5902 - val_mse: 0.5902\n",
      "Epoch 617/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.5895 - val_mse: 0.5895\n",
      "Epoch 618/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.5889 - val_mse: 0.5889\n",
      "Epoch 619/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.5882 - val_mse: 0.5882\n",
      "Epoch 620/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.5876 - val_mse: 0.5876\n",
      "Epoch 621/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.5869 - val_mse: 0.5869\n",
      "Epoch 622/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.5863 - val_mse: 0.5863\n",
      "Epoch 623/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.5856 - val_mse: 0.5856\n",
      "Epoch 624/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.5850 - val_mse: 0.5850\n",
      "Epoch 625/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.5844 - val_mse: 0.5844\n",
      "Epoch 626/1000\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.5837 - val_mse: 0.5837\n",
      "Epoch 627/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.5831 - val_mse: 0.5831\n",
      "Epoch 628/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.5825 - val_mse: 0.5825\n",
      "Epoch 629/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.5819 - val_mse: 0.5819\n",
      "Epoch 630/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.5813 - val_mse: 0.5813\n",
      "Epoch 631/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.5806 - val_mse: 0.5806\n",
      "Epoch 632/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.5800 - val_mse: 0.5800\n",
      "Epoch 633/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.5794 - val_mse: 0.5794\n",
      "Epoch 634/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.5788 - val_mse: 0.5788\n",
      "Epoch 635/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.5782 - val_mse: 0.5782\n",
      "Epoch 636/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.5776 - val_mse: 0.5776\n",
      "Epoch 637/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.5770 - val_mse: 0.5770\n",
      "Epoch 638/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.5765 - val_mse: 0.5765\n",
      "Epoch 639/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.5759 - val_mse: 0.5759\n",
      "Epoch 640/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.5753 - val_mse: 0.5753\n",
      "Epoch 641/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.5747 - val_mse: 0.5747\n",
      "Epoch 642/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.5741 - val_mse: 0.5741\n",
      "Epoch 643/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.5736 - val_mse: 0.5736\n",
      "Epoch 644/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.5730 - val_mse: 0.5730\n",
      "Epoch 645/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.5724 - val_mse: 0.5724\n",
      "Epoch 646/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.5719 - val_mse: 0.5719\n",
      "Epoch 647/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.5713 - val_mse: 0.5713\n",
      "Epoch 648/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.5707 - val_mse: 0.5707\n",
      "Epoch 649/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.5702 - val_mse: 0.5702\n",
      "Epoch 650/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.5696 - val_mse: 0.5696\n",
      "Epoch 651/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.5691 - val_mse: 0.5691\n",
      "Epoch 652/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.5685 - val_mse: 0.5685\n",
      "Epoch 653/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.5680 - val_mse: 0.5680\n",
      "Epoch 654/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.5675 - val_mse: 0.5675\n",
      "Epoch 655/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.5669 - val_mse: 0.5669\n",
      "Epoch 656/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.5664 - val_mse: 0.5664\n",
      "Epoch 657/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.5659 - val_mse: 0.5659\n",
      "Epoch 658/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.5654 - val_mse: 0.5654\n",
      "Epoch 659/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.5648 - val_mse: 0.5648\n",
      "Epoch 660/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.5643 - val_mse: 0.5643\n",
      "Epoch 661/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.5638 - val_mse: 0.5638\n",
      "Epoch 662/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.5633 - val_mse: 0.5633\n",
      "Epoch 663/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.5628 - val_mse: 0.5628\n",
      "Epoch 664/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.5623 - val_mse: 0.5623\n",
      "Epoch 665/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.5618 - val_mse: 0.5618\n",
      "Epoch 666/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.5613 - val_mse: 0.5613\n",
      "Epoch 667/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.5608 - val_mse: 0.5608\n",
      "Epoch 668/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.5603 - val_mse: 0.5603\n",
      "Epoch 669/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.5598 - val_mse: 0.5598\n",
      "Epoch 670/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.5593 - val_mse: 0.5593\n",
      "Epoch 671/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.5588 - val_mse: 0.5588\n",
      "Epoch 672/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.5583 - val_mse: 0.5583\n",
      "Epoch 673/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.5579 - val_mse: 0.5579\n",
      "Epoch 674/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.5574 - val_mse: 0.5574\n",
      "Epoch 675/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.5569 - val_mse: 0.5569\n",
      "Epoch 676/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.5565 - val_mse: 0.5565\n",
      "Epoch 677/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.5560 - val_mse: 0.5560\n",
      "Epoch 678/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.5555 - val_mse: 0.5555\n",
      "Epoch 679/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.5551 - val_mse: 0.5551\n",
      "Epoch 680/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.5546 - val_mse: 0.5546\n",
      "Epoch 681/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.5542 - val_mse: 0.5542\n",
      "Epoch 682/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.5537 - val_mse: 0.5537\n",
      "Epoch 683/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.5533 - val_mse: 0.5533\n",
      "Epoch 684/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.5528 - val_mse: 0.5528\n",
      "Epoch 685/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.5524 - val_mse: 0.5524\n",
      "Epoch 686/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.5520 - val_mse: 0.5520\n",
      "Epoch 687/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.5515 - val_mse: 0.5515\n",
      "Epoch 688/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.5511 - val_mse: 0.5511\n",
      "Epoch 689/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.8095e-04 - mse: 9.8095e-04 - val_loss: 0.5507 - val_mse: 0.5507\n",
      "Epoch 690/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 9.6174e-04 - mse: 9.6174e-04 - val_loss: 0.5503 - val_mse: 0.5503\n",
      "Epoch 691/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.4285e-04 - mse: 9.4285e-04 - val_loss: 0.5498 - val_mse: 0.5498\n",
      "Epoch 692/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.2430e-04 - mse: 9.2430e-04 - val_loss: 0.5494 - val_mse: 0.5494\n",
      "Epoch 693/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.0606e-04 - mse: 9.0606e-04 - val_loss: 0.5490 - val_mse: 0.5490\n",
      "Epoch 694/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.8814e-04 - mse: 8.8814e-04 - val_loss: 0.5486 - val_mse: 0.5486\n",
      "Epoch 695/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.7054e-04 - mse: 8.7054e-04 - val_loss: 0.5482 - val_mse: 0.5482\n",
      "Epoch 696/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 8.5324e-04 - mse: 8.5324e-04 - val_loss: 0.5478 - val_mse: 0.5478\n",
      "Epoch 697/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.3625e-04 - mse: 8.3625e-04 - val_loss: 0.5474 - val_mse: 0.5474\n",
      "Epoch 698/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 8.1956e-04 - mse: 8.1956e-04 - val_loss: 0.5470 - val_mse: 0.5470\n",
      "Epoch 699/1000\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 8.0316e-04 - mse: 8.0316e-04 - val_loss: 0.5466 - val_mse: 0.5466\n",
      "Epoch 700/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.8706e-04 - mse: 7.8706e-04 - val_loss: 0.5462 - val_mse: 0.5462\n",
      "Epoch 701/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.7124e-04 - mse: 7.7124e-04 - val_loss: 0.5458 - val_mse: 0.5458\n",
      "Epoch 702/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.5570e-04 - mse: 7.5570e-04 - val_loss: 0.5454 - val_mse: 0.5454\n",
      "Epoch 703/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.4045e-04 - mse: 7.4045e-04 - val_loss: 0.5450 - val_mse: 0.5450\n",
      "Epoch 704/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.2547e-04 - mse: 7.2547e-04 - val_loss: 0.5446 - val_mse: 0.5446\n",
      "Epoch 705/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.1076e-04 - mse: 7.1076e-04 - val_loss: 0.5443 - val_mse: 0.5443\n",
      "Epoch 706/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.9631e-04 - mse: 6.9631e-04 - val_loss: 0.5439 - val_mse: 0.5439\n",
      "Epoch 707/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.8213e-04 - mse: 6.8213e-04 - val_loss: 0.5435 - val_mse: 0.5435\n",
      "Epoch 708/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.6821e-04 - mse: 6.6821e-04 - val_loss: 0.5432 - val_mse: 0.5432\n",
      "Epoch 709/1000\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 6.5454e-04 - mse: 6.5454e-04 - val_loss: 0.5428 - val_mse: 0.5428\n",
      "Epoch 710/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.4112e-04 - mse: 6.4112e-04 - val_loss: 0.5424 - val_mse: 0.5424\n",
      "Epoch 711/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.2796e-04 - mse: 6.2796e-04 - val_loss: 0.5421 - val_mse: 0.5421\n",
      "Epoch 712/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.1503e-04 - mse: 6.1503e-04 - val_loss: 0.5417 - val_mse: 0.5417\n",
      "Epoch 713/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.0235e-04 - mse: 6.0235e-04 - val_loss: 0.5413 - val_mse: 0.5413\n",
      "Epoch 714/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.8990e-04 - mse: 5.8990e-04 - val_loss: 0.5410 - val_mse: 0.5410\n",
      "Epoch 715/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.7768e-04 - mse: 5.7768e-04 - val_loss: 0.5406 - val_mse: 0.5406\n",
      "Epoch 716/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.6569e-04 - mse: 5.6569e-04 - val_loss: 0.5403 - val_mse: 0.5403\n",
      "Epoch 717/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.5393e-04 - mse: 5.5393e-04 - val_loss: 0.5399 - val_mse: 0.5399\n",
      "Epoch 718/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.4239e-04 - mse: 5.4239e-04 - val_loss: 0.5396 - val_mse: 0.5396\n",
      "Epoch 719/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.3107e-04 - mse: 5.3107e-04 - val_loss: 0.5393 - val_mse: 0.5393\n",
      "Epoch 720/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.1996e-04 - mse: 5.1996e-04 - val_loss: 0.5389 - val_mse: 0.5389\n",
      "Epoch 721/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.0906e-04 - mse: 5.0906e-04 - val_loss: 0.5386 - val_mse: 0.5386\n",
      "Epoch 722/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.9837e-04 - mse: 4.9837e-04 - val_loss: 0.5383 - val_mse: 0.5383\n",
      "Epoch 723/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.8789e-04 - mse: 4.8789e-04 - val_loss: 0.5379 - val_mse: 0.5379\n",
      "Epoch 724/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.7760e-04 - mse: 4.7760e-04 - val_loss: 0.5376 - val_mse: 0.5376\n",
      "Epoch 725/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.6751e-04 - mse: 4.6751e-04 - val_loss: 0.5373 - val_mse: 0.5373\n",
      "Epoch 726/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.5763e-04 - mse: 4.5763e-04 - val_loss: 0.5370 - val_mse: 0.5370\n",
      "Epoch 727/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.4792e-04 - mse: 4.4792e-04 - val_loss: 0.5367 - val_mse: 0.5367\n",
      "Epoch 728/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.3841e-04 - mse: 4.3841e-04 - val_loss: 0.5363 - val_mse: 0.5363\n",
      "Epoch 729/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.2908e-04 - mse: 4.2908e-04 - val_loss: 0.5360 - val_mse: 0.5360\n",
      "Epoch 730/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.1994e-04 - mse: 4.1994e-04 - val_loss: 0.5357 - val_mse: 0.5357\n",
      "Epoch 731/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.1097e-04 - mse: 4.1097e-04 - val_loss: 0.5354 - val_mse: 0.5354\n",
      "Epoch 732/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 4.0218e-04 - mse: 4.0218e-04 - val_loss: 0.5351 - val_mse: 0.5351\n",
      "Epoch 733/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 25ms/step - loss: 3.9357e-04 - mse: 3.9357e-04 - val_loss: 0.5348 - val_mse: 0.5348\n",
      "Epoch 734/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 3.8512e-04 - mse: 3.8512e-04 - val_loss: 0.5345 - val_mse: 0.5345\n",
      "Epoch 735/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.7684e-04 - mse: 3.7684e-04 - val_loss: 0.5342 - val_mse: 0.5342\n",
      "Epoch 736/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.6872e-04 - mse: 3.6872e-04 - val_loss: 0.5339 - val_mse: 0.5339\n",
      "Epoch 737/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.6077e-04 - mse: 3.6077e-04 - val_loss: 0.5336 - val_mse: 0.5336\n",
      "Epoch 738/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.5297e-04 - mse: 3.5297e-04 - val_loss: 0.5333 - val_mse: 0.5333\n",
      "Epoch 739/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.4533e-04 - mse: 3.4533e-04 - val_loss: 0.5331 - val_mse: 0.5331\n",
      "Epoch 740/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 3.3785e-04 - mse: 3.3785e-04 - val_loss: 0.5328 - val_mse: 0.5328\n",
      "Epoch 741/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.3051e-04 - mse: 3.3051e-04 - val_loss: 0.5325 - val_mse: 0.5325\n",
      "Epoch 742/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.2332e-04 - mse: 3.2332e-04 - val_loss: 0.5322 - val_mse: 0.5322\n",
      "Epoch 743/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 3.1628e-04 - mse: 3.1628e-04 - val_loss: 0.5319 - val_mse: 0.5319\n",
      "Epoch 744/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.0938e-04 - mse: 3.0938e-04 - val_loss: 0.5317 - val_mse: 0.5317\n",
      "Epoch 745/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.0262e-04 - mse: 3.0262e-04 - val_loss: 0.5314 - val_mse: 0.5314\n",
      "Epoch 746/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2.9599e-04 - mse: 2.9599e-04 - val_loss: 0.5311 - val_mse: 0.5311\n",
      "Epoch 747/1000\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 2.8950e-04 - mse: 2.8950e-04 - val_loss: 0.5308 - val_mse: 0.5308\n",
      "Epoch 748/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2.8315e-04 - mse: 2.8315e-04 - val_loss: 0.5306 - val_mse: 0.5306\n",
      "Epoch 749/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.7692e-04 - mse: 2.7692e-04 - val_loss: 0.5303 - val_mse: 0.5303\n",
      "Epoch 750/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2.7082e-04 - mse: 2.7082e-04 - val_loss: 0.5301 - val_mse: 0.5301\n",
      "Epoch 751/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2.6485e-04 - mse: 2.6485e-04 - val_loss: 0.5298 - val_mse: 0.5298\n",
      "Epoch 752/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2.5900e-04 - mse: 2.5900e-04 - val_loss: 0.5296 - val_mse: 0.5296\n",
      "Epoch 753/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 2.5327e-04 - mse: 2.5327e-04 - val_loss: 0.5293 - val_mse: 0.5293\n",
      "Epoch 754/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2.4767e-04 - mse: 2.4767e-04 - val_loss: 0.5290 - val_mse: 0.5290\n",
      "Epoch 755/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2.4217e-04 - mse: 2.4217e-04 - val_loss: 0.5288 - val_mse: 0.5288\n",
      "Epoch 756/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2.3679e-04 - mse: 2.3679e-04 - val_loss: 0.5286 - val_mse: 0.5286\n",
      "Epoch 757/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2.3153e-04 - mse: 2.3153e-04 - val_loss: 0.5283 - val_mse: 0.5283\n",
      "Epoch 758/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2.2637e-04 - mse: 2.2637e-04 - val_loss: 0.5281 - val_mse: 0.5281\n",
      "Epoch 759/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2.2132e-04 - mse: 2.2132e-04 - val_loss: 0.5278 - val_mse: 0.5278\n",
      "Epoch 760/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2.1638e-04 - mse: 2.1638e-04 - val_loss: 0.5276 - val_mse: 0.5276\n",
      "Epoch 761/1000\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 2.1154e-04 - mse: 2.1154e-04 - val_loss: 0.5274 - val_mse: 0.5274\n",
      "Epoch 762/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2.0681e-04 - mse: 2.0681e-04 - val_loss: 0.5271 - val_mse: 0.5271\n",
      "Epoch 763/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2.0217e-04 - mse: 2.0217e-04 - val_loss: 0.5269 - val_mse: 0.5269\n",
      "Epoch 764/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.9763e-04 - mse: 1.9763e-04 - val_loss: 0.5267 - val_mse: 0.5267\n",
      "Epoch 765/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1.9319e-04 - mse: 1.9319e-04 - val_loss: 0.5264 - val_mse: 0.5264\n",
      "Epoch 766/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1.8884e-04 - mse: 1.8884e-04 - val_loss: 0.5262 - val_mse: 0.5262\n",
      "Epoch 767/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 1.8459e-04 - mse: 1.8459e-04 - val_loss: 0.5260 - val_mse: 0.5260\n",
      "Epoch 768/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 1.8042e-04 - mse: 1.8042e-04 - val_loss: 0.5258 - val_mse: 0.5258\n",
      "Epoch 769/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.7635e-04 - mse: 1.7635e-04 - val_loss: 0.5256 - val_mse: 0.5256\n",
      "Epoch 770/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.7236e-04 - mse: 1.7236e-04 - val_loss: 0.5253 - val_mse: 0.5253\n",
      "Epoch 771/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.6846e-04 - mse: 1.6846e-04 - val_loss: 0.5251 - val_mse: 0.5251\n",
      "Epoch 772/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 1.6464e-04 - mse: 1.6464e-04 - val_loss: 0.5249 - val_mse: 0.5249\n",
      "Epoch 773/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 1.6090e-04 - mse: 1.6090e-04 - val_loss: 0.5247 - val_mse: 0.5247\n",
      "Epoch 774/1000\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 1.5725e-04 - mse: 1.5725e-04 - val_loss: 0.5245 - val_mse: 0.5245\n",
      "Epoch 775/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 1.5367e-04 - mse: 1.5367e-04 - val_loss: 0.5243 - val_mse: 0.5243\n",
      "Epoch 776/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1.5017e-04 - mse: 1.5017e-04 - val_loss: 0.5241 - val_mse: 0.5241\n",
      "Epoch 777/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 1.4675e-04 - mse: 1.4675e-04 - val_loss: 0.5239 - val_mse: 0.5239\n",
      "Epoch 778/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 1.4340e-04 - mse: 1.4340e-04 - val_loss: 0.5237 - val_mse: 0.5237\n",
      "Epoch 779/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1.4012e-04 - mse: 1.4012e-04 - val_loss: 0.5235 - val_mse: 0.5235\n",
      "Epoch 780/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.3692e-04 - mse: 1.3692e-04 - val_loss: 0.5233 - val_mse: 0.5233\n",
      "Epoch 781/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 1.3379e-04 - mse: 1.3379e-04 - val_loss: 0.5231 - val_mse: 0.5231\n",
      "Epoch 782/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1.3072e-04 - mse: 1.3072e-04 - val_loss: 0.5229 - val_mse: 0.5229\n",
      "Epoch 783/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.2772e-04 - mse: 1.2772e-04 - val_loss: 0.5227 - val_mse: 0.5227\n",
      "Epoch 784/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.2479e-04 - mse: 1.2479e-04 - val_loss: 0.5225 - val_mse: 0.5225\n",
      "Epoch 785/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.2192e-04 - mse: 1.2192e-04 - val_loss: 0.5224 - val_mse: 0.5224\n",
      "Epoch 786/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.1911e-04 - mse: 1.1911e-04 - val_loss: 0.5222 - val_mse: 0.5222\n",
      "Epoch 787/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1.1637e-04 - mse: 1.1637e-04 - val_loss: 0.5220 - val_mse: 0.5220\n",
      "Epoch 788/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.1369e-04 - mse: 1.1369e-04 - val_loss: 0.5218 - val_mse: 0.5218\n",
      "Epoch 789/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1.1106e-04 - mse: 1.1106e-04 - val_loss: 0.5216 - val_mse: 0.5216\n",
      "Epoch 790/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.0850e-04 - mse: 1.0850e-04 - val_loss: 0.5215 - val_mse: 0.5215\n",
      "Epoch 791/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1.0599e-04 - mse: 1.0599e-04 - val_loss: 0.5213 - val_mse: 0.5213\n",
      "Epoch 792/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.0354e-04 - mse: 1.0354e-04 - val_loss: 0.5211 - val_mse: 0.5211\n",
      "Epoch 793/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.0114e-04 - mse: 1.0114e-04 - val_loss: 0.5209 - val_mse: 0.5209\n",
      "Epoch 794/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.8791e-05 - mse: 9.8791e-05 - val_loss: 0.5208 - val_mse: 0.5208\n",
      "Epoch 795/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.6498e-05 - mse: 9.6498e-05 - val_loss: 0.5206 - val_mse: 0.5206\n",
      "Epoch 796/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.4257e-05 - mse: 9.4257e-05 - val_loss: 0.5204 - val_mse: 0.5204\n",
      "Epoch 797/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.2065e-05 - mse: 9.2065e-05 - val_loss: 0.5203 - val_mse: 0.5203\n",
      "Epoch 798/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.9923e-05 - mse: 8.9923e-05 - val_loss: 0.5201 - val_mse: 0.5201\n",
      "Epoch 799/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.7827e-05 - mse: 8.7827e-05 - val_loss: 0.5199 - val_mse: 0.5199\n",
      "Epoch 800/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.5780e-05 - mse: 8.5780e-05 - val_loss: 0.5198 - val_mse: 0.5198\n",
      "Epoch 801/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.3779e-05 - mse: 8.3779e-05 - val_loss: 0.5196 - val_mse: 0.5196\n",
      "Epoch 802/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.1822e-05 - mse: 8.1822e-05 - val_loss: 0.5195 - val_mse: 0.5195\n",
      "Epoch 803/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.9909e-05 - mse: 7.9909e-05 - val_loss: 0.5193 - val_mse: 0.5193\n",
      "Epoch 804/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.8040e-05 - mse: 7.8040e-05 - val_loss: 0.5192 - val_mse: 0.5192\n",
      "Epoch 805/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.6213e-05 - mse: 7.6213e-05 - val_loss: 0.5190 - val_mse: 0.5190\n",
      "Epoch 806/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.4427e-05 - mse: 7.4427e-05 - val_loss: 0.5189 - val_mse: 0.5189\n",
      "Epoch 807/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.2681e-05 - mse: 7.2681e-05 - val_loss: 0.5187 - val_mse: 0.5187\n",
      "Epoch 808/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.0975e-05 - mse: 7.0975e-05 - val_loss: 0.5186 - val_mse: 0.5186\n",
      "Epoch 809/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.9307e-05 - mse: 6.9307e-05 - val_loss: 0.5184 - val_mse: 0.5184\n",
      "Epoch 810/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.7678e-05 - mse: 6.7678e-05 - val_loss: 0.5183 - val_mse: 0.5183\n",
      "Epoch 811/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.6085e-05 - mse: 6.6085e-05 - val_loss: 0.5181 - val_mse: 0.5181\n",
      "Epoch 812/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.4529e-05 - mse: 6.4529e-05 - val_loss: 0.5180 - val_mse: 0.5180\n",
      "Epoch 813/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.3008e-05 - mse: 6.3008e-05 - val_loss: 0.5179 - val_mse: 0.5179\n",
      "Epoch 814/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.1523e-05 - mse: 6.1523e-05 - val_loss: 0.5177 - val_mse: 0.5177\n",
      "Epoch 815/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.0070e-05 - mse: 6.0070e-05 - val_loss: 0.5176 - val_mse: 0.5176\n",
      "Epoch 816/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.8651e-05 - mse: 5.8651e-05 - val_loss: 0.5175 - val_mse: 0.5175\n",
      "Epoch 817/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.7264e-05 - mse: 5.7264e-05 - val_loss: 0.5173 - val_mse: 0.5173\n",
      "Epoch 818/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.5909e-05 - mse: 5.5909e-05 - val_loss: 0.5172 - val_mse: 0.5172\n",
      "Epoch 819/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.4585e-05 - mse: 5.4585e-05 - val_loss: 0.5171 - val_mse: 0.5171\n",
      "Epoch 820/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.3292e-05 - mse: 5.3292e-05 - val_loss: 0.5169 - val_mse: 0.5169\n",
      "Epoch 821/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.2027e-05 - mse: 5.2027e-05 - val_loss: 0.5168 - val_mse: 0.5168\n",
      "Epoch 822/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.0792e-05 - mse: 5.0792e-05 - val_loss: 0.5167 - val_mse: 0.5167\n",
      "Epoch 823/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.9586e-05 - mse: 4.9586e-05 - val_loss: 0.5166 - val_mse: 0.5166\n",
      "Epoch 824/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.8408e-05 - mse: 4.8408e-05 - val_loss: 0.5164 - val_mse: 0.5164\n",
      "Epoch 825/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.7255e-05 - mse: 4.7255e-05 - val_loss: 0.5163 - val_mse: 0.5163\n",
      "Epoch 826/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.6130e-05 - mse: 4.6130e-05 - val_loss: 0.5162 - val_mse: 0.5162\n",
      "Epoch 827/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.5030e-05 - mse: 4.5030e-05 - val_loss: 0.5161 - val_mse: 0.5161\n",
      "Epoch 828/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.3957e-05 - mse: 4.3957e-05 - val_loss: 0.5160 - val_mse: 0.5160\n",
      "Epoch 829/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.2908e-05 - mse: 4.2908e-05 - val_loss: 0.5158 - val_mse: 0.5158\n",
      "Epoch 830/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.1883e-05 - mse: 4.1883e-05 - val_loss: 0.5157 - val_mse: 0.5157\n",
      "Epoch 831/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.0882e-05 - mse: 4.0882e-05 - val_loss: 0.5156 - val_mse: 0.5156\n",
      "Epoch 832/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.9904e-05 - mse: 3.9904e-05 - val_loss: 0.5155 - val_mse: 0.5155\n",
      "Epoch 833/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.8949e-05 - mse: 3.8949e-05 - val_loss: 0.5154 - val_mse: 0.5154\n",
      "Epoch 834/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.8016e-05 - mse: 3.8016e-05 - val_loss: 0.5153 - val_mse: 0.5153\n",
      "Epoch 835/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.7104e-05 - mse: 3.7104e-05 - val_loss: 0.5152 - val_mse: 0.5152\n",
      "Epoch 836/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.6213e-05 - mse: 3.6213e-05 - val_loss: 0.5151 - val_mse: 0.5151\n",
      "Epoch 837/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.5344e-05 - mse: 3.5344e-05 - val_loss: 0.5150 - val_mse: 0.5150\n",
      "Epoch 838/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.4494e-05 - mse: 3.4494e-05 - val_loss: 0.5149 - val_mse: 0.5149\n",
      "Epoch 839/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.3665e-05 - mse: 3.3665e-05 - val_loss: 0.5148 - val_mse: 0.5148\n",
      "Epoch 840/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.2855e-05 - mse: 3.2855e-05 - val_loss: 0.5146 - val_mse: 0.5146\n",
      "Epoch 841/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.2063e-05 - mse: 3.2063e-05 - val_loss: 0.5145 - val_mse: 0.5145\n",
      "Epoch 842/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.1290e-05 - mse: 3.1290e-05 - val_loss: 0.5144 - val_mse: 0.5144\n",
      "Epoch 843/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.0535e-05 - mse: 3.0535e-05 - val_loss: 0.5143 - val_mse: 0.5143\n",
      "Epoch 844/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.9798e-05 - mse: 2.9798e-05 - val_loss: 0.5143 - val_mse: 0.5143\n",
      "Epoch 845/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2.9077e-05 - mse: 2.9077e-05 - val_loss: 0.5142 - val_mse: 0.5142\n",
      "Epoch 846/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.8373e-05 - mse: 2.8373e-05 - val_loss: 0.5141 - val_mse: 0.5141\n",
      "Epoch 847/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2.7687e-05 - mse: 2.7687e-05 - val_loss: 0.5140 - val_mse: 0.5140\n",
      "Epoch 848/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.7015e-05 - mse: 2.7015e-05 - val_loss: 0.5139 - val_mse: 0.5139\n",
      "Epoch 849/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 30ms/step - loss: 2.6361e-05 - mse: 2.6361e-05 - val_loss: 0.5138 - val_mse: 0.5138\n",
      "Epoch 850/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.5721e-05 - mse: 2.5721e-05 - val_loss: 0.5137 - val_mse: 0.5137\n",
      "Epoch 851/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.5096e-05 - mse: 2.5096e-05 - val_loss: 0.5136 - val_mse: 0.5136\n",
      "Epoch 852/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.4486e-05 - mse: 2.4486e-05 - val_loss: 0.5135 - val_mse: 0.5135\n",
      "Epoch 853/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2.3890e-05 - mse: 2.3890e-05 - val_loss: 0.5134 - val_mse: 0.5134\n",
      "Epoch 854/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 2.3308e-05 - mse: 2.3308e-05 - val_loss: 0.5133 - val_mse: 0.5133\n",
      "Epoch 855/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2.2740e-05 - mse: 2.2740e-05 - val_loss: 0.5132 - val_mse: 0.5132\n",
      "Epoch 856/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.2186e-05 - mse: 2.2186e-05 - val_loss: 0.5132 - val_mse: 0.5132\n",
      "Epoch 857/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2.1644e-05 - mse: 2.1644e-05 - val_loss: 0.5131 - val_mse: 0.5131\n",
      "Epoch 858/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.1115e-05 - mse: 2.1115e-05 - val_loss: 0.5130 - val_mse: 0.5130\n",
      "Epoch 859/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.0598e-05 - mse: 2.0598e-05 - val_loss: 0.5129 - val_mse: 0.5129\n",
      "Epoch 860/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.0094e-05 - mse: 2.0094e-05 - val_loss: 0.5128 - val_mse: 0.5128\n",
      "Epoch 861/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.9601e-05 - mse: 1.9601e-05 - val_loss: 0.5128 - val_mse: 0.5128\n",
      "Epoch 862/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.9121e-05 - mse: 1.9121e-05 - val_loss: 0.5127 - val_mse: 0.5127\n",
      "Epoch 863/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.8651e-05 - mse: 1.8651e-05 - val_loss: 0.5126 - val_mse: 0.5126\n",
      "Epoch 864/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.8193e-05 - mse: 1.8193e-05 - val_loss: 0.5125 - val_mse: 0.5125\n",
      "Epoch 865/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.7745e-05 - mse: 1.7745e-05 - val_loss: 0.5124 - val_mse: 0.5124\n",
      "Epoch 866/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.7308e-05 - mse: 1.7308e-05 - val_loss: 0.5124 - val_mse: 0.5124\n",
      "Epoch 867/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.6881e-05 - mse: 1.6881e-05 - val_loss: 0.5123 - val_mse: 0.5123\n",
      "Epoch 868/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.6465e-05 - mse: 1.6465e-05 - val_loss: 0.5122 - val_mse: 0.5122\n",
      "Epoch 869/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.6058e-05 - mse: 1.6058e-05 - val_loss: 0.5121 - val_mse: 0.5121\n",
      "Epoch 870/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.5661e-05 - mse: 1.5661e-05 - val_loss: 0.5121 - val_mse: 0.5121\n",
      "Epoch 871/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.5274e-05 - mse: 1.5274e-05 - val_loss: 0.5120 - val_mse: 0.5120\n",
      "Epoch 872/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.4895e-05 - mse: 1.4895e-05 - val_loss: 0.5119 - val_mse: 0.5119\n",
      "Epoch 873/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.4526e-05 - mse: 1.4526e-05 - val_loss: 0.5119 - val_mse: 0.5119\n",
      "Epoch 874/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.4166e-05 - mse: 1.4166e-05 - val_loss: 0.5118 - val_mse: 0.5118\n",
      "Epoch 875/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.3813e-05 - mse: 1.3813e-05 - val_loss: 0.5117 - val_mse: 0.5117\n",
      "Epoch 876/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.3470e-05 - mse: 1.3470e-05 - val_loss: 0.5117 - val_mse: 0.5117\n",
      "Epoch 877/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1.3134e-05 - mse: 1.3134e-05 - val_loss: 0.5116 - val_mse: 0.5116\n",
      "Epoch 878/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1.2807e-05 - mse: 1.2807e-05 - val_loss: 0.5115 - val_mse: 0.5115\n",
      "Epoch 879/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.2487e-05 - mse: 1.2487e-05 - val_loss: 0.5115 - val_mse: 0.5115\n",
      "Epoch 880/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.2175e-05 - mse: 1.2175e-05 - val_loss: 0.5114 - val_mse: 0.5114\n",
      "Epoch 881/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.1871e-05 - mse: 1.1871e-05 - val_loss: 0.5113 - val_mse: 0.5113\n",
      "Epoch 882/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.1574e-05 - mse: 1.1574e-05 - val_loss: 0.5113 - val_mse: 0.5113\n",
      "Epoch 883/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.1283e-05 - mse: 1.1283e-05 - val_loss: 0.5112 - val_mse: 0.5112\n",
      "Epoch 884/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.1000e-05 - mse: 1.1000e-05 - val_loss: 0.5112 - val_mse: 0.5112\n",
      "Epoch 885/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.0724e-05 - mse: 1.0724e-05 - val_loss: 0.5111 - val_mse: 0.5111\n",
      "Epoch 886/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.0454e-05 - mse: 1.0454e-05 - val_loss: 0.5110 - val_mse: 0.5110\n",
      "Epoch 887/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.0191e-05 - mse: 1.0191e-05 - val_loss: 0.5110 - val_mse: 0.5110\n",
      "Epoch 888/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.9336e-06 - mse: 9.9336e-06 - val_loss: 0.5109 - val_mse: 0.5109\n",
      "Epoch 889/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.6827e-06 - mse: 9.6827e-06 - val_loss: 0.5109 - val_mse: 0.5109\n",
      "Epoch 890/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.4381e-06 - mse: 9.4381e-06 - val_loss: 0.5108 - val_mse: 0.5108\n",
      "Epoch 891/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.1993e-06 - mse: 9.1993e-06 - val_loss: 0.5108 - val_mse: 0.5108\n",
      "Epoch 892/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.9660e-06 - mse: 8.9660e-06 - val_loss: 0.5107 - val_mse: 0.5107\n",
      "Epoch 893/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.7385e-06 - mse: 8.7385e-06 - val_loss: 0.5106 - val_mse: 0.5106\n",
      "Epoch 894/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.5165e-06 - mse: 8.5165e-06 - val_loss: 0.5106 - val_mse: 0.5106\n",
      "Epoch 895/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.2997e-06 - mse: 8.2997e-06 - val_loss: 0.5105 - val_mse: 0.5105\n",
      "Epoch 896/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.0881e-06 - mse: 8.0881e-06 - val_loss: 0.5105 - val_mse: 0.5105\n",
      "Epoch 897/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.8821e-06 - mse: 7.8821e-06 - val_loss: 0.5104 - val_mse: 0.5104\n",
      "Epoch 898/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.6806e-06 - mse: 7.6806e-06 - val_loss: 0.5104 - val_mse: 0.5104\n",
      "Epoch 899/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.4841e-06 - mse: 7.4841e-06 - val_loss: 0.5103 - val_mse: 0.5103\n",
      "Epoch 900/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.2922e-06 - mse: 7.2922e-06 - val_loss: 0.5103 - val_mse: 0.5103\n",
      "Epoch 901/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.1055e-06 - mse: 7.1055e-06 - val_loss: 0.5102 - val_mse: 0.5102\n",
      "Epoch 902/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.9231e-06 - mse: 6.9231e-06 - val_loss: 0.5102 - val_mse: 0.5102\n",
      "Epoch 903/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.7449e-06 - mse: 6.7449e-06 - val_loss: 0.5101 - val_mse: 0.5101\n",
      "Epoch 904/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.5713e-06 - mse: 6.5713e-06 - val_loss: 0.5101 - val_mse: 0.5101\n",
      "Epoch 905/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.4016e-06 - mse: 6.4016e-06 - val_loss: 0.5100 - val_mse: 0.5100\n",
      "Epoch 906/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.2365e-06 - mse: 6.2365e-06 - val_loss: 0.5100 - val_mse: 0.5100\n",
      "Epoch 907/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.0751e-06 - mse: 6.0751e-06 - val_loss: 0.5100 - val_mse: 0.5100\n",
      "Epoch 908/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.9178e-06 - mse: 5.9178e-06 - val_loss: 0.5099 - val_mse: 0.5099\n",
      "Epoch 909/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.7641e-06 - mse: 5.7641e-06 - val_loss: 0.5099 - val_mse: 0.5099\n",
      "Epoch 910/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.6142e-06 - mse: 5.6142e-06 - val_loss: 0.5098 - val_mse: 0.5098\n",
      "Epoch 911/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.4682e-06 - mse: 5.4682e-06 - val_loss: 0.5098 - val_mse: 0.5098\n",
      "Epoch 912/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.3259e-06 - mse: 5.3259e-06 - val_loss: 0.5097 - val_mse: 0.5097\n",
      "Epoch 913/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.1868e-06 - mse: 5.1868e-06 - val_loss: 0.5097 - val_mse: 0.5097\n",
      "Epoch 914/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.0512e-06 - mse: 5.0512e-06 - val_loss: 0.5097 - val_mse: 0.5097\n",
      "Epoch 915/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.9191e-06 - mse: 4.9191e-06 - val_loss: 0.5096 - val_mse: 0.5096\n",
      "Epoch 916/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.7899e-06 - mse: 4.7899e-06 - val_loss: 0.5096 - val_mse: 0.5096\n",
      "Epoch 917/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.6642e-06 - mse: 4.6642e-06 - val_loss: 0.5095 - val_mse: 0.5095\n",
      "Epoch 918/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.5416e-06 - mse: 4.5416e-06 - val_loss: 0.5095 - val_mse: 0.5095\n",
      "Epoch 919/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.4220e-06 - mse: 4.4220e-06 - val_loss: 0.5095 - val_mse: 0.5095\n",
      "Epoch 920/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.3053e-06 - mse: 4.3053e-06 - val_loss: 0.5094 - val_mse: 0.5094\n",
      "Epoch 921/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.1913e-06 - mse: 4.1913e-06 - val_loss: 0.5094 - val_mse: 0.5094\n",
      "Epoch 922/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.0803e-06 - mse: 4.0803e-06 - val_loss: 0.5093 - val_mse: 0.5093\n",
      "Epoch 923/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.9722e-06 - mse: 3.9722e-06 - val_loss: 0.5093 - val_mse: 0.5093\n",
      "Epoch 924/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.8667e-06 - mse: 3.8667e-06 - val_loss: 0.5093 - val_mse: 0.5093\n",
      "Epoch 925/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.7639e-06 - mse: 3.7639e-06 - val_loss: 0.5092 - val_mse: 0.5092\n",
      "Epoch 926/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.6635e-06 - mse: 3.6635e-06 - val_loss: 0.5092 - val_mse: 0.5092\n",
      "Epoch 927/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.5657e-06 - mse: 3.5657e-06 - val_loss: 0.5092 - val_mse: 0.5092\n",
      "Epoch 928/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.4704e-06 - mse: 3.4704e-06 - val_loss: 0.5091 - val_mse: 0.5091\n",
      "Epoch 929/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.3774e-06 - mse: 3.3774e-06 - val_loss: 0.5091 - val_mse: 0.5091\n",
      "Epoch 930/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.2868e-06 - mse: 3.2868e-06 - val_loss: 0.5091 - val_mse: 0.5091\n",
      "Epoch 931/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.1984e-06 - mse: 3.1984e-06 - val_loss: 0.5090 - val_mse: 0.5090\n",
      "Epoch 932/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.1122e-06 - mse: 3.1122e-06 - val_loss: 0.5090 - val_mse: 0.5090\n",
      "Epoch 933/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.0284e-06 - mse: 3.0284e-06 - val_loss: 0.5090 - val_mse: 0.5090\n",
      "Epoch 934/1000\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 2.9464e-06 - mse: 2.9464e-06 - val_loss: 0.5089 - val_mse: 0.5089\n",
      "Epoch 935/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.8668e-06 - mse: 2.8668e-06 - val_loss: 0.5089 - val_mse: 0.5089\n",
      "Epoch 936/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.7889e-06 - mse: 2.7889e-06 - val_loss: 0.5089 - val_mse: 0.5089\n",
      "Epoch 937/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.7132e-06 - mse: 2.7132e-06 - val_loss: 0.5088 - val_mse: 0.5088\n",
      "Epoch 938/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2.6393e-06 - mse: 2.6393e-06 - val_loss: 0.5088 - val_mse: 0.5088\n",
      "Epoch 939/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.5673e-06 - mse: 2.5673e-06 - val_loss: 0.5088 - val_mse: 0.5088\n",
      "Epoch 940/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2.4972e-06 - mse: 2.4972e-06 - val_loss: 0.5088 - val_mse: 0.5088\n",
      "Epoch 941/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.4288e-06 - mse: 2.4288e-06 - val_loss: 0.5087 - val_mse: 0.5087\n",
      "Epoch 942/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.3621e-06 - mse: 2.3621e-06 - val_loss: 0.5087 - val_mse: 0.5087\n",
      "Epoch 943/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.2972e-06 - mse: 2.2972e-06 - val_loss: 0.5087 - val_mse: 0.5087\n",
      "Epoch 944/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2.2340e-06 - mse: 2.2340e-06 - val_loss: 0.5086 - val_mse: 0.5086\n",
      "Epoch 945/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 2.1724e-06 - mse: 2.1724e-06 - val_loss: 0.5086 - val_mse: 0.5086\n",
      "Epoch 946/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2.1124e-06 - mse: 2.1124e-06 - val_loss: 0.5086 - val_mse: 0.5086\n",
      "Epoch 947/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.0540e-06 - mse: 2.0540e-06 - val_loss: 0.5086 - val_mse: 0.5086\n",
      "Epoch 948/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.9969e-06 - mse: 1.9969e-06 - val_loss: 0.5085 - val_mse: 0.5085\n",
      "Epoch 949/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.9414e-06 - mse: 1.9414e-06 - val_loss: 0.5085 - val_mse: 0.5085\n",
      "Epoch 950/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.8874e-06 - mse: 1.8874e-06 - val_loss: 0.5085 - val_mse: 0.5085\n",
      "Epoch 951/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.8347e-06 - mse: 1.8347e-06 - val_loss: 0.5085 - val_mse: 0.5085\n",
      "Epoch 952/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.7835e-06 - mse: 1.7835e-06 - val_loss: 0.5084 - val_mse: 0.5084\n",
      "Epoch 953/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.7334e-06 - mse: 1.7334e-06 - val_loss: 0.5084 - val_mse: 0.5084\n",
      "Epoch 954/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.6848e-06 - mse: 1.6848e-06 - val_loss: 0.5084 - val_mse: 0.5084\n",
      "Epoch 955/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.6373e-06 - mse: 1.6373e-06 - val_loss: 0.5084 - val_mse: 0.5084\n",
      "Epoch 956/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.5912e-06 - mse: 1.5912e-06 - val_loss: 0.5083 - val_mse: 0.5083\n",
      "Epoch 957/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.5463e-06 - mse: 1.5463e-06 - val_loss: 0.5083 - val_mse: 0.5083\n",
      "Epoch 958/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.5026e-06 - mse: 1.5026e-06 - val_loss: 0.5083 - val_mse: 0.5083\n",
      "Epoch 959/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.4600e-06 - mse: 1.4600e-06 - val_loss: 0.5083 - val_mse: 0.5083\n",
      "Epoch 960/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.4185e-06 - mse: 1.4185e-06 - val_loss: 0.5083 - val_mse: 0.5083\n",
      "Epoch 961/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.3781e-06 - mse: 1.3781e-06 - val_loss: 0.5082 - val_mse: 0.5082\n",
      "Epoch 962/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.3389e-06 - mse: 1.3389e-06 - val_loss: 0.5082 - val_mse: 0.5082\n",
      "Epoch 963/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.3004e-06 - mse: 1.3004e-06 - val_loss: 0.5082 - val_mse: 0.5082\n",
      "Epoch 964/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.2632e-06 - mse: 1.2632e-06 - val_loss: 0.5082 - val_mse: 0.5082\n",
      "Epoch 965/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 19ms/step - loss: 1.2269e-06 - mse: 1.2269e-06 - val_loss: 0.5082 - val_mse: 0.5082\n",
      "Epoch 966/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.1916e-06 - mse: 1.1916e-06 - val_loss: 0.5081 - val_mse: 0.5081\n",
      "Epoch 967/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.1573e-06 - mse: 1.1573e-06 - val_loss: 0.5081 - val_mse: 0.5081\n",
      "Epoch 968/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1.1239e-06 - mse: 1.1239e-06 - val_loss: 0.5081 - val_mse: 0.5081\n",
      "Epoch 969/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.0913e-06 - mse: 1.0913e-06 - val_loss: 0.5081 - val_mse: 0.5081\n",
      "Epoch 970/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.0597e-06 - mse: 1.0597e-06 - val_loss: 0.5081 - val_mse: 0.5081\n",
      "Epoch 971/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.0288e-06 - mse: 1.0288e-06 - val_loss: 0.5080 - val_mse: 0.5080\n",
      "Epoch 972/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.9883e-07 - mse: 9.9883e-07 - val_loss: 0.5080 - val_mse: 0.5080\n",
      "Epoch 973/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.6981e-07 - mse: 9.6981e-07 - val_loss: 0.5080 - val_mse: 0.5080\n",
      "Epoch 974/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.4121e-07 - mse: 9.4121e-07 - val_loss: 0.5080 - val_mse: 0.5080\n",
      "Epoch 975/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.1365e-07 - mse: 9.1365e-07 - val_loss: 0.5080 - val_mse: 0.5080\n",
      "Epoch 976/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.8686e-07 - mse: 8.8686e-07 - val_loss: 0.5079 - val_mse: 0.5079\n",
      "Epoch 977/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.6077e-07 - mse: 8.6077e-07 - val_loss: 0.5079 - val_mse: 0.5079\n",
      "Epoch 978/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.3533e-07 - mse: 8.3533e-07 - val_loss: 0.5079 - val_mse: 0.5079\n",
      "Epoch 979/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.1061e-07 - mse: 8.1061e-07 - val_loss: 0.5079 - val_mse: 0.5079\n",
      "Epoch 980/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.8655e-07 - mse: 7.8655e-07 - val_loss: 0.5079 - val_mse: 0.5079\n",
      "Epoch 981/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.6319e-07 - mse: 7.6319e-07 - val_loss: 0.5079 - val_mse: 0.5079\n",
      "Epoch 982/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.4048e-07 - mse: 7.4048e-07 - val_loss: 0.5078 - val_mse: 0.5078\n",
      "Epoch 983/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.1830e-07 - mse: 7.1830e-07 - val_loss: 0.5078 - val_mse: 0.5078\n",
      "Epoch 984/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.9681e-07 - mse: 6.9681e-07 - val_loss: 0.5078 - val_mse: 0.5078\n",
      "Epoch 985/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.7591e-07 - mse: 6.7591e-07 - val_loss: 0.5078 - val_mse: 0.5078\n",
      "Epoch 986/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.5563e-07 - mse: 6.5563e-07 - val_loss: 0.5078 - val_mse: 0.5078\n",
      "Epoch 987/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.3592e-07 - mse: 6.3592e-07 - val_loss: 0.5078 - val_mse: 0.5078\n",
      "Epoch 988/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.1668e-07 - mse: 6.1668e-07 - val_loss: 0.5078 - val_mse: 0.5078\n",
      "Epoch 989/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.9807e-07 - mse: 5.9807e-07 - val_loss: 0.5077 - val_mse: 0.5077\n",
      "Epoch 990/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.7988e-07 - mse: 5.7988e-07 - val_loss: 0.5077 - val_mse: 0.5077\n",
      "Epoch 991/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.6226e-07 - mse: 5.6226e-07 - val_loss: 0.5077 - val_mse: 0.5077\n",
      "Epoch 992/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.4517e-07 - mse: 5.4517e-07 - val_loss: 0.5077 - val_mse: 0.5077\n",
      "Epoch 993/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.2848e-07 - mse: 5.2848e-07 - val_loss: 0.5077 - val_mse: 0.5077\n",
      "Epoch 994/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.1230e-07 - mse: 5.1230e-07 - val_loss: 0.5077 - val_mse: 0.5077\n",
      "Epoch 995/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.9657e-07 - mse: 4.9657e-07 - val_loss: 0.5077 - val_mse: 0.5077\n",
      "Epoch 996/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.8130e-07 - mse: 4.8130e-07 - val_loss: 0.5076 - val_mse: 0.5076\n",
      "Epoch 997/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.6646e-07 - mse: 4.6646e-07 - val_loss: 0.5076 - val_mse: 0.5076\n",
      "Epoch 998/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.5203e-07 - mse: 4.5203e-07 - val_loss: 0.5076 - val_mse: 0.5076\n",
      "Epoch 999/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.3804e-07 - mse: 4.3804e-07 - val_loss: 0.5076 - val_mse: 0.5076\n",
      "Epoch 1000/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.2447e-07 - mse: 4.2447e-07 - val_loss: 0.5076 - val_mse: 0.5076\n",
      "mkdir: png: File exists\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9616282440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9616282440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9616282440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAScAAAEdCAYAAABOu6GKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAATeklEQVR4nO3dXYxc5X3H8d8/kFC1CC9buGoxZkuu6oCx1xdWSbwLvozDS4hz0UIgSGuhEAxqlRUXtnmTkC0l5iUS8orWOO5N7cS18aU33cUgceH1Cy63XTu0d0Z4l1QVpbL+vXiescfj2Xk5c2bPc+Z8P9JoPOec3fkns/PjPM95zvOYuwsAUvONogsAgGYIJwBJIpwAJIlwApAkwglAkggnAEkinAAk6caiC8jKzB6TNCxpnaQRSXvd/bfFVgUgL6UMpxhMp919Pr4eknTKzEbcfXehxQHIRVmbdcO1YJIkd1+QNClpV2EVAchV6cLJzEYk7TWztQ27Tsf9jdub/Y6JftSG/uOzK7duPr/ShVM8Y9otab5h11B8btzezMD+gZvZ5qJr6LOB/ewkPr96pQsnSXL3ydiUq/djhX6oxu1VM+h/3IOOzy+yQZiVIHaIn5e0rr4vquGYCcXUvummm9atXr16+QpcRouLi1qxYkXRZfTNxYsXdfvttxddRt8M+ud36tSp/5X0ad2mKXefanbsoITTcUm73H26k+NHR0d9bm6uz1UBaGRmp9x9tJNjS9msq2dmu9RFMAEoh1KHU2yqHSeYgMFT2nCKAzHn64PJzNbGoQYASq6U4WRmmxRuXZk3s5H4WCtp61Id4gDKpXS3r8Qrc8eX2E0wAQOidOEUxzFZ0XUA6K9SNusADD7CCUCSCCcASSKcACSJcAKQpNJdrQOwzHbvltav18d/Mq7ZWWlsTNrw1Yx08qT0i1/07W0JJwCtrV+v/3t0i17+n4OavjyuTTfM6NifbtE3Dx/s69vSrAPQ2vi4Dv7woA58vUU7Lu/Qga+36OAPD0rj4319W8IJQFsjT4/r3Ruf0Q69qndvfEYjT/c3mCSadQA6sOGrGY3e/I4+um+7/uHMO/rmV+OSOHMCUKSZGWlL6GO6/99eCX1NW7aE7X1EOAFo7eRJ6WBdH9P4eHh98mRf33YgpuntFtP0AsWo1DS9AAYT4QQgSYQTgCQRTgBa2737+itzMzNhex8RTgBaW7/+2qEDcWiB1q/v69syCBNAa7WhA1u2SM88I73zzrVDC/qkUmdOZrbZzKYWFxeLLgUol/HxEEyvvhqeswfTCjObMrPN7Q6sVDi5+zF3nxjkteiBvpiZCWdM27eH5+yjwxfdfcLdj7U7sFLhBCCDWh/TwYPSK69cbeJx+wqAQnH7yvLh9hWgGNy+AqD0CCcASSKcACSJcAKQJMIJQJIIJwBJIpwAJIlwApAkwglAkggnAEkinAAkiXACkCTCCUCSCCcASSKcACSJcAKQpFKHk5ntMrO1RdcBIH+lWxrKzEYkTUpakDQh6XihBQHoi9KFk7vPS9oqSWb2WMHlAOiTUjfrAAwuwglAkggnAEmqTDiZ2YSZzZnZ3MWLF4suB6iq22rfw/iYWOrA0nWIZ+XuU5KmpLBuXcHlAFX1OevWASg1wglAkggnAEkinAAkqXQd4mY2JOlFSSPxscvMpiUdd/fpImsDkJ/ShZO7LyjcWwdggNGsA5AkwglAklo268xstaShbn+pu3+UtSAAkNr3Ob0laUxS/Yhqa3jdzA091AQAbcNpm64/czJJ70naI+ls3fa7JL0h6blcKgNQaS3Dyd3/vXGbmX1H0iV3f7th14dm5pLGJf1zfiUCqKIsHeIPK0yR28wFSU9mKwUArsoSTouSxszsr5vse0pLBxcAdCxLOO2T9KWkE2b2MzO738y+b2b/qHDWtCfPAgFUU9cjxN39j2a2UdKbkt5WuHJnCmdMO939tVwrBFBJmW5fcfdzksbN7A5Jq+K2D3OsC0DFZR4hbmZ/KekhSXfWgsnMno0DNwGgJ5nCycx+KekzSS8rdILXrJX0Qg51Aai4rsPJzL4r6XmFs6Y3GnYfURhqAAA9yXLmNCZp1t2PNdl3SRnuxQOARlnHOa1aYt+YwkBMAOhJ1nFOt5rZ71QXUmb2c0kviXFOAHKQdZzTmKSjku6UJDO7rDDW6Q13/3WeBQKopl7GOd1lZpslrVEYgHnE3f8zv9IAVFlPc4jHTvFmHeNJimG6+e677y66FKCqVpjZlKRjS1xUuyLLUILH4zinZvu2x3vskuTux9x9YsWKFUWXAlTVortPtAsmKVuH+CqFplwzZxWu2AFATzpu1pnZ/fGfd0kaMrO/UegErzeupYcZAEDHuulzOqFr5w4/0bC/FlTv9VIQAEjdhdPG+PyUpHsVbmFptNBsal8A6FbH4VQ388AqhTnEmSIFQN903SHu7gcknY9TplyDKVMA5CXLUILvKMyC2QxTpgDIRdbVV866+3812XdETJkCIAdZZ8JcWGI7U6YAyEWWcPqDpDVm9mdN9o2JKVMA5CBLh/hvFJaGOlpbu87MbjazZxWmTNmZa4UAKinrjb8PSdov6ZzZNYPEX3J3liIH0LNepky5z8x+oDAgc0FMmQIgR71OmfK+pPdzqgUArmgZTnFM0xuS9tWaa2b2uMKy40txd9+UV4EAqqldh/iQms80YC0emRfqBICalmdO8f65bzRsOyDpQD+LAgDOcgAkqV2f02plGPHt7h9lLQgApPZX695SGPVdP8mcNbxu5oYeagKAtuG0TdefOZnCbJd7FOYMr7lL4crec7lU1oaZrZW0SdK8pGFJ8+4+vRzvDaD/2nWIXzerZRxecMnd327Y9aGZucLVvb6OEjezEUkvuvuP6rYdMrMv3P10P98bwPLIOmXKwhL7Lqj1GKi8TEra27DtdUm7luG9ASyDLOG0KGmsdtNvg6e0dHDlaYtCc67evEIzD8AAyBJO+xRmJThhZj8zs/vN7PtxMc0nFfqi+iY26Ybc/ZpwcveFuH9tP98fxfn4Y+n118MzBl/X99a5+x/NbKPCVL1vK1y5M4Uzpp3u/lquFV5vqM3+4T6/Pwrw8cfSgw9KX38tfetb0u9/L23YUHRV6KdeZiUYN7M7FG9tSX01FjObkDQhSStXriy4GnRrdjYE0+XL4Xl2lnAqqdvMbK7u9ZS7TzU7MPOsBHH1lYcU1qqr3RT8rKRZd/806+/t4v2Hak25TsT/A6YkaXR0tN04LSRmbCycMdXOnMbGiq4IGX3u7qOdHJgpnMzslwqrrFxSGOtUGzqwVtJ9kp7O8ns7tBCfh+v+LTMbiv/8oo/vjYJs2BCacrOzIZg4axp8XYeTmX1XYbXfhyStURhBXnNEocO8b+Hk7vNmtqDr+56G437GOQ2oDRsIpSrJcrVuTKHpdqzJvuVafWVa0kjDtpG4HcAAyDrOadUS+8a0PKuvTEp6sWHb1rgdwADI0ue0T9JLZvY7Xdvn83OF1Ve25VJZC7FpNxmvwM0rnDXtpUkHDI6s45zGJB2VdKckmdllhbFOb7j7r/MssEUdNOGAAdbLOKe7zGyzQqf4glh9BUCOslyt2y5po7tvip3izTrGAaAnWTrEj0p6YInlyAEgF1mWIz+nMNncBwQUgH7J0qx7XGEowVpJX5rZbMMhrFsHoGe9rPg7G5+tYXvjawDoWpahBKxbB6DvWLcOQJJ6mTLlfoVpeVfFTYcl7Xf3/86hLgAVl+nMKU7J+4GkRxT6mG5VmBVz1sxuzq88AFXVdTjFq3VPKZwlDbv7A+6+VuGm3xGFaVMAoCdZzpxWSTrv7j+t3+juJxTmeRrvvSwAVZclnM5q6WlRzkv6Q9ZiAKAmSzjNSlpjZn/RZN/zkv6pl4IAQMp2te5hhQnnTpjZhbrtQwozFAyZWX3T7qy7/33G+gBUVNahBBfic/1o8EWFK3iMEAfQM0aIA0gSI8QBJIlwApCkSoWTmW02s6nFxcWiSwGqaoWZTcUpvlsy9+qtzD06Oupzc3PtDwSQKzM71ely5JU6cwJQHrmHEzf+AshDlht//87MVi+x7w6FsU4A0JMsZ07Dkj4xs7+t3xjnd/okl6oAVF6W1VfekvSypANm9q6Z3RyXIj8haY+7r8u7SADVk3XF31fiqivvKcxCsKCw0OaHuVUGoNJ66RBfqXCz7ycKM2Hek0dBACBln6b3V5L2S/qVuz8g6UlJr5rZ9BJTqQBAV7JcrduuEEYPu/trkuTu7ytMl/Lnks7lWB+Aiso6E+ZGdz9Wv9HdP5O0UdLRHOoCUHFZpkw51mLfl5J+utR+AOgUt68ASFKmPiczu9zq0Y9CAVRLlnFOs5JearJ9WNI2SXt6qAcAJGXrc/pQUtPBlmb2ha4uTw4AmeXd5zSrsPIvAPQk73BaJc6cAOSg62ZdHIT5UotDzmYtBgBq8uwQl8INwO9lKwUArsq1Q3y5mdkuSf/i7qeLrgVAvrKu+FsYMxuRNKlwljYh6XihBQHoi9KFk7vPS9oqSWb2WMHlAOgTbl8BkCTCCUCSCCcASapMOJnZhJnNmdncxYsXiy4HqKrbat/D+JhY6sDSdYhn5e5TkqaksBx5weUAVfV5p8uRFxJOcTjAoS5+ZNrdJ/tVD4D0FBJOcTgA69sBWFJl+pwAlAvhBCBJpesQN7MhSS9KGomPXWY2Lem4u08XWRuA/JQunNx9QeHeOgADjGYdgCQRTgCSRDgBSBLhBCBJhBOAJBFOAJJEOAFIEuEEIEmEE4AkEU4AkkQ4AUgS4QQgSYQTgCQRTgCSRDgBSBLhBCBJhBOAJBFOAJJEOAFIUqXCycw2m9nU4uJi0aUAVbXCzKbMbHO7A829eitzj46O+tzcXNFlAJVjZqc6XY68UmdOAMqDcAKQJMIJQJIIJwBJIpwAJIlwApAkwglAkggnAEkinLowOSl9+9vhGUB/3Vh0AWUxOSnt3h3+XXvetau4eoBBx5lThw4fbv0aQL4Ipw49+mjr1wDyRbOuQ7Um3OHDIZho0gH9xawEAJYNsxIAKD3CCUCSCCcASSKcACSJcAKQpFIOJTCzxyQNS1onaUTSXnf/bbFVAchT6cIpBtNpd5+Pr4cknTKzEXffXWhxAHJTxmbdcC2YJMndFyRNSmJYJDBAShVOZjYiaa+ZrW3YdTrub9wOoKRKFU7xjGm3pPmGXUPxuXE7gJIqXZ+TuzebTenHCv1QC8tcDoA+Kf29dbFD/LykdfV9UU2Om5A0EV+ulvRp/6srxApJg7ze+m2SPi+6iD4a9M+v8bs35e5TzQ4chHA6LmmXu0938TNznd58WDZmNuXuE+2PLKdB/uwkPr96hTTrYsf2oS5+ZLpZc87MdqnLYKqAY0UXgJ7w+UWFhFNsfq3r5XfEZtpxgula7s4fd4nx+V1Vqqt1NXEg5nx9MJnZ2nhG1ommbVyUAp9duXX8+ZWuz8nMNincslJ/xjQkaau7by2kKAC5K1U4xStzl5bYPe/uf7WM5QDoo1KFE4DqKN0gzLwxw0E5xFuTNincBTCshj5HpCvrd6zS4cQMB+UQL3S86O4/qtt2yMy+cPfTBZaGNnr5jpXyal2OmOGgHCYl7W3Y9rr4nMog83essuHEDAelskXX39Q9r9DMQ6J6/Y5VNpyY4aAc4h/4UON9k7WbvPmPSLp6/Y5Vus+JGQ5KYajN/uHlKALZ9PIdq3Q4NYqddRPq8dYaAM118x2rbLNuCYck/ajV1CsoRvyjRvl1/B0r/ZkTMxwMvIX4PFz37/qw+mJ5y0FW3X7HGCGuKzMcMKgvUWZ2SdKD9WOa4n+U/sPdrbjK0Kks37HKN+tymOEA/TetMLK4XuPN30hU1u9Ypc+cmOGgHGpNd3dfV7ftkKTXGSGetl6+Y5UNJ2Y4KJe6P/L52jPN8LT1+h2rbDgBSFvl+5wApIlwApAkwglAkggnAEkinAAkiXACkCTCCUCSCCegQ2a2x8z2FF1HVZR+VgJgGa0puoAq4cwJQJIIJ5SemX3PzNzMvld0LcgP4QQgSfQ5oaV4NjLm7q8stc3MfiBpTf0xcftzkhbc/Tdt3uMeSWMKU2mcdff3lzjuCUmr4ssj7n4ubhuP254yszFJF2rv2Un93daB5cGZE9oZk/Rym20XJL1QfyXLzHZIelPS2Va/PAbYJ5IeUQiZ/Wa2r+GYe8zsfPx94/HYT+LP3qerHdVr4v77uqy/ozqwvDhzQs/iGcxPJB01szMKYfWypJ+4+7k2Pz4raYW7fyldOav5wMxm6s649sff+VDdcSvje39W+xlJ29z9RMb/GZ3UgWVEOCEX7v6+me1UOLtZkPReJ1/qxvBy9xNmJsXmW63JKOneWnDE4z7Lq/ZO6sDyo1mH3MQ+nLMKfTbbOv05M3vCzGbM7JKZNc5+uCb+7nZnYD1rUweWGeGE3NSd5SxIerLDnzmj0AT8V4VmW+NqKgvxuFtyKjNrHVhmNOuQxa2NG2If0H5JOxXOnj4ws7Ot+oBiv84aSRtbHHdEoan4pKS3eim6zjX1d1gHlhnhhHYWpCuX8S8oXOl6vv6AeFZzVOHy+1tx206FDvJ7W/QPnY3Pb8bjFyS9UH9A7PDeGY8ZUui4HlK4qnYmvt9CPPyReEz9sIa29XdSBwrg7jx4LPmQdIukM5Jc0nlJ+yTtCX86V47Zp7DKxsqGnz0TH7e0+P1PxJ/1eOyO+O8dTY47H/ddiu95T0MNV2rspv5O65A0I2mm6M+kKg9WXwGQJDrEASSJcAKQJMIJQJIIJwBJIpwAJIlwApAkwglAkggnAEkinAAkiXACkKT/BxPCJ0It3msPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAScAAAEdCAYAAABOu6GKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAATqElEQVR4nO3dT4wc5ZnH8d+TkHDYaKcZwWmzYM9yixPs8XAYLRtmgqUcFgdQgBxWGEy0Y6Hwd/dgcSDmTyTL1kY2ECnySFljs5e1443Bt9jRDHYkHxgbx8uNzXhg92YWz8Belix69vC+jdvtnunu6uqut7q+H2nU01U10w/09M/v+9Zbb5m7CwBS85WiCwCAVggnAEkinAAkiXACkCTCCUCSCCcASSKcACTphqILyMrMHpQ0KmmzpDFJB9z918VWBSAvpQynGEzn3X0xPq9JOmdmY+6+t9DiAOSirN260XowSZK7L0vaKWlPYRUByFXpwsnMxiQdMLPxpl3n4/7m7a1+x0w/akP/8d6VWzfvX+nCKbaY9kpabNpVi4/N21sZ2j9wM9tadA19NrTvncT716h04SRJ7r4zduUa/UhhHKp5e9UM+x/3sOP9i2wYViWIA+KXJG1uHItqOmZGMbVvvPHGzRs2bBhcgQO0srKikZGRosvom8uXL+uWW24puoy+Gfb379y5c/8r6f2GTbPuPtvq2GEJp5OS9rj7qU6On5iY8IWFhT5XBaCZmZ1z94lOji1lt66Rme1RF8EEoBxKHU6xq3aSYAKGT2nDKU7EXGwMJjMbj1MNAJRcKcPJzLYoXLqyaGZj8Wtc0o7VBsQBlEvpLl+JZ+ZOrrKbYAKGROnCKc5jsqLrANBfpezWARh+hBOAJBFOAJJEOAFIEuEEIEmEE4COzM5K3/9+eByE0k0lADB4s7PSjh3h+9/+NjzO9HllLVpOANo6dmzt5/1AOAFo64c/XPt5P9CtA9BWvQt37FgIpn536STCCUCHZmYGE0p1dOsAJIlwApAkwglAkggnAEkinAAkiXACkCTCCUCSCCcASapUOJnZVjObXVlZKboUoKpGzGzWzLa2O3AobkfeLW5HDhSjUrcjBzCcCCcASSKcACSJcAKQJMIJQJIIJwBJIpwAJIlwApAkwglAkggnAEkinAAkiXACkCTCCUCSCCcASSKcACSJcAKQJMIJQJIIJwBJIpwAJKnU4WRme8xsvOg6AOTvhqIL6JaZjUnaKWlZ0oykk4UWBKAvShdO7r4oaYckmdmDBZcDoE9K3a0DMLwIJwBJIpwAJKky4WRmM2a2YGYLly9fLrocoKpurn8O49fMageWbkA8K3eflTQrhduRF1wOUFUfcztyAKVGOAFIEuEEIEmEE4AklW5A3Mxqkp6XNBa/9pjZKUkn3f1UkbUByE/pwsndlxWurQMwxOjWAUgS4QQgSWt268xsg6Rat7/U3X+ftSAAkNqPOb0maUpS44xqa3reyld7qAkA2obTM7q+5WSS3pC0T9KFhu3rJe2X9HQulQGotDXDyd3/vXmbmX1b0hV3f71p1xkzc0nTkv4lvxIBVFGWAfH7FZbIbWVJ0mPZSgGAq7KE04qkKTP7Vot927V6cAFAx7KE00FJn0o6bWY/MbO7zOxeM/uVQqtpX54FAqimrmeIu/tnZna3pFclva5w5s4UWky73P1nuVYIoJIyTcJ094vuPi3pNoWpBne7+6i7v5JncYAkae9eaW7u2m1zc2E7hlbmGeJm9k1J90m6zd3PxG1PxombQH7uvFN6+OGrATU3F57feWexdaGvMoWTmf1c0keSXlIYBK8bl/RcDnUBV01PS0eOhED66U/D45EjYTuGVtfhZGZ/I+lZhVbT/qbdxxWmGgD5mp6WnnhCeuWV8EgwDb0sLacpSfPufqLFvivKcC0e0NbcnPTLX0ovvBAem8egMHSyznNat8q+KYWJmEB+6mNMR45IL798tYtHQA21rPOcbjKzY2oIKTN7StKLYp4T8vbuu9eOMdXHoN59t9i60FdZ5zlNSXpLYSqBzOwLhblO+939F3kWCKCaepnntF5hUHyXwgD5be7+DznWBgRMJagkc6/OzW/NbKukrbfffvvff/DBB0WXg27UA+mJJ8KAOFMJSsnM/kPSnKQTq5xU+1KWqQSPxHlOrfa9EK+xS5K7n3D3mZGRkaJLQbeYSjAsVtx9pl0wSdm6deskbVxl3wWFM3ZAvphKUDkdD4ib2V3x2/WSamb21wqD4I2mtfo0AyCbxqkE09Phi1niQ6+bs3Wnde3a4aeb9teD6o1eCgKus9ZUAsJpaHU8IB4vW5HCtXR3KJyha7bcamnf1ExMTPjCwkLRZQCVY2bn3H2ik2M7bjk1rDywTmEN8TPZygOA9roeEHf3NyVdikumXIMlUwDkJctUgm8rrILZCkumAMhF1ruvXHD3/2qx77hYMgVADrKuhLm8ynaWTAGQiyzh9KGkjWb2Zy32TYklUwDkIMuA+GGFW0O9Vb93nZl9w8yeVFgyZVeuFQKopK6XTInuk3RI0kWzayaJv+ju3IocQM8yhZO7X5S0ycx+oDAhc1nScXf/zxxrA1BhWVtOkiR3f1vS2znVAgBfWjOc4pym/ZIO1rtrZvaIwm3HV+PuviWvAgFUU7sB8ZparzRga3xlvlEnANSt2XKK1899pWnbm5Le7GdRAEArB0CS2o05bVCGGd/u/vusBQGA1P5s3WsKs74bF32ypuetfLWHmgCgbTg9o+tbTqaw2uU+hTXD69YrnNl7OpfK2jCzcUlbJC1KGpW06O6nBvHaAPqv3YD4dataxukFV9z99aZdZ8zMFc7u9XWWuJmNSXre3R9q2HbUzD5x9/P9fG0Ag5F1yZTlVfYtae05UHnZKelA07bdkvYM4LUBDECWcFqRNFW/6LfJdq0eXHl6WKE712hRoZsHYAhkCaeDCqsSnDazn5jZXWZ2b7yZ5mMKY1F9E7t0NXe/JpzcfTnuH+/n66M4Z89Ku3eHRwy/rq+tc/fPzOxuhaV6X1c4c2cKLaZd7v6zXCu8Xq3N/tE+vz4KcPasdM890uefS1//uvS730mTk0VXhX7qZVWCaTP7S8VLW1K/G4uZzUiakaRbb7214GrQrfn5EExffBEe5+cJp5K62cwa78s26+6zrQ7MvCpBvPvKfQr3qqtfFPykpHl3fz/r7+3i9Wv1rlwn4v+AWSnct65fdaE/pqZCi6necpqaKroiZPRx7veta2RmP1e4y8oVhblO9akD45I2Sfpxlt/boeX4ONrwvcysFr/9pI+vjYJMToau3Px8CCZaTcOv63CKd/59VqHVtFFhBnndcYUB876Fk7svmtmyrh97Go37mec0pCYnCaUqyXK2bkqh63aixb5B3X3llKSxpm1jcTuAIZB1ntO6VfZNaTB3X9kp6fmmbTvidgBDIMuY00FJL5rZMV075vOUwt1XnsmlsjXErt3OeAZuUaHVdIAuHTA8ss5zmpL0lqTbJMnMvlCY67Tf3X+RZ4Fr1EEXDhhivcxzWm9mWxUGxZfF3VcA5CjL2boXJN3t7lvioHirgXEA6EmWAfG3JH1vlduRA0AustyO/KLCYnPvEFAA+iVLt+4RhakE45I+NbP5pkO4bx2AnvVyx9/5+GhN25ufA0DXskwl4L51APqO+9YBSFIvS6bcpbAs77q46d8kHXL3/8mhLgAVl6nlFJfkfUfSAwpjTDcprIo5b2bfyK88AFXVdTjFs3XbFVpJo+7+PXcfV7jod0xh2RQA6EmWltM6SZfc/fHGje5+WmGdp+neywJQdVnC6YJWXxblkqQPsxYDAHVZwmle0kYz+4sW+56V9M+9FAQAUrazdfcrLDh32syWGrbXFFYoqJlZY9fugrv/Y8b6AFRU1qkES/GxcTb4isIZPGaIA+gZM8QBJIkZ4gCSRDgBSFKlwsnMtprZ7MrKStGlAFU1YmazcYnvNZl79e7MPTEx4QsLC+0PBJArMzvX6e3IK9VyAlAehBOAJGW58PcpM/uVmf1tPwoCAClby2lJ4eLeE2b2RzP7JzP7Vr5lAai6LHdfOeHuYwpLpJyW9Liki2Z2zsyeNLNv5lwjgArKPObk7qfdfbu7jypcU/eGwh1ZPjSzY2b2d/mUCKCKeh4QN7N7FVYjeE7houBDCtfZvR5bUxt6fQ0A1ZN1md4NcVD8vyW9LWmTpF2SbnP3x+NCdOsU1nZ6Ka9iAVRHlrN1L0i6qLB++CFJd7j7uLu/6e6f1Y9z908lvaewlAoAdCXLkikXJD0aVydoZ0nSixleA0DFZVky5UQXx7K0CoBMmCEOIEmZxpzM7Is1vv6vH4UCqJYsY07zaj2ONCrpGUn7s5cDAEGWMaczks602mdmn+jq7ckBILO8x5zmFS5rAYCe5B1O60TLCUAOuu7WxUmYL65xyIWsxQBAXZ4D4pK0rHABMAD0JNcB8UEzsz2S/tXdzxddC4B8Zb3jb2HMbEzSToVW2oykk4UWBKAvShdO7r4oaYckmdmDBZcDoE+4fAVAkggnAEkinAAkqTLhZGYzZrZgZguXL18uuhygqm6ufw7j18xqB5ZuQDwrd5+VNCuF25EXXA5QVR93ejvyQsIpTgc42sWPnHL3nf2qB0B6CgmnOB1gcxGvDaAcKjPmBKBcCCcASSrdgLiZ1SQ9L2ksfu0xs1OSTrr7qSJrA5Cf0oWTuy8rXFsHYIjRrQOQJMIJQJIIJwBJIpwAJIlwApAkwglAkggnAEkinAAkiXACkCTCCUCSCCcASSKcACSJcAKQJMIJQJIIJwBJIpwAJIlwApAkwglAkggnAEmqVDiZ2VYzm11ZWSm6FKCqRsxs1sy2tjvQ3Kt3Z+6JiQlfWFgougygcszsXKe3I69UywlAeRBOAJJEOAFIEuEEIEmEE4AkEU4AkkQ4AUgS4QQgSYRTF86elXbvDo8A+uuGogsoi7Nnpakp6U9/kr72NWl+XpqcLLoqYHjRcurQ4cPS559L7uHx8OGiKwKGG+EEIEmEU4e2bZNuvFEyC4/bthVdETDcGHPq0OSkNDcXxpqmphhvAvqNcOrC5CShBAwK3ToASSKcACSJcAKQJMIJQJJKOSBuZg9KGpW0WdKYpAPu/utiqwKQp9KFUwym8+6+GJ/XJJ0zszF331tocQByU8Zu3Wg9mCTJ3Zcl7ZS0p7CKAOSuVOFkZmOSDpjZeNOu83F/83YAJVWqcIotpr2SFpt21eJj83YAJVW6MSd339li848UxqGWB1wOgD4p/R1/44D4JUmbG8eiWhw3I2kmPt0g6f3+V1eIEUnDfL/1myV9XHQRfTTs71/zZ2/W3WdbHTgM4XRS0h53P9XFzyx0ekvksjGzWXefaX9kOQ3zeyfx/jUqpFsXB7aPdvEjp1p158xsj7oMpgo4UXQB6AnvX1RIOMXu1+Zefkfspp0kmK7l7vxxlxjv31WlOltXFydiLjYGk5mNxxZZJ1r2cVEKvHfl1vH7V7oxJzPbonDJSmOLqSZph7vvKKQoALkrVTjFM3NXVtm96O5/NcByAPRRqcIJQHWUbhJm3ljhoBzipUlbFK4CGFXTmCPSlfUzVulwYoWDcognOp5394cath01s0/c/XyBpaGNXj5jpTxblyNWOCiHnZIONG3bLd6nMsj8GatsOLHCQak8rOsv6l5U6OYhUb1+xiobTqxwUA7xD7zWfN1k/SJv/hFJV6+fsUqPObHCQSnU2uwfHUQRyKaXz1ilw6lZHKybUY+X1gBorZvPWGW7das4KumhtZZeQTHiHzXKr+PPWOlbTqxwMPSW4+Now/eNYfXJYMtBVt1+xpghri9XOGBSX6LM7IqkexrnNMV/lP7o7lZcZehUls9Y5bt1OaxwgP47pTCzuFHzxd9IVNbPWKVbTqxwUA71rru7b27YdlTSbmaIp62Xz1hlw4kVDsql4Y98sf5INzxtvX7GKhtOANJW+TEnAGkinAAkiXACkCTCCUCSCCcASSKcACSJcAKQJMIJ6JCZ7TOzfUXXURWlX5UAGKCNRRdQJbScACSJcELpmdl3zczN7LtF14L8EE4AksSYE9YUWyNT7v7yatvM7AeSNjYe07B9nbu/1uY1viNpSmEpjQvu/vYqx22TtC4+Pe7uF+O26bhtu5lNSVpy98Od1t9tHRgMWk5oZ0rSS2221SS9FD/cjV6VtGmtX25mT0v6g6QHFELmkJkdbDrmO2Z2Kf6+6XjsH+LPbtLVgeqNcX/ja3ZSf0d1YLBoOaFn7n7YzF6V9Jyk7dKXrZN1kn7T5sfnJY24+6cNP/eOmc3VWz+SDklaknRfw3G3xtf+qP4zkp5x99MZ/zM6qQMDRDghL29IekwxnOLjUruukbtfbHp+2syk2H2rdxkl3VEPjnjcR/mU3VkdGDy6dcjLQUm1GCaSdL+k4538oJltM7M5M7tiZs2rH26Urg+PfmhTBwaMcEIuYnhckPRAHHuqKQTWmszsPYXxn98odNua76ayHI/78xzLzVIHBoxuHbK4aZXt+xQ+4O8pnO1as7UTx3U2Srp7jbGi4woD4Y9JWvOsXxeuqb/DOjBghBPaWZa+PI2/pHCm69lVjj2uq2foOjnTdSE+vmpmu+JrPdd4QBzw3hWPqSkMXNcUzqq9F6cpLMfDH4jHNE5r6KT+tnVg8OjWoZ03FD68h+LXekn7Wx0YB6yPK7Ryjrf7xfH4RxUGnd9SCLb3Whz3cjxuu8JZuUNx13zcfzHW+Wz8Heu7qb/TOjBY3H0FuYpzg2ru/kDRtaDcaDkhN3HQ+n61n9sEtEXLCT1ruKxkk8LlKmvOCgc6QcsJeVjS1evbHi2wDgwRWk4AkkTLCUCSCCcASSKcACSJcAKQJMIJQJIIJwBJ+n9AnEdy2wzUMgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " def main():\n",
    " \n",
    "    if not eval(confParser['neural net regression']['regression']):\n",
    "        return\n",
    "\n",
    "    nn = NeuralNetwork(\n",
    "                        hidden_layer_sizes = eval(confParser['neural net regression']['hidden_layer_sizes']),\n",
    "                        learning_rate_init = eval(confParser['neural net regression']['learning_rate_init']),\n",
    "                        n_iter_no_change   = eval(confParser['neural net regression']['n_iter_no_change']),\n",
    "                        tol                = eval(confParser['neural net regression']['tol']),\n",
    "                        max_iter           = eval(confParser['neural net regression']['max_iter']),\n",
    "                        alpha              = eval(confParser['neural net regression']['alpha']),\n",
    "                        hidden_layer_size  = eval(confParser['neural net regression']['hidden_layer_size']),\n",
    "                        fully_connected    = eval(confParser['neural net regression']['fully_connected']),\n",
    "                        implementation     = eval(confParser['neural net regression']['implementation']),\n",
    "                        cnn                = eval(confParser['neural net regression']['cnn']),\n",
    "                        n_channels         = eval(confParser['neural net regression']['n_channels']),\n",
    "                        kernel_size        = eval(confParser['neural net regression']['kernel_size']),\n",
    "                        activation         = eval(confParser['neural net regression']['activation']),\n",
    "                        number_hidden_layers = eval(confParser['neural net regression']['number_hidden_layers']),\n",
    "                        verbose            = True \n",
    "                    )\n",
    "    \n",
    "    nn.Parse( path  = confParser['neural net']['input_path'],\n",
    "              nruns = eval(confParser['neural net']['nruns']))\n",
    "\n",
    "    nn.Combine() #--- concat. descriptors\n",
    "\n",
    "\n",
    "    #--- filter data\n",
    "    train_type = int(eval(confParser['neural net regression']['train_type']))\n",
    "    \n",
    "    filtr      = {\n",
    "                     0: nn.perAtomData.defect_label == 1.0, #--- only non-crystalline atoms\n",
    "                     1: np.ones(len(nn.perAtomData),dtype=bool), #--- every atom\n",
    "                    }[train_type]\n",
    "    stratify   = {\n",
    "                    0:None,\n",
    "                    1:np.c_[nn.perAtomData.defect_label].astype(int), #--- ensure the same ratio in test and training\n",
    "                    }[train_type]\n",
    "\n",
    "\n",
    "    \n",
    "    #--- train regressor\n",
    "\n",
    "    nn.TrainRegressor(  stratify=stratify,\n",
    "                        y=np.c_[nn.perAtomData[filtr]['ux uy uz'.split()]],\n",
    "                        printOvito = True,\n",
    "                        filtr=filtr,\n",
    "                     )\n",
    "    return nn\n",
    "\n",
    "#!rm -r png;mkdir png\n",
    "model_regr = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864aefa4",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6f1a5a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    if eval(confParser['flags']['RemoteMachine']):\n",
    "        return\n",
    "    \n",
    "\n",
    "    \n",
    "    #--- ann\n",
    "    number_hidden_layers  = dict(zip(range(4),[1]))\n",
    "    hidden_layer_size     = dict(zip(range(4),[1]))\n",
    "    n_channels            = dict(zip(range(4),[1]))\n",
    "    activations           = dict(zip(range(20),['relu']))\n",
    "#     string[ inums ] = \"\\t\\'5\\':\\'neuralNet/20x20/ann/classifier/layer%s/channel%s/activation%s/layer_size%s\\',\\n\" % (key_n,key_c,key_a,key_h) #--- change job name\n",
    "    \n",
    "    #--- cnn\n",
    "#     number_hidden_layers  = dict(zip(range(4),[1,2,3]))\n",
    "#     hidden_layer_size     = dict(zip(range(4),[1]))\n",
    "#     n_channels            = dict(zip(range(4),[8,16,32,64]))\n",
    "#     activations           = dict(zip(range(20),['linear']))\n",
    "\n",
    "    runs = [0,1,2]\n",
    "    \n",
    "    legend = utl.Legends()\n",
    "    legend.Set(fontsize=14,bbox_to_anchor=(1.5, 0.3, 0.5, 0.5))\n",
    "    symbols = utl.Symbols()\n",
    "    \n",
    "    nphi = len(number_hidden_layers)\n",
    "    #---\n",
    "    count = 0\n",
    "    ax = utl.PltErr(None, None, Plot=False )\n",
    "    for key_n in number_hidden_layers:\n",
    "        number_hidden_layer = number_hidden_layers[key_n]\n",
    "#         if number_hidden_layer != 2:\n",
    "#             continue\n",
    "        for key_c in n_channels:\n",
    "            n_channel = n_channels[key_c]\n",
    "#             if n_channel != 16:\n",
    "#                 continue\n",
    "            for key_a in activations:\n",
    "                activation = activations[key_a]\n",
    "                for key_h in hidden_layer_size:\n",
    "                    nsize = hidden_layer_size[key_h]\n",
    "\n",
    "        #---\t\n",
    "#                    path = 'neuralNet/20x20/cnn/classifier/layer%s/channel%s/activation%s/layer_size%s'%(key_n,key_c,key_a,key_h) #--- change job name\n",
    "                    path = 'test' #--- change job name\n",
    "                    fp = ['confusion.txt', 'val_loss_classification.txt','loss.txt'][2]\n",
    "                    for irun in runs:\n",
    "                        try:\n",
    "                            data = np.loadtxt('%s/Run%s/png/%s'%(path,irun,fp))\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                        if fp == 'confusion.txt':\n",
    "                            accuracy_crystals = data[0,0]/np.sum(data[0,:])\n",
    "                            accuracy_defects = data[1,1]/np.sum(data[1,:])\n",
    "                            print(data)\n",
    "                            utl.PltErr(accuracy_crystals, accuracy_defects,\n",
    "                               attrs=symbols.GetAttrs(count=count%7,nevery=800,\n",
    "                                    label='%s layers, %s channels, act. %s'%(number_hidden_layer,n_channel,activation)), \n",
    "                                       Plot=False,\n",
    "                                       ax=ax,\n",
    "                                       )\n",
    "                        else:\n",
    "                            epoch = data[:,0]\n",
    "                            loss = data[:,1]\n",
    "                            val_loss = data[:,2]\n",
    "\n",
    "                            utl.PltErr(epoch, val_loss,\n",
    "                               attrs=symbols.GetAttrs(count=count%7,nevery=800,\n",
    "                                    label='%s layers, %s channels, act. %s'%(number_hidden_layer,n_channel,activation)), \n",
    "                                       Plot=False,\n",
    "                                       ax=ax,\n",
    "                                       )\n",
    "                    count += 1\n",
    "    ax = utl.PltErr(None, None,\n",
    "                        yscale='log',xscale='log',\n",
    "                       xstr='epoch',ystr='validation loss',\n",
    "#                     ylim=(1e-1,1e1),\n",
    "                    ax=ax,\n",
    "                    legend=legend.Get(),\n",
    "                       title='png/training_loss.png',\n",
    "                   )\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b74daa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[337.   3.]\n",
      " [  0.   2.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# legend = utl.Legends()\n",
    "# legend.Set(fontsize=14,bbox_to_anchor=(1.5, 0.3, 0.5, 0.5))\n",
    "# symbols = utl.Symbols()\n",
    "\n",
    "# fp = ['confusion.txt', 'val_loss_classification.txt'][0]\n",
    "# data = np.loadtxt('png/%s'%(fp))\n",
    "# ax = utl.PltErr(None, None, Plot=False )\n",
    "# if fp == 'confusion.txt':\n",
    "#     accuracy_crystals = data[0,0]/np.sum(data[0,:])\n",
    "#     accuracy_defects = data[1,1]/np.sum(data[1,:])\n",
    "#     print(data)\n",
    "#     utl.PltErr(accuracy_crystals, accuracy_defects,\n",
    "#        attrs=symbols.GetAttrs(count=0,nevery=800,\n",
    "#             ), \n",
    "#                Plot=False,\n",
    "#                ax=ax,\n",
    "#                )\n",
    "# else:\n",
    "#     epoch = data[:,0]\n",
    "#     loss = data[:,1]\n",
    "#     val_loss = data[:,2]\n",
    "\n",
    "#     utl.PltErr(epoch, val_loss,\n",
    "#        attrs=symbols.GetAttrs(count=0,nevery=10,\n",
    "#             ), \n",
    "#                Plot=False,\n",
    "#                ax=ax,\n",
    "#                )\n",
    "    \n",
    "# ax = utl.PltErr(None, None,\n",
    "# yscale='log',xscale='log',\n",
    "# xstr='epoch',ystr='validation loss',\n",
    "# #                     ylim=(1e-1,1e1),\n",
    "# ax=ax,\n",
    "# # legend=legend.Get(),\n",
    "# title='png/training_loss.png',\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f506974a",
   "metadata": {},
   "source": [
    "## test example: 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1be92045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.725966 1.725967]]\n",
      "(3, 441)\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# y=np.c_[[1.725966,1.725967],\n",
    "#             [-1.725966,1.725967],\n",
    "#             [-1.725966,-1.725967],\n",
    "#             [1.725966,-1.725967],\n",
    "#            ].T\n",
    "\n",
    "# X=np.concatenate([list(map(lambda x:np.load('png/descriptor%s.npy'%x).flatten(),range(4)))],axis=1)\n",
    "\n",
    "# #--- zscore\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X)\n",
    "# X_transfrmd = scaler.transform( X )\n",
    "\n",
    "# X_train_transfrmd, X_test_transfrmd, y_train, y_test = train_test_split(X_transfrmd, y, test_size=0.25)\n",
    "# print(y_test)\n",
    "\n",
    "\n",
    "# print(X_train_transfrmd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69377834",
   "metadata": {},
   "source": [
    "### fully connected in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c56d633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.79024675\n",
      "Iteration 2, loss = 4.79153813\n",
      "Iteration 3, loss = 0.48272480\n",
      "Iteration 4, loss = 0.84242732\n",
      "Iteration 5, loss = 1.20200108\n",
      "Iteration 6, loss = 0.50029766\n",
      "Iteration 7, loss = 0.08083888\n",
      "Iteration 8, loss = 0.31471365\n",
      "Iteration 9, loss = 0.39602049\n",
      "Iteration 10, loss = 0.14417737\n",
      "Iteration 11, loss = 0.05911270\n",
      "Iteration 12, loss = 0.17784119\n",
      "Iteration 13, loss = 0.18843745\n",
      "Iteration 14, loss = 0.08671426\n",
      "Iteration 15, loss = 0.05304450\n",
      "Iteration 16, loss = 0.09624741\n",
      "Iteration 17, loss = 0.09671026\n",
      "Iteration 18, loss = 0.05631733\n",
      "Iteration 19, loss = 0.05017827\n",
      "Iteration 20, loss = 0.06458879\n",
      "Iteration 21, loss = 0.05694448\n",
      "Iteration 22, loss = 0.04181389\n",
      "Iteration 23, loss = 0.04398522\n",
      "Iteration 24, loss = 0.04448364\n",
      "Iteration 25, loss = 0.03786413\n",
      "Iteration 26, loss = 0.03483561\n",
      "Iteration 27, loss = 0.03725800\n",
      "Iteration 28, loss = 0.03546887\n",
      "Iteration 29, loss = 0.03089711\n",
      "Iteration 30, loss = 0.03086940\n",
      "Iteration 31, loss = 0.03225031\n",
      "Iteration 32, loss = 0.02917275\n",
      "Iteration 33, loss = 0.02726652\n",
      "Iteration 34, loss = 0.02892336\n",
      "Iteration 35, loss = 0.02808117\n",
      "Iteration 36, loss = 0.02537011\n",
      "Iteration 37, loss = 0.02600391\n",
      "Iteration 38, loss = 0.02698854\n",
      "Iteration 39, loss = 0.02494826\n",
      "Iteration 40, loss = 0.02394280\n",
      "Iteration 41, loss = 0.02572275\n",
      "Iteration 42, loss = 0.02497078\n",
      "Iteration 43, loss = 0.02293285\n",
      "Iteration 44, loss = 0.02413425\n",
      "Iteration 45, loss = 0.02455046\n",
      "Iteration 46, loss = 0.02270986\n",
      "Iteration 47, loss = 0.02347172\n",
      "Iteration 48, loss = 0.02360856\n",
      "Iteration 49, loss = 0.02255866\n",
      "Iteration 50, loss = 0.02314975\n",
      "Iteration 51, loss = 0.02309027\n",
      "Iteration 52, loss = 0.02259666\n",
      "Iteration 53, loss = 0.02288285\n",
      "Iteration 54, loss = 0.02297684\n",
      "Iteration 55, loss = 0.02300910\n",
      "Iteration 56, loss = 0.02306715\n",
      "Iteration 57, loss = 0.02287899\n",
      "Iteration 58, loss = 0.02262910\n",
      "Iteration 59, loss = 0.02261209\n",
      "Iteration 60, loss = 0.02282631\n",
      "Iteration 61, loss = 0.02271234\n",
      "Iteration 62, loss = 0.02245436\n",
      "Iteration 63, loss = 0.02261107\n",
      "Iteration 64, loss = 0.02257859\n",
      "Iteration 65, loss = 0.02275319\n",
      "Iteration 66, loss = 0.02301587\n",
      "Iteration 67, loss = 0.02290518\n",
      "Iteration 68, loss = 0.02263656\n",
      "Iteration 69, loss = 0.02244420\n",
      "Iteration 70, loss = 0.02264114\n",
      "Iteration 71, loss = 0.02285952\n",
      "Iteration 72, loss = 0.02284724\n",
      "Iteration 73, loss = 0.02254051\n",
      "Iteration 74, loss = 0.02253781\n",
      "Iteration 75, loss = 0.02259026\n",
      "Iteration 76, loss = 0.02266804\n",
      "Iteration 77, loss = 0.02250421\n",
      "Iteration 78, loss = 0.02246337\n",
      "Iteration 79, loss = 0.02244526\n",
      "Iteration 80, loss = 0.02255226\n",
      "Iteration 81, loss = 0.02250348\n",
      "Iteration 82, loss = 0.02240397\n",
      "Iteration 83, loss = 0.02242926\n",
      "Iteration 84, loss = 0.02245178\n",
      "Iteration 85, loss = 0.02249455\n",
      "Iteration 86, loss = 0.02250251\n",
      "Iteration 87, loss = 0.02240619\n",
      "Iteration 88, loss = 0.02238247\n",
      "Iteration 89, loss = 0.02239953\n",
      "Iteration 90, loss = 0.02242186\n",
      "Iteration 91, loss = 0.02243013\n",
      "Iteration 92, loss = 0.02237696\n",
      "Iteration 93, loss = 0.02237238\n",
      "Iteration 94, loss = 0.02236637\n",
      "Iteration 95, loss = 0.02241898\n",
      "Iteration 96, loss = 0.02244555\n",
      "Iteration 97, loss = 0.02243013\n",
      "Iteration 98, loss = 0.02236881\n",
      "Iteration 99, loss = 0.02236044\n",
      "Iteration 100, loss = 0.02237842\n",
      "mkdir: png: File exists\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# #        pdb.set_trace()\n",
    "# #--- tune parameters\n",
    "\n",
    "# #--- train\n",
    "# mlp = MLPRegressor(random_state=1,\n",
    "#                     verbose=True,\n",
    "#                    n_iter_no_change=100000,\n",
    "#                     max_iter=100,#00,\n",
    "#                    hidden_layer_sizes=(1000,1000),\n",
    "# #                    shuffle=False,\n",
    "# #                     alpha=1e-1,\n",
    "\n",
    "#                   )\n",
    "# mlp.fit(X_train_transfrmd,y_train)\n",
    "\n",
    "# #--- validate\n",
    "# !mkdir png\n",
    "# utl.PltErr(range(len(mlp.loss_curve_)), mlp.loss_curve_,\n",
    "#            attrs={'fmt':'-'},\n",
    "#            yscale='log',xscale='log',\n",
    "# #           xlim=(1,self.max_iter[0]),\n",
    "#            xstr='epoch',ystr='loss',\n",
    "#            title='png/loss.png',\n",
    "#           )\n",
    "\n",
    "# # #         pdb.set_trace()\n",
    "# y_pred =mlp.predict(X_test_transfrmd)        \n",
    "# y_pred_train = mlp.predict(X_train_transfrmd)        \n",
    "# for idime, xstr in zip(range(2),'ux uy'.split()):\n",
    "#     ax = utl.PltErr(None,None,Plot=False)\n",
    "#     #\n",
    "#     utl.PltErr(y_test[:,idime],y_pred[:,idime],\n",
    "#                attrs={'fmt':'x','color':'red','zorder':10,'markersize':6},\n",
    "#                ax=ax,\n",
    "#                Plot = False,\n",
    "\n",
    "#               )\n",
    "#     #\n",
    "#     utl.PltErr(y_train[:,idime],y_pred_train[:,idime],\n",
    "#                attrs={'fmt':'.','color':'blue','zorder':1,'markersize':6},\n",
    "#                ax=ax,\n",
    "#                Plot = False,\n",
    "\n",
    "#               )\n",
    "#     #\n",
    "#     utl.PltErr(None,None,Plot=False,\n",
    "#                    title='png/scatter%s.png'%idime,\n",
    "#                     ax=ax,\n",
    "#                xstr='%s actual'%xstr,ystr='%s predicted'%xstr,\n",
    "#                xlim=(-3,3),ylim=(-3,3),\n",
    "#                    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1353cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp.best_loss_, mlp.loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ux,uy=mlp.predict(X_test_transfrmd)[0]\n",
    "# ax=utl.PltErr([0,ux],[0,uy],\n",
    "#               Plot=False\n",
    "#           )\n",
    "# utl.PltErr([0,y_test[0][0]],[0,y_test[0][1]],\n",
    "#            xlim=(-3,3),ylim=(-3,3),\n",
    "#             ax=ax\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ux,uy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce616c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = utl.PltErr(None,None,Plot=False)\n",
    "\n",
    "# for i in range(2):\n",
    "#     utl.PltErr(range(data.descriptors[0,:].shape[0]),data.descriptors[i,:],\n",
    "#               attrs={'fmt':'-'},#,'color':'C0'},\n",
    "#                xscale='log',yscale='log',\n",
    "#                ax=ax,\n",
    "#                Plot=False,\n",
    "#               )\n",
    "\n",
    "# utl.PltErr(range(data.descriptors[100,:].shape[0]),data.descriptors[100,:],\n",
    "#           attrs={'fmt':'-','color':'C0'},\n",
    "#            xscale='log',yscale='log',\n",
    "#            ax=ax,\n",
    "#            Plot=False,\n",
    "#           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5598bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.Spectra(nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d061978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPRegressor\n",
    "# from sklearn.datasets import make_regression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X, y = make_regression(n_samples=200, random_state=1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "#                                                     random_state=1)\n",
    "# regr = MLPRegressor(verbose=False,\n",
    "#                     random_state=1, \n",
    "# #                     learning_rate='adaptive',\n",
    "# #                    early_stopping=True, \n",
    "#                      n_iter_no_change=1, \n",
    "#                     tol=1e-2,\n",
    "#                      max_iter=10000000,\n",
    "# #                     solver='sgd',\n",
    "#                    ).fit(X_train, y_train)\n",
    "# regr.tol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5eecba",
   "metadata": {},
   "source": [
    "### fully connected in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a27a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #--- The network architecture\n",
    "# model = keras.Sequential([\n",
    "#     layers.Dense(512), #activation=\"relu\"),\n",
    "# #     layers.Dense(1000), #activation=\"relu\"),\n",
    "#     layers.Dense(2) #, activation=\"relu\")\n",
    "#     ])\n",
    "\n",
    "# #--- The compilation step\n",
    "# optimizer = tf.keras.optimizers.Adam() #learning_rate=1e-4)\n",
    "# model.compile( optimizer=optimizer,#\"rmsprop\",\n",
    "#                loss=\"mean_squared_error\",#\"sparse_categorical_crossentropy\",\n",
    "#                metrics=[\"mse\"]\n",
    "#              )\n",
    "\n",
    "# #--- Preparing the image data\n",
    "# # train_images = train_images.reshape((60000, 28 * 28))\n",
    "# # train_images = train_images.astype(\"float32\") / 255\n",
    "# # test_images = test_images.reshape((10000, 28 * 28))\n",
    "# # test_images = test_images.astype(\"float32\") / 255\n",
    "\n",
    "# #--- “Fitting” the model X_train_transfrmd,y_train\n",
    "# model.fit(X_train_transfrmd, y_train, \n",
    "#             validation_data=(X_test_transfrmd, y_test),\n",
    "\n",
    "#           epochs=100, verbose=False)#, batch_size=128)\n",
    "\n",
    "# loss = model.history.history['loss']\n",
    "# val_loss = model.history.history['val_loss']\n",
    "# #--- validate\n",
    "\n",
    "# ax = utl.PltErr(range(len(val_loss)), val_loss,\n",
    "#            attrs={'fmt':'-'}, Plot=False,\n",
    "#           )\n",
    "# utl.PltErr(range(len(loss)), loss,\n",
    "#            attrs={'fmt':'-'},\n",
    "#            ax=ax,\n",
    "#            yscale='log',xscale='log',\n",
    "#            xlim=(1,100),\n",
    "#            xstr='epoch',ystr='loss',\n",
    "#            title='png/loss.png',\n",
    "#           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262922c9",
   "metadata": {},
   "source": [
    "### cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f22b71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tf.random.set_random_seed(812)\n",
    "\n",
    "# shape=(300,300,1)\n",
    "# kernel_size = (3,3)\n",
    "# epochs = 1000\n",
    "# activation = ['linear','sigmoid','relu'][0]\n",
    "# padding='same'\n",
    "# filters = 1\n",
    "# #\n",
    "# ndime = y_train.shape[1]\n",
    "# n_train = X_train_transfrmd.shape[0]\n",
    "# n_test = X_test_transfrmd.shape[0]\n",
    "# assert shape[0]*shape[1]*shape[2] == X_train_transfrmd.shape[1]\n",
    "# inputs = keras.Input(shape=shape)\n",
    "# #\n",
    "# x = layers.Conv2D(filters=filters, kernel_size=kernel_size,activation=activation,padding=padding)(inputs)\n",
    "# # x = layers.AveragePooling2D(pool_size=2)(x)\n",
    "# # x = layers.Conv2D(filters=2*filters, kernel_size=kernel_size,activation=activation,padding=padding)(x)\n",
    "# # x = layers.AveragePooling2D(pool_size=2)(x)\n",
    "# # x = layers.Conv2D(filters=4*filters, kernel_size=kernel_size,activation=activation,padding=padding)(x)\n",
    "# # x = layers.AveragePooling2D(pool_size=2)(x)\n",
    "# # x = layers.Conv2D(filters=8*filters, kernel_size=kernel_size,activation=activation,padding=padding)(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# outputs = layers.Dense( ndime, activation=activation)(x)\n",
    "\n",
    "# #--- The network architecture\n",
    "# model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "# #--- The compilation step\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5,epsilon=1e-08)\n",
    "# model.compile( optimizer=optimizer,#\"rmsprop\",\n",
    "#                loss=\"mean_squared_error\",#\"sparse_categorical_crossentropy\",\n",
    "#                metrics=[\"mse\"]\n",
    "#              )\n",
    "\n",
    "# #--- save best model \n",
    "# callbacks=[keras.callbacks.ModelCheckpoint( filepath='png/convnet_from_scratch.keras',  \n",
    "#                                            monitor=\"val_loss\",\n",
    "#                                            save_freq=10,\n",
    "#                                             save_best_only=True)]\n",
    "\n",
    "# #--- “Fitting” the model X_train_transfrmd,y_train\n",
    "# X_train_reshaped = X_train_transfrmd.reshape((n_train,shape[0],shape[1],1))\n",
    "# X_test_reshaped = X_test_transfrmd.reshape((n_test,shape[0],shape[1],1))\n",
    "# model.fit(X_train_reshaped, y_train, \n",
    "#             validation_data=(X_test_reshaped, y_test),\n",
    "#             #callbacks=callbacks,\n",
    "#           epochs=epochs, verbose=False, shuffle=False)#, batch_size=128)\n",
    "\n",
    "# loss = model.history.history['loss']\n",
    "# val_loss = model.history.history['val_loss']\n",
    "# #--- validate\n",
    "\n",
    "# ax = utl.PltErr(range(len(val_loss)), val_loss,\n",
    "#            attrs={'fmt':'-'}, Plot=False,\n",
    "#           )\n",
    "# utl.PltErr(range(len(loss)), loss,\n",
    "#            attrs={'fmt':'-'},\n",
    "#            ax=ax,\n",
    "#            yscale='log',xscale='log',\n",
    "#            xlim=(1,epochs),\n",
    "#            xstr='epoch',ystr='loss',\n",
    "#            title='png/loss.png',\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ae920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model = keras.models.load_model(\"png/convnet_from_scratch.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa9c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ux,uy=best_model.predict(X_test_reshaped)[0]\n",
    "# ax=utl.PltErr([0,ux],[0,uy],\n",
    "#               Plot=False\n",
    "#           )\n",
    "# utl.PltErr([0,y_test[0][0]],[0,y_test[0][1]],\n",
    "#            xlim=(-3,3),ylim=(-3,3),\n",
    "#             ax=ax\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf166dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (ux,uy), y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e9080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "430.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
