{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cded12cf",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#import-libs\" data-toc-modified-id=\"import-libs-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>import libs</a></span></li><li><span><a href=\"#Train-NN\" data-toc-modified-id=\"Train-NN-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Train NN</a></span><ul class=\"toc-item\"><li><span><a href=\"#main()\" data-toc-modified-id=\"main()-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>main()</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot\" data-toc-modified-id=\"Plot-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Plot</a></span></li></ul></li><li><span><a href=\"#test-example:-2d\" data-toc-modified-id=\"test-example:-2d-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>test example: 2d</a></span><ul class=\"toc-item\"><li><span><a href=\"#fully-connected-in-sklearn\" data-toc-modified-id=\"fully-connected-in-sklearn-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>fully connected in sklearn</a></span></li><li><span><a href=\"#fully-connected-in-keras\" data-toc-modified-id=\"fully-connected-in-keras-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>fully connected in keras</a></span></li><li><span><a href=\"#cnn\" data-toc-modified-id=\"cnn-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>cnn</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117638d",
   "metadata": {},
   "source": [
    "# import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49343b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf. file sections: ['flags', 'input files', 'EnergyBarrier', 'neural net', 'ml mc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'LammpsPostProcess' from '../../HeaDef/postprocess/LammpsPostProcess.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "confParser = configparser.ConfigParser()\n",
    "\n",
    "#--- parse conf. file\n",
    "confParser.read('configuration.ini')\n",
    "print('conf. file sections:',confParser.sections())\n",
    "#\n",
    "import os\n",
    "import sys\n",
    "list(map(lambda x:sys.path.append(x), confParser['input files']['lib_path'].split()))\n",
    "from dscribe.descriptors import SOAP, ACSF\n",
    "import ase\n",
    "import ase.io\n",
    "import ase.build\n",
    "from ase.io import lammpsdata\n",
    "import pdb\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import matplotlib.pyplot as plt\n",
    "if not eval(confParser['flags']['RemoteMachine']):\n",
    "    plt.rc('text', usetex=True)\n",
    "#\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#\n",
    "from scipy.stats import gaussian_kde\n",
    "#\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#--- user modules\n",
    "import LammpsPostProcess as lp\n",
    "import utility as utl\n",
    "import imp\n",
    "imp.reload(utl)\n",
    "imp.reload(lp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a2e60",
   "metadata": {},
   "source": [
    "# Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cd8c31d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "\n",
    "    def __init__(self, verbose=False,\n",
    "                **kwargs\n",
    "                ):\n",
    "        self.verbose = verbose\n",
    "        for key in kwargs:\n",
    "            setattr(self,key,kwargs[key])\n",
    "    \n",
    "    def Parse(self,path,nruns):\n",
    "        self.perAtomData = {}\n",
    "        rwjs = utl.ReadWriteJson()\n",
    "        for irun in range(nruns):\n",
    "            if irun == 0:\n",
    "                #--- same configurations!\n",
    "                self.descriptors  = np.c_[rwjs.Read('%s/Run%s/descriptors.json'%(path,irun))[0]['data']]\n",
    "                self.shape        = np.c_[rwjs.Read('%s/Run%s/descriptors.json'%(path,irun))[0]['shape']].flatten()\n",
    "                self.positions    = np.c_[rwjs.Read('%s/Run%s/descriptors.json'%(path,irun))[0]['xyz']]\n",
    "                os.system('ln -s %s/Run%s/dumpFile/dump.xyz .'%(path,irun))\n",
    "            try:\n",
    "                data = np.loadtxt('%s/Run%s/perAtomData.txt'%(path,irun))\n",
    "                #--- displacement data\n",
    "                self.perAtomData[irun] = pd.DataFrame(np.c_[data],\n",
    "                columns='id\ttype\tx\ty\tz\tux\tuy\tuz\tenergy_barrier\tdefect_label'.split()\n",
    "                            )\n",
    "            except:\n",
    "#                 if self.verbose:\n",
    "#                     traceback.print_exc()\n",
    "                continue\n",
    "                \n",
    "        \n",
    "        self.nruns = list(self.perAtomData.keys())\n",
    "        self.nruns.sort()\n",
    "        \n",
    "    def Combine(self):\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('concatenating descriptors ...')\n",
    "#         pdb.set_trace()\n",
    "        #--- center atoms\n",
    "        center_atom_indices = list(map(lambda x:NeuralNetwork.GetCenterAtom( self.perAtomData[x])[0],self.nruns))\n",
    "        sdict = dict(zip(center_atom_indices,self.nruns))\n",
    "        \n",
    "        atom_ids = list(sdict.keys())\n",
    "        atom_ids.sort()\n",
    "        #         center_atom_indices = list( set( center_atom_indices ) )\n",
    "        data = np.concatenate(list(map(lambda x: np.c_[self.perAtomData[sdict[x]].iloc[ x ]],atom_ids)),axis=1).T\n",
    "        descriptors_center_atoms = self.descriptors[atom_ids]\n",
    "    \n",
    "        #--- data frame\n",
    "#        print(data.shape)\n",
    "        irun = self.nruns[0]\n",
    "        df_combined = pd.DataFrame(data,columns=list(self.perAtomData[irun].keys()))\n",
    "    \n",
    "        #--- filter crystaliine atoms\n",
    "        filtr = self.perAtomData[irun].defect_label == 0.0\n",
    "        df_crystalline = self.perAtomData[irun][filtr]\n",
    "        descriptors_crystalline = self.descriptors[filtr]\n",
    "\n",
    "        #--- merge\n",
    "        keys = list(df_combined.keys())\n",
    "        data_concat = np.concatenate([np.c_[df_combined[keys]],np.c_[df_crystalline[keys]]],axis=0) \n",
    "        self.perAtomData = pd.DataFrame(data_concat,\n",
    "                              columns=keys\n",
    "                             )\n",
    "\n",
    "        \n",
    "        #--- merge descriptors\n",
    "        self.descriptors = np.concatenate([descriptors_center_atoms,descriptors_crystalline],axis=0)\n",
    "\n",
    "        assert self.perAtomData.shape[ 0 ] == self.descriptors.shape[0], 'need more mc swaps: %s %s'\\\n",
    "        %(self.perAtomData.shape[ 0 ],self.descriptors.shape[0])\n",
    "                            \n",
    "                                 \n",
    "    \n",
    "    @staticmethod\n",
    "    def GetCenterAtom(df):\n",
    "        disp_magnitude = df.ux**2+df.uy**2+df.uz**2\n",
    "        center_atom_indx = disp_magnitude.sort_values(ascending=False).index[0]\n",
    "        return center_atom_indx, int(df.iloc[ center_atom_indx ].id)\n",
    "\n",
    "    @staticmethod\n",
    "    def zscore(slist):\n",
    "        tmp = np.copy(slist)\n",
    "        print(np.mean(tmp),np.std(tmp))\n",
    "        tmp -= np.mean(tmp)\n",
    "        tmp /= np.std(tmp)\n",
    "        return tmp\n",
    "\n",
    "    def PCA(self,\n",
    "           n_components=2,\n",
    "            random_state = 1,\n",
    "           ):\n",
    "        #--- concat. data\n",
    "        X = self.descriptors\n",
    "        pca = PCA(n_components=n_components,random_state=random_state)\n",
    "        pca.fit(X)\n",
    "        X_transformed = pca.transform(X)\n",
    "\n",
    "        xdata = X_transformed[:,0]\n",
    "        ydata = X_transformed[:,1]\n",
    "        #\n",
    "        filtr_defects = self.perAtomData.defect_label == 0.0\n",
    "        #\n",
    "\n",
    "        legend = utl.Legends()\n",
    "        legend.Set(bbox_to_anchor=(1.1,.5, 0.5, 0.5))\n",
    "#         pdb.set_trace()\n",
    "        #ax = utl.PltErr(zscore(xdata)[filtr_defects],zscore(ydata)[filtr_defects],\n",
    "        ax = utl.PltErr(xdata[filtr_defects],ydata[filtr_defects],\n",
    "                  attrs={'fmt':'x','alpha':1,'label':'defect_free'},\n",
    "                        Plot = False,\n",
    "        #                 xlim=(-2,2),\n",
    "        #                 ylim=(-2,2),\n",
    "                  )\n",
    "\n",
    "        #utl.PltErr(zscore(xdata)[~filtr_defects],zscore(ydata)[~filtr_defects],\n",
    "        !mkdir png\n",
    "        utl.PltErr(xdata[~filtr_defects],ydata[~filtr_defects],\n",
    "                  attrs={'fmt':'.','color':'red','label':'defects'},\n",
    "                   ax=ax,\n",
    "                   xstr='pca_1',ystr='pca_2',\n",
    "                   legend = legend.Get(),\n",
    "                   title='png/pca.png'\n",
    "                  )\n",
    "    def Spectra(self,\n",
    "               nrows=100,\n",
    "               ):\n",
    "        assert nrows <= self.descriptors.shape[ 0 ]\n",
    "        !mkdir png\n",
    "        utl.PltBitmap(np.log10(np.abs(self.descriptors[:nrows,:])),\n",
    "                      xlabel=r'$\\mathrm{ndim}$',ylabel=r'$\\mathrm{natom}$',\n",
    "                      xlim=(0,self.descriptors.shape[1]),\n",
    "                      ylim=(0,nrows),\n",
    "                      colorbar=True,\n",
    "                      zscore=False,\n",
    "                      vminmax=(-3,3),\n",
    "                      title='png/feature_bitmap.png'\n",
    "                     )\n",
    "        \n",
    "    def SklearnMLP(self,X_train,y_train):\n",
    "        #-----------------------\n",
    "        #--- parameter grid\n",
    "        #-----------------------\n",
    "#         param_grid = {\n",
    "#                         'hidden_layer_sizes':self.hidden_layer_sizes,\n",
    "#                          #'activation' : ['tanh', 'relu'],\n",
    "#                          'learning_rate_init':self.learning_rate_init,\n",
    "# #                         'alpha':self.alpha, #--- regularization \n",
    "#                          #'learning_rate' : ['invscaling', 'adaptive'],\n",
    "#                         'n_iter_no_change':self.n_iter_no_change,\n",
    "# #                        'tol':self.tol,\n",
    "#                         'max_iter':self.max_iter,\n",
    "#                      } \n",
    "        mlp   =  MLPClassifier(random_state=1,\n",
    "                               hidden_layer_sizes = self.hidden_layer_sizes[0],\n",
    "                               learning_rate_init = self.learning_rate_init[0],\n",
    "                               n_iter_no_change   = self.n_iter_no_change[0],\n",
    "                               max_iter           = self.max_iter[0],\n",
    "                               verbose=self.verbose)\n",
    "#         clf  =  GridSearchCV(mlp, param_grid)\n",
    "#        clf.fit(X_train,y_train)\n",
    "        mlp.fit(X_train,y_train)\n",
    "        model =  mlp #clf.best_estimator_\n",
    "        loss  =  model.loss_curve_\n",
    "        val_loss = loss\n",
    "        return (model, loss, val_loss)\n",
    "\n",
    "    def KerasANN(self, X_train, y_train,X_test, y_test, ndime):\n",
    "\n",
    "        model     = keras.Sequential([ #--- The network architecture\n",
    "                                    layers.Dense(self.hidden_layer_size, activation=self.activation),\n",
    "                #                    layers.Dense(self.hidden_layer_size, activation=self.activation),\n",
    "                                    layers.Dense(ndime, activation='softmax')\n",
    "                                    ])\n",
    "        \n",
    "#         shape         =  (self.shape[0]*self.shape[1]*self.shape[2],)\n",
    "#         inputs        =  keras.Input(shape=shape)\n",
    "#         #------------------------------\n",
    "#         #--- The network architecture\n",
    "#         #------------------------------\n",
    "#         x             =  layers.Dense(   self.hidden_layer_size, activation=self.activation\n",
    "#                                        )(inputs)\n",
    "#         for i in range( self.number_hidden_layers ):\n",
    "#             x       = layers.Dense( self.hidden_layer_size, activation=self.activation\n",
    "#                                      )(x)\n",
    "#         #--- output layer\n",
    "# #         x       = layers.Flatten()(x)\n",
    "#         outputs = layers.Dense( ndime, activation=self.activation)(x)\n",
    "#         model   = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        \n",
    "        \n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate_init) #--- compilation step\n",
    "        model.compile( optimizer=optimizer,\n",
    "                       loss=\"sparse_categorical_crossentropy\",\n",
    "                       metrics=[\"mse\"]\n",
    "                     )\n",
    "        #--- save best model\n",
    "        !mkdir best_model\n",
    "#         callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnetClassifier_from_scratch.tf',  \n",
    "#                                                     monitor=\"mse\",\n",
    "#                                                     save_freq=10,\n",
    "#                                                     save_best_only=True)]\n",
    "\n",
    "        model.fit( X_train, y_train, \n",
    "           validation_data      = ( X_test, y_test ),\n",
    "#             callbacks           = callbacks,\n",
    "            epochs              = self.max_iter[0], \n",
    "            verbose             = self.verbose, \n",
    "            shuffle             = False, \n",
    "#             batch_size     = 32,\n",
    "#                     use_multiprocessing = True,\n",
    "#                     workers             = 4,\n",
    "         )    \n",
    "\n",
    "        model.save('best_model/convnetClassifier_from_scratch.keras')\n",
    "        loss      = model.history.history['loss']\n",
    "        val_loss  = model.history.history['val_loss']\n",
    "        best_model = model #keras.models.load_model(\"best_model/convnetClassifier_from_scratch.tf\")\n",
    "\n",
    "        return (best_model, loss, val_loss)\n",
    "                             \n",
    "    def TrainClassifier(self,y,\n",
    "                       random_state=1,\n",
    "                       ):\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "\n",
    "        ndime = 2 #--- binary classification\n",
    "        #---------------\n",
    "        #--- zscore X\n",
    "        #---------------        \n",
    "        X      = np.c_[self.descriptors]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "        X      = scaler.transform( X )\n",
    "\n",
    "        #--- train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                            random_state=random_state)\n",
    "\n",
    "        #-----------------------\n",
    "        #--- train model\n",
    "        #-----------------------\n",
    "        if self.fully_connected: #--- dense nn\n",
    "            if self.implementation == 'sklearn':\n",
    "                (model, loss, val_loss) = self.SklearnMLP(X_train,y_train)\n",
    "                classes_x = model.predict(X_test) \n",
    "                \n",
    "            elif self.implementation == 'keras': #--- dense nn in keras\n",
    "                (model, loss, val_loss) = self.KerasANN(X_train, y_train,X_test, y_test, ndime)\n",
    "                predict_x = model.predict(X_test) \n",
    "                classes_x = np.argmax(predict_x,axis=1)\n",
    "                \n",
    "        elif self.cnn: #--- convolutional\n",
    "            (model, loss, val_loss), (X_train, X_test) =\\\n",
    "            self.ConvNetworkClassifier( y )\n",
    "            predict_x = model.predict(X_test) \n",
    "            classes_x = np.argmax(predict_x,axis=1)\n",
    "                    \n",
    "        #--- save loss data\n",
    "        !mkdir png\n",
    "        np.savetxt('png/val_loss_classification.txt',\n",
    "                   np.c_[range(len(loss)),loss,val_loss],\n",
    "                   header='epoch loss val_loss')\n",
    "\n",
    "        #--- confusion matrix\n",
    "        cm = confusion_matrix(y_test, classes_x,\n",
    "                         labels=[0, 1]\n",
    "                        )\n",
    "        np.savetxt('png/confusion.txt',np.c_[cm])\n",
    "\n",
    "        \n",
    "\n",
    "    def TrainRegressor(self,stratify,y,\n",
    "                       random_state=1,\n",
    "                       printOvito = False,\n",
    "                       filtr = None,\n",
    "                       ):\n",
    "        '''\n",
    "        Multi-layer Perceptron regressor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        stratify : array-like, default=None\n",
    "        If not None, data is split in a stratified fashion, using this as\n",
    "        the class labels.\n",
    "        \n",
    "        y : array-like, target data\n",
    "        \n",
    "        random_state : initial seed, default=1\n",
    "        \n",
    "        printOvito : bool, default=False\n",
    "        \n",
    "        filtr : bool, default=False\n",
    "        if not None, data is filtered before calling train-test split\n",
    "        '''\n",
    "        if self.verbose:\n",
    "            print('dim(y)=',y.shape)\n",
    "\n",
    "        ndime  = y.shape[1] #--- dimension of the target vector\n",
    "        \n",
    "        #---------------\n",
    "        #--- zscore X\n",
    "        #---------------        \n",
    "        X      = np.c_[self.descriptors[filtr]]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "        X      = scaler.transform( X )\n",
    "    \n",
    "        #-----------------------\n",
    "        #--- train-test split\n",
    "        #-----------------------\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=stratify,\n",
    "                                                            random_state=random_state)\n",
    "\n",
    "\n",
    "        #-----------------------\n",
    "        #--- train model\n",
    "        #-----------------------\n",
    "        if self.fully_connected: #--- dense nn\n",
    "            if self.implementation == 'sklearn':\n",
    "                #-----------------------\n",
    "                #--- parameter grid\n",
    "                #-----------------------\n",
    "                param_grid = {\n",
    "                                'hidden_layer_sizes':self.hidden_layer_sizes,\n",
    "                                 #'activation' : ['tanh', 'relu'],\n",
    "                                 'learning_rate_init':self.learning_rate_init,\n",
    "                                'alpha':self.alpha, #--- regularization \n",
    "                                 #'learning_rate' : ['invscaling', 'adaptive'],\n",
    "                                'n_iter_no_change':self.n_iter_no_change,\n",
    "                                'tol':self.tol,\n",
    "                                'max_iter':self.max_iter,\n",
    "                             } \n",
    "                mlp   =  MLPRegressor(random_state=random_state,verbose=self.verbose) #--- mlp regressor\n",
    "                regr  =  GridSearchCV(mlp, param_grid)\n",
    "                regr.fit(X_train,y_train)\n",
    "                model =  regr.best_estimator_\n",
    "                loss  =  model.loss_curve_\n",
    "                \n",
    "            elif self.implementation == 'keras': #--- dense nn in keras\n",
    "                model     = keras.Sequential([ #--- The network architecture\n",
    "                    layers.Dense(self.hidden_layer_size, activation=self.activation),\n",
    "                    layers.Dense(self.hidden_layer_size, activation=self.activation),\n",
    "                    layers.Dense(ndime, activation=self.activation)\n",
    "                    ])\n",
    "                optimizer = tf.keras.optimizers.Adam() #learning_rate=1e-4) #--- compilation step\n",
    "                model.compile( optimizer=optimizer,#\"rmsprop\",\n",
    "                               loss=\"mean_squared_error\",#\"sparse_categorical_crossentropy\",\n",
    "                               metrics=[\"mse\"]\n",
    "                             )\n",
    "                model.fit(X_train, y_train, #--- “Fitting”\n",
    "                          validation_data=(X_test, y_test),\n",
    "                          epochs=self.max_iter[0], verbose=self.verbose)#, batch_size=128)\n",
    "                loss      = model.history.history['loss']\n",
    "                val_loss  = model.history.history['val_loss']\n",
    "                \n",
    "        elif self.cnn: #--- convolutional\n",
    "            (model, loss, val_loss), (X_train, X_test) =\\\n",
    "            self.ConvNetwork(X_train, y_train, X_test, y_test )\n",
    "            \n",
    "\n",
    "        #-----------------------\n",
    "        #--- validation\n",
    "        #-----------------------\n",
    "        !mkdir png         #--- plot validation loss \n",
    "        ax = utl.PltErr(range(len(val_loss)), val_loss,\n",
    "                   attrs={'fmt':'-'}, Plot=False,\n",
    "                  )\n",
    "        utl.PltErr(range(len(loss)), loss,\n",
    "                   attrs={'fmt':'-'},\n",
    "                   ax=ax,\n",
    "                   yscale='log',xscale='log',\n",
    "                   xlim=(1,self.max_iter[0]),\n",
    "                   xstr='epoch',ystr='loss',\n",
    "                   title='png/loss.png',\n",
    "                  )\n",
    "        \n",
    "        np.savetxt('png/loss.txt',np.c_[range(len(loss)),loss,val_loss],header='epoch loss val_loss')\n",
    "        \n",
    "        #--- plot predictions\n",
    "        y_pred       = model.predict(X_test)        \n",
    "        y_pred_train = model.predict(X_train)        \n",
    "        for idime, xstr in zip(range(3),'ux uy uz'.split()):\n",
    "            ax = utl.PltErr(None,None,Plot=False)\n",
    "            #\n",
    "            utl.PltErr(y_test[:,idime],y_pred[:,idime],\n",
    "                       attrs={'fmt':'x','color':'red','zorder':10,'markersize':6},\n",
    "                       ax=ax,\n",
    "                       Plot = False,\n",
    "\n",
    "                      )\n",
    "            #\n",
    "            utl.PltErr(y_train[:,idime],y_pred_train[:,idime],\n",
    "                       attrs={'fmt':'.','color':'blue','zorder':1,'markersize':6},\n",
    "                       ax=ax,\n",
    "                       Plot = False,\n",
    "\n",
    "                      )\n",
    "            #\n",
    "            utl.PltErr(None,None,Plot=False,\n",
    "                           title='png/scatter%s.png'%idime,\n",
    "                            ax=ax,\n",
    "                       xstr='%s actual'%xstr,ystr='%s predicted'%xstr,\n",
    "                       xlim=(-2,2),ylim=(-2,2),\n",
    "                           )\n",
    "        \n",
    "        #--- save in ovito\n",
    "        if printOvito:\n",
    "            X_train, X_test, _, _ = train_test_split(self.perAtomData[filtr]['id type x y z'.split()], y, stratify=stratify,\n",
    "                                                    random_state=random_state)\n",
    "#            pdb.set_trace()\n",
    "            with open('original.xyz','w') as fp:\n",
    "                utl.PrintOvito(self.perAtomData[filtr], fp, '0', attr_list='id type x y z ux uy uz'.split())\n",
    "            with open('train.xyz','w') as fp:\n",
    "                cordc = pd.DataFrame(np.c_[X_train,y_pred_train],columns='id type x y z ux uy uz'.split())\n",
    "                utl.PrintOvito(cordc, fp, '0', attr_list='id type x y z ux uy uz'.split())\n",
    "            with open('test.xyz','w') as fp:\n",
    "                cordc = pd.DataFrame(np.c_[X_test,y_pred],columns='id type x y z ux uy uz'.split())\n",
    "                utl.PrintOvito(cordc, fp, '0', attr_list='id type x y z ux uy uz'.split())\n",
    "                \n",
    "\n",
    "    def ConvNetwork(self,X_train, y_train, X_test, y_test):\n",
    "        '''\n",
    "        Convolutional neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : array-like training x input\n",
    "        \n",
    "        y_train : array-like, training y input\n",
    "        \n",
    "        X_test : array-like test x input\n",
    "        \n",
    "        y_test : array-like, training y input\n",
    "\n",
    "        Return\n",
    "        ---------- ( , loss,  )\n",
    "        best_model : cnn object, best trained model based on on the validation loss\n",
    "        \n",
    "        loss : array-like, mse loss\n",
    "\n",
    "        val_loss : array-like, validation loss\n",
    "\n",
    "        '''\n",
    "#         tf.random.set_random_seed(812)\n",
    "\n",
    "        shape         =  (self.shape[0],self.shape[1],self.shape[2],1) #--- rows, cols, thickness, channels\n",
    "        kernel_size   =  self.kernel_size \n",
    "        epochs        =  self.max_iter[0]\n",
    "        activation    =  self.activation\n",
    "        padding       = 'same'\n",
    "        filters       =  self.n_channels\n",
    "        learning_rate = self.learning_rate_init[0]\n",
    "        #\n",
    "        ndime         =  y_train.shape[1]\n",
    "        n_train       =  X_train.shape[0]\n",
    "        n_test        =  X_test.shape[0]\n",
    "        assert        shape[0] * shape[1] * shape[2] == X_train.shape[ 1 ]\n",
    "        inputs        =  keras.Input(shape=shape)\n",
    "        #\n",
    "\n",
    "        #------------------------------\n",
    "        #--- The network architecture\n",
    "        #------------------------------\n",
    "        x             =  layers.Conv3D(   filters     =  filters, \n",
    "                                          kernel_size =  kernel_size,\n",
    "                                          activation  =  activation,\n",
    "                                          padding     =  padding\n",
    "                                       )(inputs)\n",
    "        filters       *=  2\n",
    "        for i in range( self.number_hidden_layers ):\n",
    "            x       = layers.AveragePooling3D( pool_size = 2 )( x )\n",
    "            x       = layers.Conv3D( filters       =  filters, \n",
    "                                     kernel_size   =  kernel_size,\n",
    "                                     activation    =  activation,\n",
    "                                     padding       =  padding\n",
    "                                     )(x)\n",
    "            filters *= 2\n",
    "            \n",
    "        #--- output layer\n",
    "        x       = layers.Flatten()(x)\n",
    "        outputs = layers.Dense( ndime, activation=activation)(x)\n",
    "        model   = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        if self.verbose:\n",
    "            print('cnn model summary:',model.summary())\n",
    "\n",
    "        #--- The compilation step\n",
    "        optimizer = tf.keras.optimizers.Adam( learning_rate = learning_rate )\n",
    "        model.compile( optimizer =  optimizer,\n",
    "                       loss      =  \"mean_squared_error\",\n",
    "                       metrics   =  [\"mse\"]\n",
    "                     )\n",
    "\n",
    "        #--- save best model\n",
    "        !mkdir best_model\n",
    "        callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnet_from_scratch.tf',  \n",
    "                                                   monitor=\"loss\",\n",
    "                                                  save_freq=10,\n",
    "                                                    save_best_only=True)]\n",
    "\n",
    "        #--- “Fitting” the model X_train_transfrmd, y_train\n",
    "        X_train_reshaped =  X_train.reshape((n_train,shape[0],shape[1],shape[2],1))\n",
    "        X_test_reshaped  =  X_test.reshape((n_test,shape[0],shape[1],shape[2],1))\n",
    "        model.fit( X_train_reshaped, y_train, \n",
    "                   validation_data      = ( X_test_reshaped, y_test ),\n",
    "                   callbacks            = callbacks,\n",
    "                    epochs              = epochs, \n",
    "                    verbose             = self.verbose, \n",
    "                    shuffle             = False, \n",
    "#                     batch_size     = 128,\n",
    "#                     use_multiprocessing = True,\n",
    "#                     workers             = 4,\n",
    "                 )\n",
    "\n",
    "        #--- validation loss\n",
    "        loss       = model.history.history['loss']\n",
    "        val_loss   = model.history.history['val_loss']\n",
    "        best_model = keras.models.load_model(\"best_model/convnet_from_scratch.tf\")\n",
    "\n",
    "        \n",
    "        return ( best_model, loss, val_loss ), (X_train_reshaped, X_test_reshaped)\n",
    "    \n",
    "\n",
    "    def ConvNetworkClassifier(self,y,\n",
    "                               random_state=1\n",
    "                               ):\n",
    "        '''\n",
    "        Convolutional neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : array-like training x input\n",
    "        \n",
    "        y_train : array-like, training y input\n",
    "        \n",
    "        X_test : array-like test x input\n",
    "        \n",
    "        y_test : array-like, training y input\n",
    "\n",
    "        Return\n",
    "        ---------- ( , loss,  )\n",
    "        best_model : cnn object, best trained model based on on the validation loss\n",
    "        \n",
    "        loss : array-like, mse loss\n",
    "\n",
    "        val_loss : array-like, validation loss\n",
    "\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('dim(y)=',y.shape)\n",
    "        \n",
    "\n",
    "        #---------------\n",
    "        #--- zscore X\n",
    "        #---------------        \n",
    "        X      = np.c_[self.descriptors ]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "        X      = scaler.transform( X )\n",
    "    \n",
    "        if self.verbose:\n",
    "            print('X.shape:=',X.shape)\n",
    "            \n",
    "            \n",
    "            \n",
    "        #-----------------------\n",
    "        #--- train-test split\n",
    "        #-----------------------\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                            random_state=random_state)\n",
    "\n",
    "        \n",
    "        \n",
    "        #---- set model parameters\n",
    "        shape         =  (self.shape[0],self.shape[1],self.shape[2],1) #--- rows, cols, thickness, channels\n",
    "        kernel_size   =  self.kernel_size \n",
    "        epochs        =  self.max_iter[0]\n",
    "        activation    =  self.activation\n",
    "        padding       = 'same'\n",
    "        filters       =  self.n_channels\n",
    "        learning_rate = self.learning_rate_init[0]\n",
    "        #\n",
    "        ndime         =  y_train.shape[1]\n",
    "        n_train       =  X_train.shape[0]\n",
    "        n_test        =  X_test.shape[0]\n",
    "        assert        shape[0] * shape[1] * shape[2] == X_train.shape[ 1 ]\n",
    "        inputs        =  keras.Input(shape=shape)\n",
    "\n",
    "        #------------------------------\n",
    "        #--- The network architecture\n",
    "        #------------------------------\n",
    "        x             =  layers.Conv3D(   filters     =  filters, \n",
    "                                          kernel_size =  kernel_size,\n",
    "                                          activation  =  activation,\n",
    "                                          padding     =  padding\n",
    "                                       )(inputs)\n",
    "        filters       *=  2\n",
    "        for i in range( self.number_hidden_layers ):\n",
    "            x       = layers.AveragePooling3D( pool_size = 2 )( x )\n",
    "            x       = layers.Conv3D( filters       =  filters, \n",
    "                                     kernel_size   =  kernel_size,\n",
    "                                     activation    =  activation,\n",
    "                                     padding       =  padding\n",
    "                                     )(x)\n",
    "            filters *= 2\n",
    "            \n",
    "        #--- output layer\n",
    "        x       = layers.Flatten()(x)\n",
    "        outputs = layers.Dense( ndime, activation=activation)(x)\n",
    "        model   = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        if self.verbose:\n",
    "            print('cnn model summary:',model.summary())\n",
    "\n",
    "        #--- The compilation step\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate) #--- compilation step\n",
    "        model.compile( optimizer =  optimizer,\n",
    "                       loss=\"binary_crossentropy\",#\"sparse_categorical_crossentropy\",\n",
    "                       metrics   =  [\"accuracy\"]\n",
    "                     )\n",
    "\n",
    "        #--- save best model\n",
    "        !mkdir best_model\n",
    "        #--- save best model\n",
    "        !mkdir best_model\n",
    "        callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnetClassifier_from_scratch.tf',  \n",
    "                                                    monitor=\"accuracy\",\n",
    "                                                    save_freq=10,\n",
    "                                                    save_best_only=True)]\n",
    "\n",
    "        #--- “Fitting” the model X_train_transfrmd, y_train\n",
    "        X_train_reshaped =  X_train.reshape((n_train,shape[0],shape[1],shape[2],1))\n",
    "        X_test_reshaped  =  X_test.reshape((n_test,shape[0],shape[1],shape[2],1))\n",
    "        model.fit( X_train_reshaped, y_train, \n",
    "                   validation_data      = ( X_test_reshaped, y_test ),\n",
    "                   callbacks            = callbacks,\n",
    "                    epochs              = epochs, \n",
    "                    verbose             = self.verbose, \n",
    "                    shuffle             = False, \n",
    "#                     batch_size     = 128,\n",
    "#                     use_multiprocessing = True,\n",
    "#                     workers             = 4,\n",
    "                 )\n",
    "\n",
    "        #--- validation loss\n",
    "        loss       = model.history.history['loss']\n",
    "        val_loss   = model.history.history['val_loss']\n",
    "        best_model = keras.models.load_model(\"best_model/convnetClassifier_from_scratch.tf\")\n",
    "\n",
    "        \n",
    "        return ( best_model, loss, val_loss ), (X_train_reshaped, X_test_reshaped)\n",
    "    \n",
    "\n",
    "    \n",
    "    def PrintDensityMap(self, atomIndx, fout):\n",
    "        with open(fout,'w') as fp:\n",
    "                    disp           = np.c_[self.perAtomData.iloc[atomIndx]['ux uy uz'.split()]].flatten()\n",
    "                    df             = pd.DataFrame(np.c_[self.positions.T,self.descriptors[atomIndx]],\n",
    "                                                  columns='x y z mass'.split())\n",
    "                    utl.PrintOvito(df, fp, 'disp = %s'%disp, attr_list='x y z mass'.split())\n",
    "#     def SaveConf(self,fout):\n",
    "#         with open(fout,'w') as fp:\n",
    "#             np.savetxt(fp,np.c_[self.perAtomData],header=' '.join(list(self.perAtomData.keys())))\n",
    "\n",
    "#     def Test(self,y,\n",
    "#                                    random_state=1\n",
    "#                                    ):\n",
    "#             '''\n",
    "#             Convolutional neural network.\n",
    "\n",
    "#             Parameters\n",
    "#             ----------\n",
    "#             X_train : array-like training x input\n",
    "\n",
    "#             y_train : array-like, training y input\n",
    "\n",
    "#             X_test : array-like test x input\n",
    "\n",
    "#             y_test : array-like, training y input\n",
    "\n",
    "#             Return\n",
    "#             ---------- ( , loss,  )\n",
    "#             best_model : cnn object, best trained model based on on the validation loss\n",
    "\n",
    "#             loss : array-like, mse loss\n",
    "\n",
    "#             val_loss : array-like, validation loss\n",
    "\n",
    "#             '''\n",
    "\n",
    "#             if self.verbose:\n",
    "#                 print('dim(y)=',y.shape)\n",
    "\n",
    "#             ndime  = y.shape[1] #--- dimension of the target vector\n",
    "\n",
    "\n",
    "#             #---------------\n",
    "#             #--- zscore X\n",
    "#             #---------------        \n",
    "#             X      = np.c_[self.descriptors ]\n",
    "#             scaler = StandardScaler()\n",
    "#             scaler.fit(X)\n",
    "#             X      = scaler.transform( X )\n",
    "\n",
    "#             if self.verbose:\n",
    "#                 print('X.shape:=',X.shape)\n",
    "\n",
    "\n",
    "\n",
    "#             #-----------------------\n",
    "#             #--- train-test split\n",
    "#             #-----------------------\n",
    "#             X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "#                                                                 random_state=random_state)\n",
    "\n",
    "\n",
    "\n",
    "#             #---- set model parameters\n",
    "#             shape         =  (self.shape[0],self.shape[1],self.shape[2],1) #--- rows, cols, thickness, channels\n",
    "#             kernel_size   =  self.kernel_size \n",
    "#             epochs        =  self.max_iter[0]\n",
    "#             activation    =  self.activation\n",
    "#             padding       = 'same'\n",
    "#             filters       =  self.n_channels\n",
    "#             learning_rate = self.learning_rate_init[0]\n",
    "#             #\n",
    "#             ndime         =  y_train.shape[1]\n",
    "#             n_train       =  X_train.shape[0]\n",
    "#             n_test        =  X_test.shape[0]\n",
    "#             assert        shape[0] * shape[1] * shape[2] == X_train.shape[ 1 ]\n",
    "#             inputs        =  keras.Input(shape=shape)\n",
    "#             #\n",
    "#     #         pdb.set_trace()\n",
    "#             #------------------------------\n",
    "#             #--- The network architecture\n",
    "#             #------------------------------\n",
    "#             model     = keras.Sequential([\n",
    "#                 layers.Dense(self.hidden_layer_size, activation=\"relu\"),\n",
    "#     #             layers.Dense(self.hidden_layer_size), #activation=\"relu\"),\n",
    "#                 layers.Dense(2, activation=\"softmax\")\n",
    "#                 ])\n",
    "#             optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate) #--- compilation step\n",
    "\n",
    "#             model.compile( optimizer=\"rmsprop\",\n",
    "#                            loss=\"sparse_categorical_crossentropy\",\n",
    "#                            metrics=[\"mse\"])\n",
    "\n",
    "\n",
    "#             #--- “Fitting” the model X_train_transfrmd, y_train\n",
    "#             X_train_reshaped =  X_train \n",
    "#             X_test_reshaped  =  X_test\n",
    "#             model.fit( X_train_reshaped, y_train, \n",
    "#                        validation_data      = ( X_test_reshaped, y_test ),\n",
    "#     #                     callbacks=callbacks,\n",
    "#                         epochs              = epochs, \n",
    "#                         verbose             = self.verbose, \n",
    "#                         shuffle             = False, \n",
    "#     #                     batch_size     = 128,\n",
    "#                         use_multiprocessing = True,\n",
    "#                         workers             = 4,\n",
    "#                      )        \n",
    "\n",
    "\n",
    "#             #--- save best model\n",
    "#             !mkdir best_model\n",
    "#     #         callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnetClassifier_from_scratch.tf',  \n",
    "#     #                                                    monitor=\"val_loss\",\n",
    "#     #                                                   save_freq=10,\n",
    "#     #                                                     save_best_only=True)]\n",
    "\n",
    "\n",
    "#             #--- validation loss\n",
    "#             loss       = model.history.history['loss']\n",
    "#             val_loss   = model.history.history['val_loss']\n",
    "#             best_model = model #keras.models.load_model(\"best_model/convnet_from_scratch.tf\")\n",
    "\n",
    "\n",
    "#             !mkdir png\n",
    "#             utl.PltErr(range(len(loss)), loss,\n",
    "#                        yscale='log',\n",
    "#                        xstr='epoch',ystr='loss',\n",
    "#                        title='png/loss_classification.png',\n",
    "#                       )\n",
    "\n",
    "#     #         pdb.set_trace()\n",
    "#             #--- confusion matrix\n",
    "#             cm = confusion_matrix(y_test, model.predict_classes(X_test),\n",
    "#                              labels=[0, 1]\n",
    "#                             )\n",
    "#             print('cm=',cm)\n",
    "#             np.savetxt('png/confusion.txt',np.c_[cm])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "628ad42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Combine',\n",
       " 'ConvNetwork',\n",
       " 'ConvNetworkClassifier',\n",
       " 'GetCenterAtom',\n",
       " 'KerasANN',\n",
       " 'PCA',\n",
       " 'Parse',\n",
       " 'PrintDensityMap',\n",
       " 'SklearnMLP',\n",
       " 'Spectra',\n",
       " 'TrainClassifier',\n",
       " 'TrainRegressor',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'zscore']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(NeuralNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37a6f2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ModelCheckpoint in module keras.callbacks:\n",
      "\n",
      "class ModelCheckpoint(Callback)\n",
      " |  ModelCheckpoint(filepath, monitor: str = 'val_loss', verbose: int = 0, save_best_only: bool = False, save_weights_only: bool = False, mode: str = 'auto', save_freq='epoch', options=None, initial_value_threshold=None, **kwargs)\n",
      " |  \n",
      " |  Callback to save the Keras model or model weights at some frequency.\n",
      " |  \n",
      " |  `ModelCheckpoint` callback is used in conjunction with training using\n",
      " |  `model.fit()` to save a model or weights (in a checkpoint file) at some\n",
      " |  interval, so the model or weights can be loaded later to continue the\n",
      " |  training from the state saved.\n",
      " |  \n",
      " |  A few options this callback provides include:\n",
      " |  \n",
      " |  - Whether to only keep the model that has achieved the \"best performance\" so\n",
      " |    far, or whether to save the model at the end of every epoch regardless of\n",
      " |    performance.\n",
      " |  - Definition of 'best'; which quantity to monitor and whether it should be\n",
      " |    maximized or minimized.\n",
      " |  - The frequency it should save at. Currently, the callback supports saving\n",
      " |    at the end of every epoch, or after a fixed number of training batches.\n",
      " |  - Whether only weights are saved, or the whole model is saved.\n",
      " |  \n",
      " |  Note: If you get `WARNING:tensorflow:Can save best model only with <name>\n",
      " |  available, skipping` see the description of the `monitor` argument for\n",
      " |  details on how to get this right.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  model.compile(loss=..., optimizer=...,\n",
      " |                metrics=['accuracy'])\n",
      " |  \n",
      " |  EPOCHS = 10\n",
      " |  checkpoint_filepath = '/tmp/checkpoint'\n",
      " |  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
      " |      filepath=checkpoint_filepath,\n",
      " |      save_weights_only=True,\n",
      " |      monitor='val_accuracy',\n",
      " |      mode='max',\n",
      " |      save_best_only=True)\n",
      " |  \n",
      " |  # Model weights are saved at the end of every epoch, if it's the best seen\n",
      " |  # so far.\n",
      " |  model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])\n",
      " |  \n",
      " |  # The model weights (that are considered the best) are loaded into the\n",
      " |  # model.\n",
      " |  model.load_weights(checkpoint_filepath)\n",
      " |  ```\n",
      " |  \n",
      " |  Args:\n",
      " |      filepath: string or `PathLike`, path to save the model file. e.g.\n",
      " |        filepath = os.path.join(working_dir, 'ckpt', file_name). `filepath`\n",
      " |        can contain named formatting options, which will be filled the value\n",
      " |        of `epoch` and keys in `logs` (passed in `on_epoch_end`). For example:\n",
      " |        if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`, then the\n",
      " |        model checkpoints will be saved with the epoch number and the\n",
      " |        validation loss in the filename. The directory of the filepath should\n",
      " |        not be reused by any other callbacks to avoid conflicts.\n",
      " |      monitor: The metric name to monitor. Typically the metrics are set by\n",
      " |        the `Model.compile` method. Note:\n",
      " |  \n",
      " |        * Prefix the name with `\"val_`\" to monitor validation metrics.\n",
      " |        * Use `\"loss\"` or \"`val_loss`\" to monitor the model's total loss.\n",
      " |        * If you specify metrics as strings, like `\"accuracy\"`, pass the same\n",
      " |          string (with or without the `\"val_\"` prefix).\n",
      " |        * If you pass `metrics.Metric` objects, `monitor` should be set to\n",
      " |          `metric.name`\n",
      " |        * If you're not sure about the metric names you can check the contents\n",
      " |          of the `history.history` dictionary returned by\n",
      " |          `history = model.fit()`\n",
      " |        * Multi-output models set additional prefixes on the metric names.\n",
      " |  \n",
      " |      verbose: Verbosity mode, 0 or 1. Mode 0 is silent, and mode 1\n",
      " |        displays messages when the callback takes an action.\n",
      " |      save_best_only: if `save_best_only=True`, it only saves when the model\n",
      " |        is considered the \"best\" and the latest best model according to the\n",
      " |        quantity monitored will not be overwritten. If `filepath` doesn't\n",
      " |        contain formatting options like `{epoch}` then `filepath` will be\n",
      " |        overwritten by each new better model.\n",
      " |      mode: one of {'auto', 'min', 'max'}. If `save_best_only=True`, the\n",
      " |        decision to overwrite the current save file is made based on either\n",
      " |        the maximization or the minimization of the monitored quantity.\n",
      " |        For `val_acc`, this should be `max`, for `val_loss` this should be\n",
      " |        `min`, etc. In `auto` mode, the mode is set to `max` if the quantities\n",
      " |        monitored are 'acc' or start with 'fmeasure' and are set to `min` for\n",
      " |        the rest of the quantities.\n",
      " |      save_weights_only: if True, then only the model's weights will be saved\n",
      " |        (`model.save_weights(filepath)`), else the full model is saved\n",
      " |        (`model.save(filepath)`).\n",
      " |      save_freq: `'epoch'` or integer. When using `'epoch'`, the callback\n",
      " |        saves the model after each epoch. When using integer, the callback\n",
      " |        saves the model at end of this many batches. If the `Model` is\n",
      " |        compiled with `steps_per_execution=N`, then the saving criteria will\n",
      " |        be checked every Nth batch. Note that if the saving isn't aligned to\n",
      " |        epochs, the monitored metric may potentially be less reliable (it\n",
      " |        could reflect as little as 1 batch, since the metrics get reset every\n",
      " |        epoch). Defaults to `'epoch'`.\n",
      " |      options: Optional `tf.train.CheckpointOptions` object if\n",
      " |        `save_weights_only` is true or optional `tf.saved_model.SaveOptions`\n",
      " |        object if `save_weights_only` is false.\n",
      " |      initial_value_threshold: Floating point initial \"best\" value of the\n",
      " |        metric to be monitored. Only applies if `save_best_value=True`. Only\n",
      " |        overwrites the model weights already saved if the performance of\n",
      " |        current model is better than this value.\n",
      " |      **kwargs: Additional arguments for backwards compatibility. Possible key\n",
      " |        is `period`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ModelCheckpoint\n",
      " |      Callback\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, filepath, monitor: str = 'val_loss', verbose: int = 0, save_best_only: bool = False, save_weights_only: bool = False, mode: str = 'auto', save_freq='epoch', options=None, initial_value_threshold=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  on_epoch_begin(self, epoch, logs=None)\n",
      " |      Called at the start of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should\n",
      " |      only be called during TRAIN mode.\n",
      " |      \n",
      " |      Args:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this\n",
      " |            method but that may change in the future.\n",
      " |  \n",
      " |  on_epoch_end(self, epoch, logs=None)\n",
      " |      Called at the end of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should\n",
      " |      only be called during TRAIN mode.\n",
      " |      \n",
      " |      Args:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict, metric results for this training epoch, and for the\n",
      " |            validation epoch if validation is performed. Validation result\n",
      " |            keys are prefixed with `val_`. For training epoch, the values of\n",
      " |            the `Model`'s metrics are returned. Example:\n",
      " |            `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  on_train_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every\n",
      " |      `N` batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_train_begin(self, logs=None)\n",
      " |      Called at the beginning of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this\n",
      " |            method but that may change in the future.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Callback:\n",
      " |  \n",
      " |  on_batch_begin(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_begin`.\n",
      " |  \n",
      " |  on_batch_end(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_end`.\n",
      " |  \n",
      " |  on_predict_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every\n",
      " |      `N` batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this\n",
      " |            method but that may change in the future.\n",
      " |  \n",
      " |  on_predict_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every\n",
      " |      `N` batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_predict_begin(self, logs=None)\n",
      " |      Called at the beginning of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this\n",
      " |            method but that may change in the future.\n",
      " |  \n",
      " |  on_predict_end(self, logs=None)\n",
      " |      Called at the end of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this\n",
      " |            method but that may change in the future.\n",
      " |  \n",
      " |  on_test_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the beginning of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every\n",
      " |      `N` batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this\n",
      " |            method but that may change in the future.\n",
      " |  \n",
      " |  on_test_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the end of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every\n",
      " |      `N` batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_test_begin(self, logs=None)\n",
      " |      Called at the beginning of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this\n",
      " |            method but that may change in the future.\n",
      " |  \n",
      " |  on_test_end(self, logs=None)\n",
      " |      Called at the end of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently the output of the last call to\n",
      " |            `on_test_batch_end()` is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every\n",
      " |      `N` batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this\n",
      " |            method but that may change in the future.\n",
      " |  \n",
      " |  on_train_end(self, logs=None)\n",
      " |      Called at the end of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently the output of the last call to\n",
      " |            `on_epoch_end()` is passed to this argument for this method but\n",
      " |            that may change in the future.\n",
      " |  \n",
      " |  set_model(self, model)\n",
      " |  \n",
      " |  set_params(self, params)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Callback:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(keras.callbacks.ModelCheckpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f8fc70",
   "metadata": {},
   "source": [
    "## main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfe6a63d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating descriptors ...\n",
      "mkdir: best_model: File exists\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f961628c3b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f961628c3b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f961628c3b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      " 1/32 [..............................] - ETA: 17s - loss: 0.6944 - mse: 0.2522WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7f9608e6d050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7f9608e6d050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7f9608e6d050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f9606cbf050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f9606cbf050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f9606cbf050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model/convnetClassifier_from_scratch.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model/convnetClassifier_from_scratch.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/32 [========>.....................] - ETA: 1s - loss: 0.7395 - mse: 0.2605 WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7f9606cbf9e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7f9606cbf9e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7f9606cbf9e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f960e4e9830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f960e4e9830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f960e4e9830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model/convnetClassifier_from_scratch.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model/convnetClassifier_from_scratch.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/32 [=================>............] - ETA: 0s - loss: 0.7222 - mse: 0.2586WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f9605f499e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f9605f499e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f9605f499e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.7519 - mse: 0.2591 - val_loss: 0.7847 - val_mse: 0.2572\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.7194 - mse: 0.2589 - val_loss: 0.7566 - val_mse: 0.2572\n",
      "Epoch 3/10\n",
      "11/32 [=========>....................] - ETA: 0s - loss: 0.6611 - mse: 0.2600WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7f9605f5a200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7f9605f5a200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7f9605f5a200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f9605caed40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f9605caed40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f9605caed40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model/convnetClassifier_from_scratch.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model/convnetClassifier_from_scratch.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 44ms/step - loss: 0.6924 - mse: 0.2587 - val_loss: 0.7280 - val_mse: 0.2573\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.6676 - mse: 0.2592 - val_loss: 0.6977 - val_mse: 0.2577\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.6425 - mse: 0.2597 - val_loss: 0.6652 - val_mse: 0.2583\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.6177 - mse: 0.2604 - val_loss: 0.6335 - val_mse: 0.2592\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.5936 - mse: 0.2612 - val_loss: 0.6014 - val_mse: 0.2602\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.5702 - mse: 0.2623 - val_loss: 0.5685 - val_mse: 0.2614\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.5476 - mse: 0.2636 - val_loss: 0.5400 - val_mse: 0.2623\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.5257 - mse: 0.2652 - val_loss: 0.5222 - val_mse: 0.2646\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9606ceb8c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9606ceb8c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9606ceb8c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "mkdir: png: File exists\n"
     ]
    }
   ],
   "source": [
    " def main():\n",
    " \n",
    "    nn = NeuralNetwork(\n",
    "                        hidden_layer_sizes = eval(confParser['neural net']['hidden_layer_sizes']),\n",
    "                        learning_rate_init = eval(confParser['neural net']['learning_rate_init']),\n",
    "                        n_iter_no_change   = eval(confParser['neural net']['n_iter_no_change']),\n",
    "                        tol                = eval(confParser['neural net']['tol']),\n",
    "                        max_iter           = eval(confParser['neural net']['max_iter']),\n",
    "                        alpha              = eval(confParser['neural net']['alpha']),\n",
    "                        hidden_layer_size  = eval(confParser['neural net']['hidden_layer_size']),\n",
    "                        fully_connected    = eval(confParser['neural net']['fully_connected']),\n",
    "                        implementation     = eval(confParser['neural net']['implementation']),\n",
    "                        cnn                = eval(confParser['neural net']['cnn']),\n",
    "                        n_channels         = eval(confParser['neural net']['n_channels']),\n",
    "                        kernel_size        = eval(confParser['neural net']['kernel_size']),\n",
    "                        activation         = eval(confParser['neural net']['activation']),\n",
    "                        number_hidden_layers = eval(confParser['neural net']['number_hidden_layers']),\n",
    "                        verbose            = True \n",
    "                    )\n",
    "    \n",
    "    nn.Parse( path  = confParser['neural net']['input_path'],\n",
    "              nruns = eval(confParser['neural net']['nruns']))\n",
    "\n",
    "    nn.Combine() #--- concat. descriptors\n",
    "    \n",
    "#     tmp = nn.df_combined.set_index('id')\n",
    "#    display(nn.perAtomData.iloc[:13])\n",
    "#    nn.PrintDensityMap(6,'density6.xyz')\n",
    "#    nn.PCA()\n",
    "#    nn.Spectra()\n",
    "\n",
    "\n",
    "    #--- classifier\n",
    "    if eval(confParser['neural net']['classification']):\n",
    "        nn.TrainClassifier(np.c_[nn.perAtomData.defect_label].astype(int))\n",
    "#        nn.Test(np.c_[nn.perAtomData.defect_label].astype(int))\n",
    "    \n",
    "    \n",
    "    #--- filter data\n",
    "    train_type = int(eval(confParser['neural net']['train_type']))\n",
    "    \n",
    "    filtr      = {\n",
    "                     0: nn.perAtomData.defect_label == 1.0, #--- only non-crystalline atoms\n",
    "                     1: np.ones(len(nn.perAtomData),dtype=bool), #--- every atom\n",
    "                    }[train_type]\n",
    "    stratify   = {\n",
    "                    0:None,\n",
    "                    1:np.c_[nn.perAtomData.defect_label].astype(int), #--- ensure same ratio in test and training\n",
    "                    }[train_type]\n",
    "\n",
    "\n",
    "    \n",
    "    #--- train regressor\n",
    "    if eval(confParser['neural net']['regression']):\n",
    "\n",
    "        nn.TrainRegressor(  stratify=stratify,\n",
    "                            y=np.c_[nn.perAtomData[filtr]['ux uy uz'.split()]],\n",
    "                            printOvito = True,\n",
    "                            filtr=filtr,\n",
    "                         )\n",
    "    return nn\n",
    "\n",
    "!rm -r png;mkdir png\n",
    "data = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864aefa4",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b74daa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[279.  61.]\n",
      " [  1.   1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "legend = utl.Legends()\n",
    "legend.Set(fontsize=14,bbox_to_anchor=(1.5, 0.3, 0.5, 0.5))\n",
    "symbols = utl.Symbols()\n",
    "\n",
    "fp = ['confusion.txt', 'val_loss_classification.txt'][0]\n",
    "data = np.loadtxt('png/%s'%(fp))\n",
    "ax = utl.PltErr(None, None, Plot=False )\n",
    "if fp == 'confusion.txt':\n",
    "    accuracy_crystals = data[0,0]/np.sum(data[0,:])\n",
    "    accuracy_defects = data[1,1]/np.sum(data[1,:])\n",
    "    print(data)\n",
    "    utl.PltErr(accuracy_crystals, accuracy_defects,\n",
    "       attrs=symbols.GetAttrs(count=0,nevery=800,\n",
    "            ), \n",
    "               Plot=False,\n",
    "               ax=ax,\n",
    "               )\n",
    "else:\n",
    "    epoch = data[:,0]\n",
    "    loss = data[:,1]\n",
    "    val_loss = data[:,2]\n",
    "\n",
    "    utl.PltErr(epoch, val_loss,\n",
    "       attrs=symbols.GetAttrs(count=0,nevery=10,\n",
    "            ), \n",
    "               Plot=False,\n",
    "               ax=ax,\n",
    "               )\n",
    "    \n",
    "ax = utl.PltErr(None, None,\n",
    "yscale='log',xscale='log',\n",
    "xstr='epoch',ystr='validation loss',\n",
    "#                     ylim=(1e-1,1e1),\n",
    "ax=ax,\n",
    "# legend=legend.Get(),\n",
    "title='png/training_loss.png',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f1a5a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[312.  28.]\n",
      " [  1.   2.]]\n",
      "[[313.  27.]\n",
      " [  0.   3.]]\n",
      "[[300.  40.]\n",
      " [  0.   3.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    if eval(confParser['flags']['RemoteMachine']):\n",
    "        return\n",
    "    \n",
    "\n",
    "    \n",
    "    #--- ann\n",
    "    number_hidden_layers  = dict(zip(range(4),[1]))\n",
    "    hidden_layer_size     = dict(zip(range(4),[1]))\n",
    "    n_channels            = dict(zip(range(4),[1]))\n",
    "    activations           = dict(zip(range(20),['relu']))\n",
    "#     string[ inums ] = \"\\t\\'5\\':\\'neuralNet/20x20/ann/classifier/layer%s/channel%s/activation%s/layer_size%s\\',\\n\" % (key_n,key_c,key_a,key_h) #--- change job name\n",
    "    \n",
    "    #--- cnn\n",
    "#     number_hidden_layers  = dict(zip(range(4),[1,2,3]))\n",
    "#     hidden_layer_size     = dict(zip(range(4),[1]))\n",
    "#     n_channels            = dict(zip(range(4),[8,16,32,64]))\n",
    "#     activations           = dict(zip(range(20),['linear']))\n",
    "\n",
    "    runs = [0,1,2]\n",
    "    \n",
    "    legend = utl.Legends()\n",
    "    legend.Set(fontsize=14,bbox_to_anchor=(1.5, 0.3, 0.5, 0.5))\n",
    "    symbols = utl.Symbols()\n",
    "    \n",
    "    nphi = len(number_hidden_layers)\n",
    "    #---\n",
    "    count = 0\n",
    "    ax = utl.PltErr(None, None, Plot=False )\n",
    "    for key_n in number_hidden_layers:\n",
    "        number_hidden_layer = number_hidden_layers[key_n]\n",
    "#         if number_hidden_layer != 2:\n",
    "#             continue\n",
    "        for key_c in n_channels:\n",
    "            n_channel = n_channels[key_c]\n",
    "#             if n_channel != 16:\n",
    "#                 continue\n",
    "            for key_a in activations:\n",
    "                activation = activations[key_a]\n",
    "                for key_h in hidden_layer_size:\n",
    "                    nsize = hidden_layer_size[key_h]\n",
    "\n",
    "        #---\t\n",
    "#                    path = 'neuralNet/20x20/cnn/classifier/layer%s/channel%s/activation%s/layer_size%s'%(key_n,key_c,key_a,key_h) #--- change job name\n",
    "                    path = 'test' #--- change job name\n",
    "                    fp = ['confusion.txt', 'val_loss_classification.txt'][0]\n",
    "                    for irun in runs:\n",
    "                        try:\n",
    "                            data = np.loadtxt('%s/Run%s/png/%s'%(path,irun,fp))\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                        if fp == 'confusion.txt':\n",
    "                            accuracy_crystals = data[0,0]/np.sum(data[0,:])\n",
    "                            accuracy_defects = data[1,1]/np.sum(data[1,:])\n",
    "                            print(data)\n",
    "                            utl.PltErr(accuracy_crystals, accuracy_defects,\n",
    "                               attrs=symbols.GetAttrs(count=count%7,nevery=800,\n",
    "                                    label='%s layers, %s channels, act. %s'%(number_hidden_layer,n_channel,activation)), \n",
    "                                       Plot=False,\n",
    "                                       ax=ax,\n",
    "                                       )\n",
    "                        else:\n",
    "                            epoch = data[:,0]\n",
    "                            loss = data[:,1]\n",
    "                            val_loss = data[:,2]\n",
    "\n",
    "                            utl.PltErr(epoch, val_loss,\n",
    "                               attrs=symbols.GetAttrs(count=count%7,nevery=800,\n",
    "                                    label='%s layers, %s channels, act. %s'%(number_hidden_layer,n_channel,activation)), \n",
    "                                       Plot=False,\n",
    "                                       ax=ax,\n",
    "                                       )\n",
    "                    count += 1\n",
    "    ax = utl.PltErr(None, None,\n",
    "#                         yscale='log',xscale='log',\n",
    "                       xstr='epoch',ystr='validation loss',\n",
    "#                     ylim=(1e-1,1e1),\n",
    "                    ax=ax,\n",
    "                    legend=legend.Get(),\n",
    "                       title='png/training_loss.png',\n",
    "                   )\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f506974a",
   "metadata": {},
   "source": [
    "## test example: 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1be92045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.725966 1.725967]]\n",
      "(3, 441)\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# y=np.c_[[1.725966,1.725967],\n",
    "#             [-1.725966,1.725967],\n",
    "#             [-1.725966,-1.725967],\n",
    "#             [1.725966,-1.725967],\n",
    "#            ].T\n",
    "\n",
    "# X=np.concatenate([list(map(lambda x:np.load('png/descriptor%s.npy'%x).flatten(),range(4)))],axis=1)\n",
    "\n",
    "# #--- zscore\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X)\n",
    "# X_transfrmd = scaler.transform( X )\n",
    "\n",
    "# X_train_transfrmd, X_test_transfrmd, y_train, y_test = train_test_split(X_transfrmd, y, test_size=0.25)\n",
    "# print(y_test)\n",
    "\n",
    "\n",
    "# print(X_train_transfrmd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69377834",
   "metadata": {},
   "source": [
    "### fully connected in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c56d633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.79024675\n",
      "Iteration 2, loss = 4.79153813\n",
      "Iteration 3, loss = 0.48272480\n",
      "Iteration 4, loss = 0.84242732\n",
      "Iteration 5, loss = 1.20200108\n",
      "Iteration 6, loss = 0.50029766\n",
      "Iteration 7, loss = 0.08083888\n",
      "Iteration 8, loss = 0.31471365\n",
      "Iteration 9, loss = 0.39602049\n",
      "Iteration 10, loss = 0.14417737\n",
      "Iteration 11, loss = 0.05911270\n",
      "Iteration 12, loss = 0.17784119\n",
      "Iteration 13, loss = 0.18843745\n",
      "Iteration 14, loss = 0.08671426\n",
      "Iteration 15, loss = 0.05304450\n",
      "Iteration 16, loss = 0.09624741\n",
      "Iteration 17, loss = 0.09671026\n",
      "Iteration 18, loss = 0.05631733\n",
      "Iteration 19, loss = 0.05017827\n",
      "Iteration 20, loss = 0.06458879\n",
      "Iteration 21, loss = 0.05694448\n",
      "Iteration 22, loss = 0.04181389\n",
      "Iteration 23, loss = 0.04398522\n",
      "Iteration 24, loss = 0.04448364\n",
      "Iteration 25, loss = 0.03786413\n",
      "Iteration 26, loss = 0.03483561\n",
      "Iteration 27, loss = 0.03725800\n",
      "Iteration 28, loss = 0.03546887\n",
      "Iteration 29, loss = 0.03089711\n",
      "Iteration 30, loss = 0.03086940\n",
      "Iteration 31, loss = 0.03225031\n",
      "Iteration 32, loss = 0.02917275\n",
      "Iteration 33, loss = 0.02726652\n",
      "Iteration 34, loss = 0.02892336\n",
      "Iteration 35, loss = 0.02808117\n",
      "Iteration 36, loss = 0.02537011\n",
      "Iteration 37, loss = 0.02600391\n",
      "Iteration 38, loss = 0.02698854\n",
      "Iteration 39, loss = 0.02494826\n",
      "Iteration 40, loss = 0.02394280\n",
      "Iteration 41, loss = 0.02572275\n",
      "Iteration 42, loss = 0.02497078\n",
      "Iteration 43, loss = 0.02293285\n",
      "Iteration 44, loss = 0.02413425\n",
      "Iteration 45, loss = 0.02455046\n",
      "Iteration 46, loss = 0.02270986\n",
      "Iteration 47, loss = 0.02347172\n",
      "Iteration 48, loss = 0.02360856\n",
      "Iteration 49, loss = 0.02255866\n",
      "Iteration 50, loss = 0.02314975\n",
      "Iteration 51, loss = 0.02309027\n",
      "Iteration 52, loss = 0.02259666\n",
      "Iteration 53, loss = 0.02288285\n",
      "Iteration 54, loss = 0.02297684\n",
      "Iteration 55, loss = 0.02300910\n",
      "Iteration 56, loss = 0.02306715\n",
      "Iteration 57, loss = 0.02287899\n",
      "Iteration 58, loss = 0.02262910\n",
      "Iteration 59, loss = 0.02261209\n",
      "Iteration 60, loss = 0.02282631\n",
      "Iteration 61, loss = 0.02271234\n",
      "Iteration 62, loss = 0.02245436\n",
      "Iteration 63, loss = 0.02261107\n",
      "Iteration 64, loss = 0.02257859\n",
      "Iteration 65, loss = 0.02275319\n",
      "Iteration 66, loss = 0.02301587\n",
      "Iteration 67, loss = 0.02290518\n",
      "Iteration 68, loss = 0.02263656\n",
      "Iteration 69, loss = 0.02244420\n",
      "Iteration 70, loss = 0.02264114\n",
      "Iteration 71, loss = 0.02285952\n",
      "Iteration 72, loss = 0.02284724\n",
      "Iteration 73, loss = 0.02254051\n",
      "Iteration 74, loss = 0.02253781\n",
      "Iteration 75, loss = 0.02259026\n",
      "Iteration 76, loss = 0.02266804\n",
      "Iteration 77, loss = 0.02250421\n",
      "Iteration 78, loss = 0.02246337\n",
      "Iteration 79, loss = 0.02244526\n",
      "Iteration 80, loss = 0.02255226\n",
      "Iteration 81, loss = 0.02250348\n",
      "Iteration 82, loss = 0.02240397\n",
      "Iteration 83, loss = 0.02242926\n",
      "Iteration 84, loss = 0.02245178\n",
      "Iteration 85, loss = 0.02249455\n",
      "Iteration 86, loss = 0.02250251\n",
      "Iteration 87, loss = 0.02240619\n",
      "Iteration 88, loss = 0.02238247\n",
      "Iteration 89, loss = 0.02239953\n",
      "Iteration 90, loss = 0.02242186\n",
      "Iteration 91, loss = 0.02243013\n",
      "Iteration 92, loss = 0.02237696\n",
      "Iteration 93, loss = 0.02237238\n",
      "Iteration 94, loss = 0.02236637\n",
      "Iteration 95, loss = 0.02241898\n",
      "Iteration 96, loss = 0.02244555\n",
      "Iteration 97, loss = 0.02243013\n",
      "Iteration 98, loss = 0.02236881\n",
      "Iteration 99, loss = 0.02236044\n",
      "Iteration 100, loss = 0.02237842\n",
      "mkdir: png: File exists\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASwAAAEcCAYAAAB9B4nYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZAklEQVR4nO3df5hdVX3v8ffHQECRG5CAVCqE8iO19VFohwrSWxJzAeU2oGDUKvKjaKy/kNZ7RVsR0KKAXPllAeepGAkoopFSrghYzIAgFCairaWA4JOi/NBEYOSKQAjf+8fah5ycnDPn7Jl9zp4183k9z3n2nL3XnPnOeTKf7L3O2mspIjAzy8EL6i7AzKxXDiwzy4YDy8yy4cAys2w4sMwsGw4sM8uGA2uKk7S07hpy4Pepdzm/Vw6sqW/K/eOStLjuGtqYcu8T+L3qVa/vkwPLJmIq/hFOVX6vetPT+ySPdH8+3RdvvfXW795zzz3rLmcja9asYfvtt6+7jI2MjY0xZ86cusvYyFR8n8DvVa9WrVr1JHAZcHVEXN2pnQOrydDQUIyOjtZdhtmMI2lVRAx1a+dLQjPLhgPLzLLhwDKzbDiwzCwbDiwzy4YDy8yy4cAys2w4sMwsGw4sM8uGA8vMsuHAMrNsOLDMLBsOLDPLhgPLzLLhwDKzbDiwzCwbDiwzy4YDy8yy4cAys2w4sMwsGw4sM8uGA8vMsuHAMrNsOLDMLBsOLDPLhgPLzLLhwDKzbDiwzCwb2QeWpO0kvUvSlZLuk/RbSWOSbpZ0nKTsf0czSzaru4AKLAEuBB4GVgIPAC8FDgf+EXiDpCUREfWVaGZVmA6BdS9wKPCtiHiusVPS3wK3A0eQwmtFPeWZWVWyv1yKiO9GxNXNYVXsfwS4qHi6YOCFmVnlsg+sLtYV22drrcLMKjFtA0vSZsBRxdNr66zFzKoxbQMLOB14JXBNRFxXdzFmNnnTMrAkHQ98GLgbeGeXtksljUoaXbNmzUDqM7NNzG38HRaPpe0aabp92i/pA8D5wF3AoqLzvSdDQ0MxOjrat9rMrD1JqyJiqFu7aXWGJekEUlj9GFhYJqzMbOqbNoEl6UTgbOCHpLD6Zb0VmVnVpkVgSTqJ1Mm+inQZuLbmksysD7If6S7paOCTwHrge8DxklqbrY6IZQMuzcwqln1gAbsW21nACR3a3AgsG0QxZtY/2V8SRsQpEaEujwV112nWT7feCp/5TNpOZ9PhDMtsRrv1Vli0CJ55BmbPhhtugP32q7uq/sj+DMtsphsZSWG1fn3ajozUXVH/OLDMMrdgQTqzmjUrbRcsqLui/vEloVnm9tsvXQaOjKSwmq6XgzBOYEk6qtOxbiLikol+r5mVt99+0zuoGsY7w1oGNN9oqJbn7TTaOLDMrHLjBdaxbfYdDiwmjWsaAR4BdgQWAn8G/DNwZbUlmpklHQMrIr7c/FzSIcDrgcMi4uqW5qdKOgy4gg3TEpuZVarMp4R/B1zZJqwAiIirgH8CTqqgLjOzTZQJrFcD93Vpcx/wqomXY2bWWZnAeoYUWuN5NRsWfjAzq1SZwLoBOETSB9QyHYKSDwJvAP6lygLNzBrKDBz9KOnTwHOBEyTdDPyCtMryn5JmTXi0aGdmVrmeAysi7pe0L3AB8D+A32tp8h3g/RHx0wrrMzN7XqlbcyLiPuAgSTsBewNzgDHgzoh4sA/1mZk9b0L3Ehbh5IAys4GaUGBJ+n3gFcCLI2J5tSWZmbVXanoZSXtJGgX+A/gGTdMOSzpA0pOSFldboplZ0nNgSdqTdP/gfNInhd9uaXIT6VPCN1dVnJlZszJnWCcDs4HXRMTfAHc0H4y0hPStwD7VlWdmtkGZwFoEfDMi7hqnzc+Al02uJDOz9soE1rbAz7u0EekszMyscmUC6xfA7l3a/CHpLMvMrHJlAuu7wGJJ89sdlLQP6bLxuioKMzNrVSawPgM8C9wk6b0UfVWS/rB4fjXwBHBW5VWamVHuXsJ7JB0BfBX4fLFbwL8V28eBwyPigaqLNDOD8vcSXitpV+BoYF9gO9K9hLcBX4qIR6sv0cwsKX1rTkQ8Tho4em7l1ZiZjaPMSPeLJR3apc2fS7p48mUNlqTFkobHxsbqLsVsppojabjbrX1lOt2PAfbq0ubVpMvFrETE1RGxdM6cOXWXYjZTjUXE0k6L3DSUuvm5B1sA6yt+TTMzoHxgdVz5WdIWpMVUH5lURWZmHYzb6S6pdbrjv5bUbkXoWcD2pDMsL6RqZn3R7VPCF7DhrCpI463Upt064N9JK+v8fWXVmZk1GTewImJe42tJzwFnR8Qn+12UmVk7ZcZhLQRW96kOM7Ouytyac2M/CzEz66bMwNGPS1onqe0EfZJ2kvSMpBOrK8/MbIMywxoWAyMR8VC7g8XSXyuBN1ZQl5nZJsoE1u7AeNMjUxzvNsmfmdmElAmsFwJPdmnzFLD1xMsxM+usTGD9nDSlzHj2xStCm1mflAmsa4E/k/TWdgclvQ04gE3XKzQzq0SZcVhnAO8AvlKE1rWks6mdgDcAh5IWUj296iLNzKDcOKwHJR0MfJ30SeBhTYdFGlS6JCK6LQVmZjYhZadIHi2WrF9M6q/ahjSX+23A1RGxruoCzcwaJjJF8jrgm8XDzGxgqp7Az8ysbzqeYUk6qvjyyoh4oul5VxFxyaQrMzNrMd4l4TLSHFi3kRZIbTwfj4o2Diwzq9x4gfWXpPB5uHjebqZRM7OB6RhYEbGs5fmX+16Nmdk43OluZtlwYJlZNsb7lLB1xZxeRUTsNsHvNTPraLxO9+YVcxpmA79TfL0eWAvMJS3zBamD/pkqCzQza+h4SRgR8yJi18aDtAz9g6RhDguBLSPid4AtgdcB/0qaguZV/S/bzGaiMn1Yp5HuHVwQETdGxHqAiFgfESOkEHtJ0c7MrHJlAutNwFUR0faSLyKeAq4CDq+isLIk/a6kiyU9JOlpSaslnSNp2zrqMbPqlbn5eTtg8y5tNi/aDZSk3YDvAzuQQvNu4E+ADwGvl7R/RPxq0HWZWbXKnGHdD7xZ0px2B4szmTcDE/10cTIuIIXV8RHxxoj4aES8DjgbmI8vU82mhTKBdRHwMuB2SUdJmifphcX2aFKn+47AP/Sj0E6Ks6uDSBMItv7sk4HfAO+UtNUg6zKz6pWZcfTzkvYAPgh8qU0TAedHxAVVFdejhcX2+oh4rvlAMcvELaRA2xe4YcC1mVmFSo10j4gPAfsDFwN3ki7/7gS+CPxpcXzQ5hfbezsc/0mx3XMAtZhZH01kxtFbgVv7UMtENfrUxjocb+zfpt1BSUuBpQA777xzpYWZWc/mShptej4cEcOtjUoH1nRTvCnDAENDQ93m+zKz/lgbEUPdGpW++VnSYkmXS/qRpPua9r9C0kck7VT2NSepcQbV9tPLpv2P978UM+unns+wJIk06+iRxa7fkpavb3gM+DSp8/2MiurrxT3FtlMf1R7FtlMfl5lloswZ1vuAd5I+IXwJcFbzwYh4BLgF+J+VVdeblcX2IEkb/T6StiZ9SPAk6R5IM8tYmcA6DvgR8O6IGKP9/O4/AXatorBeRcT9wPXAPOD9LYdPBbYClkfEbwZZl5lVr0yn+3zgCxExXsf0L4HtJ1fShLyPdGvOeZIWAf8JvIY0Rute4O9qqMnMKlbmDOtZ0lQy49kJ+H8TL2diirOsIVIf22uADwO7AecC+/o+QrPpocwZ1l3AAklqd5YlqTEv1p1VFVdGRPwMr+xjNq2VOcNaDvw+cHabzu1ZwOdI9xouq6w6M7MmZc6wvgAcChwPLCEtroqkb5Du03sZab6sy6ou0swMSpxhFTOM/jnwSWAL0rgnkSbsexHwKVKQmZn1RalbcyLiWeAUSaeSAms70kjzuxtTJpuZ9UuZke7rgcsj4h1Fp/s93b7HzKxKZTrdnwAe6FchZmbdlAmsO4E/6FchZmbdlAmsM4BDJB3Yr2LMzMZTptN9B+Ba4NuS/gm4A3iENvcURsQllVRnZtakTGAtI4VTYyhDY/3B5sBS8dyBZWaVKxNYvu3FzGpVZtWcL/ezEDOzbkpPkWxmVpfSi1BIejHwJmBv0nzpY6QhD1dGxMCnljGzmaNUYElaQloBehtSB3tDAOdIek9EfKO68szMNihza86BwFeB50ifAo6QhjXsSJrZ8+3AVyU9HhH/Un2pZjbTlenD+gTwNGkGz2Mj4ssRcV2xPQZ4LbCuaGdmg3DmmbBy5cb7Vq5M+6ehMoG1N/C1iPhBu4MRMQpcAfxRFYWZWQ/22Qfe8pYNobVyZXq+zz711tUnZfqwngYe7tLmoaKdmQ3CwoVwxRUppN77XrjwwvR84cK6K+uLMmdY3yOt8Tee/YGbJl6OmZW2cGEKq099Km2naVhBucA6EXiVpNMlbdV8QNJWks4EXgl8tMoCzayLlSvTmdVJJ6Vta5/WNFLmkvBE4N+A/w0slfQD4BfAS0n9VnNIZ1cnplXtnxcRcVw15ZrZRhp9Vo3LwIULN34+zZQJrGOavt6GtKRXqwOKR7MgrRptZlW7446Nw6nRp3XHHdMysDT+Qs5NDaVdJvpDIuK/Jvq9gzQ0NBSjo6N1l2E240haFRFD3dqVufk5i9Axs+nLNz+bWTYcWGaWDQcWIGmxpOGxsbG6SzGbqeZIGpa0eLxGPXe6zwTudDerR6+d7j7DMrNsOLDMLBsOLDPLRs+BJekdPbTZTNLZkyvJzKy9MmdYyyX9o6Qt2x2UtCvwfeD4SiozM2tRJrBuBP4SuEPSHzQfkPQW4AfAEHBOZdWZmTUpE1ivAz4FvAK4XdJxkraQNEya6/1ZYHFEfLgPdZqZ9R5YkZwMHEha2msY+BlpJobvAa+OiG/1pUozMybwKWFErATOJy3zNRdYC7w9Ih6quDYzs42UCqxiZtHLgNNI87dfDmwPrJJ0UB/qMzN7XplhDXuTVnj+C+A6YK+IeDtpPcKtgGsknSlpVl8qNbMZr8wZ1q3APODEiDgkItYCRMTlpCmSfwj8L+CWims0MwPKBdbDwH+PiM+2HoiI+4D9gPOA6bkgmpnVrsyc7ntHxOOdDkbEOuAESV6m3sz6osywhsd7bPd/J1yNmdk4fPOzmWWj50tCST/tsWlExG4TrMfMrKMyfVgvIK0x2Gob0iKqkMZmrZtkTWZmbZVZ5mtep2OSdid9QrgVcPDkyzIz21QlfVjFsIbDgZ2Ak6t4TTOzVpV1ukfEU8B3SCPhzcwqV/WnhM8CO1b8mmZmQIWBJWku8CbSlDNmZpUrM6zhE+O8xsuBw0ifFn6sgrqshOFhWLECjjgCli6tuxqz/ikzrOGULsd/Dfx9RJw58XKsrOFheM970tfXX5+2Di2brsoE1sIO+58DHgPujohnJ1+SlbFixabPHVg2XZUZh3VjPwuxiTniiA1nVo3nZtNVmTMsm4IaZ1Puw7KZIOvAkrQHacDqwcAewEtJl6e3AecU889Pe0uXOqhsZsg6sEjLjr0VuAu4BngUmA8cChwq6UMRcV6N9ZlZhXIPrGuBMyLizuadkg4gjbr/rKSvR8TDtVRnZpXKej6siFjWGlbF/huBEWA28NpB12Vm/ZF1YHXRmObGQy3MpolpGViSdgEWAU8CN9VcjplVJPc+rE1I2gK4DNgC+EhEPFZzSWZWkdrPsCStlhQlHpeO81qzgOXA/sDXgLN6+PlLJY1KGl2zZk11v5iZlTG38XdYPNoO1JkKZ1j3A0+VaP9Qu51FWF0KLAGuAI6MiHZTOm8kIoaBYYChoaGu7c2sL9ZGxFC3RrUHVkQsmuxrSNqcdBm4BPgKcFRErJ/s65rZ1FJ7YE2WpNmkM6rDgEuAYyPiuXqrMrN+qL0PazKKDvYrSWH1RRxWZtNa7mdYFwGHAGuBB4FPSGptMxIRIwOuy8z6IPfA2rXYzgU6zYgKadS7mWUu68CKiAV112Bmg5N1H5aZzSwOLDPLhgPLzLLhwDKzbDiwzCwbDiwzy4YDy8yy4cAys2w4sMwsGw4sM8uGA8vMsuHAMrNsOLDMLBsOLDPLhgPLzLLhwDKzbDiwzCwbDiwzy4YDy8yy4cAys2w4sMwsGw4sM8uGA8vMsuHAMrNsOLDMLBsOLDPLhgPLzLLhwDKzbCgi6q5hypC0BvivuutoMRdYW3cRLeYAY3UX0WIqvk/g96pXe0TEnG6NNhtEJbmIiO3rrqGVpNGIGKq7jmaShiNiad11NJuK7xP4veqVpOFe2vmS0Cbi6roLyIjfq9709D45sKy0iPAfYY/8XvWm1/fJgTX19XSqbH6fSsj2vXKnu5llw2dYZpYNB5aZZcOBlQlJe0g6UdJ3Jf1M0jOSfiHpKkkL666vDpJ+V9LFkh6S9LSk1ZLOkbRt3bVNFZK2k/QuSVdKuk/SbyWNSbpZ0nGSssoA92FlQtLlwFuBu4CbgUeB+cChwCzgQxFxXn0VDpak3YDvAzsAVwF3A38CLATuAfaPiF/VV+HUIOmvgAuBh4GVwAPAS4HDSYNaVwBLIpMgcGBlQtIxwI8i4s6W/QcA3wECmBcRD9dQ3sBJug44CDg+Is5v2v854K+BL0TEX9VV31Qh6XXAVsC3IuK5pv07ArcDLwfeHBEraiqxFAfWNCDpeuBAMvqHNxnF2dV9wGpgt5Y/xK1JZxMCdoiI39RSZAYk/S1wGvD5iPhg3fX0IqvrV+toXbF9ttYqBqfRZ3d9c1gBRMQTwC3Ai4B9B11YZrL7d+PAypykXYBFwJPATTWXMyjzi+29HY7/pNjuOYBasiRpM+Co4um1ddZShm9+zpikLYDLgC2Aj0TEYzWXNCiNu/o7zYLQ2L9N/0vJ1unAK4FrIuK6uovplc+wBqj42D1KPC4d57VmAcuB/YGvAWcN6vewvEk6Hvgw6ZPVd9ZcTik+wxqs+4GnSrR/qN3OIqwuBZYAVwBH5vKxdEUaZ1Cd5k9q7H+8/6XkRdIHgHNJw2MWRcSjNZdUigNrgCJi0WRfQ9LmpMvAJcBXgKMiYv1kXzcz9xTbTn1UexTbTn1cM5KkE4CzgR+TwuqX9VZUnoc1ZETSbNIZ1WHAJcCxrZ+SzQQe1lCepBNJ/VY/BA6MiKk242hP3IeViaKD/UpSWH2RGRpWABFxP3A9MA94f8vhU0kDJZc7rBJJJ5HCahXpzCrLsAKfYWVD0peAY0hzcV9AGtneaiQiRgZYVm3a3Jrzn8BrSGO07gVe61tzQNLRwDJgPXA+7T9ZXR0RywZY1oS5DysfuxbbucAnxmk30v9S6hcR90saAj4JvB44hHQpeC5w6gwa4tFN49/NLOCEDm1uJIXalOczLDPLhvuwzCwbDiwzy4YDy8yy4cAys2w4sMwsGw4sM8uGA8vMsuHAMhswSSOSPAByAhxYNqM4LPLmwDKzbDiwzCwbDizbhKQFxRTNp3Q4vlrS6qbn2xb7npb0xy1tXyBpZfF6PU3HK+kYSSsk/bRYqfjXkm6RdOQ43/MSSadJ+rGkJ4vVjX8k6XRJW0maV1wKHlC0b56KeqTpdTZ63vIzlhXH5022XpsYz9ZgkxYRj0n6C9KqPV+TtHex3BbAycACYFlELO/xJS8E/qN4vYeB7UizMSyXND8iTmpuLGlX0qrGu5DmfLqQ9J/xnqRFVS8iTZd8KmmKnl2KrxtW9/7bTr5em4SI8MOPjR6kgAnglA7HV5PmUGrd/5Hi+75aPF9ImofpLuBFJX7+bm32zQZuIK2lt1PLse8XP/djbb5vLrBl0/OR9M++488O0rxi7Y4tY8MK25Opd9wa/Oj88CWhVemzpDXu3ibpY6S5558G3hoRT/b6IpFmFG3d9wzwD6Srgufnxi8uQfcjTf17RpvvWxsRZRb+KK1MvTY5viS0ykRESDqKFB6fLna/JyL+vczrSNoZOJH0h74z8MKWJjs1fd1Y3fm6qGnK6JL12iQ4sKxSEbFG0k3A24BfkdZO7Jmk3wNuB7YFvkeau32MdGk5DziatHBswzbF9sHJ1D1RE6jXJsGBZe00zlQ6/fvYhg5r/kl6Gyms1pL6j84D3l3iZ/8NqdP62GiZZ7zo2D+6pX2jjqrOYoLxf+9WZeu1SXAflrXTmA/95a0HJO1OhwVMi2PDwBpgb9KnZu8qQqxXuxfbFW2OHdBm323F9mBJvfx7Xl/UOqvD8cdo/3vPAvZq075svTYJDixr527g18BhknZo7JT0QtIZ0yaKNRMvB14MHB0RPwfeTros/EKxyk0vVhfbBS2vfzDwrtbGEbGK9CnhXqR+pNa6tpO0ZdOuxko6O3f4+bcDO0s6qGX/x0nDISZVr02OA8s2ERHrSKvPzAHulPR5SReRVgzeGniozbedCfwxcHZEfLt4nQdJ457+G2l81uwefvwFwDPA1yVdKulMSdcA3wa+0eF7jgQeAD4taVTSWZL+j6R/JvVt7djU9oZi+81ioOnHWwa0nkW6LLyqGCj6OUm3Ae+j/YpEE6nXJqrucRV+TM0HaeXkjwL3k/4gHyCF0otoGYcFLCb9kd8BbN7mtT5XHD+3x5/9WuC7pMuzJ4CbgTcyzvgwUj/SGaRl7J8i9W39EDiNpjFgpOWuPg38lDRGapNxV8ChwGjxOr8inTnuQudxWKXqxeOwJvzwMl9mlg1fEppZNhxYZpYNB5aZZcOBZWbZcGCZWTYcWGaWDQeWmWXDgWVm2XBgmVk2HFhmlo3/D9aj703nMZ5uAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# #        pdb.set_trace()\n",
    "# #--- tune parameters\n",
    "\n",
    "# #--- train\n",
    "# mlp = MLPRegressor(random_state=1,\n",
    "#                     verbose=True,\n",
    "#                    n_iter_no_change=100000,\n",
    "#                     max_iter=100,#00,\n",
    "#                    hidden_layer_sizes=(1000,1000),\n",
    "# #                    shuffle=False,\n",
    "# #                     alpha=1e-1,\n",
    "\n",
    "#                   )\n",
    "# mlp.fit(X_train_transfrmd,y_train)\n",
    "\n",
    "# #--- validate\n",
    "# !mkdir png\n",
    "# utl.PltErr(range(len(mlp.loss_curve_)), mlp.loss_curve_,\n",
    "#            attrs={'fmt':'-'},\n",
    "#            yscale='log',xscale='log',\n",
    "# #           xlim=(1,self.max_iter[0]),\n",
    "#            xstr='epoch',ystr='loss',\n",
    "#            title='png/loss.png',\n",
    "#           )\n",
    "\n",
    "# # #         pdb.set_trace()\n",
    "# y_pred =mlp.predict(X_test_transfrmd)        \n",
    "# y_pred_train = mlp.predict(X_train_transfrmd)        \n",
    "# for idime, xstr in zip(range(2),'ux uy'.split()):\n",
    "#     ax = utl.PltErr(None,None,Plot=False)\n",
    "#     #\n",
    "#     utl.PltErr(y_test[:,idime],y_pred[:,idime],\n",
    "#                attrs={'fmt':'x','color':'red','zorder':10,'markersize':6},\n",
    "#                ax=ax,\n",
    "#                Plot = False,\n",
    "\n",
    "#               )\n",
    "#     #\n",
    "#     utl.PltErr(y_train[:,idime],y_pred_train[:,idime],\n",
    "#                attrs={'fmt':'.','color':'blue','zorder':1,'markersize':6},\n",
    "#                ax=ax,\n",
    "#                Plot = False,\n",
    "\n",
    "#               )\n",
    "#     #\n",
    "#     utl.PltErr(None,None,Plot=False,\n",
    "#                    title='png/scatter%s.png'%idime,\n",
    "#                     ax=ax,\n",
    "#                xstr='%s actual'%xstr,ystr='%s predicted'%xstr,\n",
    "#                xlim=(-3,3),ylim=(-3,3),\n",
    "#                    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1353cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp.best_loss_, mlp.loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ux,uy=mlp.predict(X_test_transfrmd)[0]\n",
    "# ax=utl.PltErr([0,ux],[0,uy],\n",
    "#               Plot=False\n",
    "#           )\n",
    "# utl.PltErr([0,y_test[0][0]],[0,y_test[0][1]],\n",
    "#            xlim=(-3,3),ylim=(-3,3),\n",
    "#             ax=ax\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ux,uy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce616c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = utl.PltErr(None,None,Plot=False)\n",
    "\n",
    "# for i in range(2):\n",
    "#     utl.PltErr(range(data.descriptors[0,:].shape[0]),data.descriptors[i,:],\n",
    "#               attrs={'fmt':'-'},#,'color':'C0'},\n",
    "#                xscale='log',yscale='log',\n",
    "#                ax=ax,\n",
    "#                Plot=False,\n",
    "#               )\n",
    "\n",
    "# utl.PltErr(range(data.descriptors[100,:].shape[0]),data.descriptors[100,:],\n",
    "#           attrs={'fmt':'-','color':'C0'},\n",
    "#            xscale='log',yscale='log',\n",
    "#            ax=ax,\n",
    "#            Plot=False,\n",
    "#           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5598bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.Spectra(nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d061978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPRegressor\n",
    "# from sklearn.datasets import make_regression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X, y = make_regression(n_samples=200, random_state=1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "#                                                     random_state=1)\n",
    "# regr = MLPRegressor(verbose=False,\n",
    "#                     random_state=1, \n",
    "# #                     learning_rate='adaptive',\n",
    "# #                    early_stopping=True, \n",
    "#                      n_iter_no_change=1, \n",
    "#                     tol=1e-2,\n",
    "#                      max_iter=10000000,\n",
    "# #                     solver='sgd',\n",
    "#                    ).fit(X_train, y_train)\n",
    "# regr.tol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5eecba",
   "metadata": {},
   "source": [
    "### fully connected in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a27a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #--- The network architecture\n",
    "# model = keras.Sequential([\n",
    "#     layers.Dense(512), #activation=\"relu\"),\n",
    "# #     layers.Dense(1000), #activation=\"relu\"),\n",
    "#     layers.Dense(2) #, activation=\"relu\")\n",
    "#     ])\n",
    "\n",
    "# #--- The compilation step\n",
    "# optimizer = tf.keras.optimizers.Adam() #learning_rate=1e-4)\n",
    "# model.compile( optimizer=optimizer,#\"rmsprop\",\n",
    "#                loss=\"mean_squared_error\",#\"sparse_categorical_crossentropy\",\n",
    "#                metrics=[\"mse\"]\n",
    "#              )\n",
    "\n",
    "# #--- Preparing the image data\n",
    "# # train_images = train_images.reshape((60000, 28 * 28))\n",
    "# # train_images = train_images.astype(\"float32\") / 255\n",
    "# # test_images = test_images.reshape((10000, 28 * 28))\n",
    "# # test_images = test_images.astype(\"float32\") / 255\n",
    "\n",
    "# #--- “Fitting” the model X_train_transfrmd,y_train\n",
    "# model.fit(X_train_transfrmd, y_train, \n",
    "#             validation_data=(X_test_transfrmd, y_test),\n",
    "\n",
    "#           epochs=100, verbose=False)#, batch_size=128)\n",
    "\n",
    "# loss = model.history.history['loss']\n",
    "# val_loss = model.history.history['val_loss']\n",
    "# #--- validate\n",
    "\n",
    "# ax = utl.PltErr(range(len(val_loss)), val_loss,\n",
    "#            attrs={'fmt':'-'}, Plot=False,\n",
    "#           )\n",
    "# utl.PltErr(range(len(loss)), loss,\n",
    "#            attrs={'fmt':'-'},\n",
    "#            ax=ax,\n",
    "#            yscale='log',xscale='log',\n",
    "#            xlim=(1,100),\n",
    "#            xstr='epoch',ystr='loss',\n",
    "#            title='png/loss.png',\n",
    "#           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262922c9",
   "metadata": {},
   "source": [
    "### cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f22b71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tf.random.set_random_seed(812)\n",
    "\n",
    "# shape=(300,300,1)\n",
    "# kernel_size = (3,3)\n",
    "# epochs = 1000\n",
    "# activation = ['linear','sigmoid','relu'][0]\n",
    "# padding='same'\n",
    "# filters = 1\n",
    "# #\n",
    "# ndime = y_train.shape[1]\n",
    "# n_train = X_train_transfrmd.shape[0]\n",
    "# n_test = X_test_transfrmd.shape[0]\n",
    "# assert shape[0]*shape[1]*shape[2] == X_train_transfrmd.shape[1]\n",
    "# inputs = keras.Input(shape=shape)\n",
    "# #\n",
    "# x = layers.Conv2D(filters=filters, kernel_size=kernel_size,activation=activation,padding=padding)(inputs)\n",
    "# # x = layers.AveragePooling2D(pool_size=2)(x)\n",
    "# # x = layers.Conv2D(filters=2*filters, kernel_size=kernel_size,activation=activation,padding=padding)(x)\n",
    "# # x = layers.AveragePooling2D(pool_size=2)(x)\n",
    "# # x = layers.Conv2D(filters=4*filters, kernel_size=kernel_size,activation=activation,padding=padding)(x)\n",
    "# # x = layers.AveragePooling2D(pool_size=2)(x)\n",
    "# # x = layers.Conv2D(filters=8*filters, kernel_size=kernel_size,activation=activation,padding=padding)(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# outputs = layers.Dense( ndime, activation=activation)(x)\n",
    "\n",
    "# #--- The network architecture\n",
    "# model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "# #--- The compilation step\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5,epsilon=1e-08)\n",
    "# model.compile( optimizer=optimizer,#\"rmsprop\",\n",
    "#                loss=\"mean_squared_error\",#\"sparse_categorical_crossentropy\",\n",
    "#                metrics=[\"mse\"]\n",
    "#              )\n",
    "\n",
    "# #--- save best model \n",
    "# callbacks=[keras.callbacks.ModelCheckpoint( filepath='png/convnet_from_scratch.keras',  \n",
    "#                                            monitor=\"val_loss\",\n",
    "#                                            save_freq=10,\n",
    "#                                             save_best_only=True)]\n",
    "\n",
    "# #--- “Fitting” the model X_train_transfrmd,y_train\n",
    "# X_train_reshaped = X_train_transfrmd.reshape((n_train,shape[0],shape[1],1))\n",
    "# X_test_reshaped = X_test_transfrmd.reshape((n_test,shape[0],shape[1],1))\n",
    "# model.fit(X_train_reshaped, y_train, \n",
    "#             validation_data=(X_test_reshaped, y_test),\n",
    "#             #callbacks=callbacks,\n",
    "#           epochs=epochs, verbose=False, shuffle=False)#, batch_size=128)\n",
    "\n",
    "# loss = model.history.history['loss']\n",
    "# val_loss = model.history.history['val_loss']\n",
    "# #--- validate\n",
    "\n",
    "# ax = utl.PltErr(range(len(val_loss)), val_loss,\n",
    "#            attrs={'fmt':'-'}, Plot=False,\n",
    "#           )\n",
    "# utl.PltErr(range(len(loss)), loss,\n",
    "#            attrs={'fmt':'-'},\n",
    "#            ax=ax,\n",
    "#            yscale='log',xscale='log',\n",
    "#            xlim=(1,epochs),\n",
    "#            xstr='epoch',ystr='loss',\n",
    "#            title='png/loss.png',\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ae920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model = keras.models.load_model(\"png/convnet_from_scratch.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa9c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ux,uy=best_model.predict(X_test_reshaped)[0]\n",
    "# ax=utl.PltErr([0,ux],[0,uy],\n",
    "#               Plot=False\n",
    "#           )\n",
    "# utl.PltErr([0,y_test[0][0]],[0,y_test[0][1]],\n",
    "#            xlim=(-3,3),ylim=(-3,3),\n",
    "#             ax=ax\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf166dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (ux,uy), y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e9080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "453.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
