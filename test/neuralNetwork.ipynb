{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cded12cf",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#import-libs\" data-toc-modified-id=\"import-libs-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>import libs</a></span></li><li><span><a href=\"#Train-NN\" data-toc-modified-id=\"Train-NN-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Train NN</a></span><ul class=\"toc-item\"><li><span><a href=\"#main():-classifier\" data-toc-modified-id=\"main():-classifier-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>main(): classifier</a></span></li><li><span><a href=\"#main():-regressor\" data-toc-modified-id=\"main():-regressor-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>main(): regressor</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot\" data-toc-modified-id=\"Plot-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Plot</a></span></li></ul></li><li><span><a href=\"#test-example:-2d\" data-toc-modified-id=\"test-example:-2d-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>test example: 2d</a></span><ul class=\"toc-item\"><li><span><a href=\"#fully-connected-in-sklearn\" data-toc-modified-id=\"fully-connected-in-sklearn-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>fully connected in sklearn</a></span></li><li><span><a href=\"#fully-connected-in-keras\" data-toc-modified-id=\"fully-connected-in-keras-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>fully connected in keras</a></span></li><li><span><a href=\"#cnn\" data-toc-modified-id=\"cnn-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>cnn</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117638d",
   "metadata": {},
   "source": [
    "# import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49343b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conf. file sections: ['flags', 'input files', 'descriptors', 'neural net', 'neural net classification', 'neural net regression', 'gnn', 'ml mc']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "confParser = configparser.ConfigParser()\n",
    "\n",
    "#--- parse conf. file\n",
    "confParser.read('configuration.ini')\n",
    "print('conf. file sections:',confParser.sections())\n",
    "#\n",
    "import os\n",
    "import sys\n",
    "list(map(lambda x:sys.path.append(x), confParser['input files']['lib_path'].split()))\n",
    "from dscribe.descriptors import SOAP, ACSF\n",
    "import ase\n",
    "import ase.io\n",
    "import ase.build\n",
    "from ase.io import lammpsdata\n",
    "import pdb\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import matplotlib.pyplot as plt\n",
    "if not eval(confParser['flags']['RemoteMachine']):\n",
    "    plt.rc('text', usetex=True)\n",
    "import pickle\n",
    "#\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#\n",
    "from scipy.stats import gaussian_kde\n",
    "#\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#--- user modules\n",
    "import LammpsPostProcess as lp\n",
    "import utility as utl\n",
    "import imp\n",
    "imp.reload(utl)\n",
    "imp.reload(lp)\n",
    "\n",
    "#--- increase width\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a2e60",
   "metadata": {},
   "source": [
    "# Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd8c31d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "\n",
    "    def __init__(self, verbose=False,\n",
    "                **kwargs\n",
    "                ):\n",
    "        self.verbose = verbose\n",
    "        for key in kwargs:\n",
    "            setattr(self,key,kwargs[key])\n",
    "            \n",
    "            \n",
    "        !mkdir $self.best_model\n",
    "\n",
    "    \n",
    "    def Parse(self,path,nruns):\n",
    "        self.perAtomData = {}\n",
    "        rwjs = utl.ReadWriteJson()\n",
    "        for irun in range(nruns):\n",
    "            if irun == 0:\n",
    "                #--- same configurations!\n",
    "                self.descriptors  = np.c_[rwjs.Read('%s/Run%s/descriptors.json'%(path,irun))[0]['data']]\n",
    "                self.shape        = np.c_[rwjs.Read('%s/Run%s/descriptors.json'%(path,irun))[0]['shape']].flatten()\n",
    "                self.positions    = np.c_[rwjs.Read('%s/Run%s/descriptors.json'%(path,irun))[0]['xyz']]\n",
    "                os.system('ln -s %s/Run%s/dumpFile/dump.xyz .'%(path,irun))\n",
    "            try:\n",
    "                data = np.loadtxt('%s/Run%s/perAtomData.txt'%(path,irun))\n",
    "                #--- displacement data\n",
    "                self.perAtomData[irun] = pd.DataFrame(np.c_[data],\n",
    "                columns='id\ttype\tx\ty\tz\tux\tuy\tuz\tenergy_barrier\tdefect_label'.split()\n",
    "                            )\n",
    "            except:\n",
    "#                 if self.verbose:\n",
    "#                     traceback.print_exc()\n",
    "                continue\n",
    "                \n",
    "        \n",
    "        self.nruns = list(self.perAtomData.keys())\n",
    "        self.nruns.sort()\n",
    "\n",
    "#     def Junk(self,path,nruns):\n",
    "#         self.perAtomData = {}\n",
    "#         self.Descriptors = {}\n",
    "#         self.Shape       = {}\n",
    "#         self.Positions   = {}\n",
    "#         self.Catalogs    = {}\n",
    "#         #\n",
    "#         rwjs = utl.ReadWriteJson()\n",
    "#         for irun in range(nruns):\n",
    "#             try:\n",
    "# #                 data_json               = rwjs.Read('%s/Run%s/descriptors.json'%(path,irun))[0]\n",
    "# #                 self.Descriptors[irun]  = np.c_[data_json['data']]\n",
    "# #                 self.Shape[irun]        = np.c_[data_json['shape']].flatten()\n",
    "# #                 self.Positions[irun]    = np.c_[data_json['xyz']]\n",
    "#                 os.system('ln -s %s/Run%s/dumpFile/dump.xyz ./dump.%s.xyz'%(path,irun,irun))\n",
    "#                 data                    = np.loadtxt('%s/Run%s/perAtomData.txt'%(path,irun))\n",
    "#                 self.perAtomData[irun]  = pd.DataFrame(np.c_[data],\n",
    "#                                                        columns ='id type x y z'.split()\n",
    "#                                                       )\n",
    "#                 self.Catalogs[irun]     = pd.read_csv('%s/Run%s/catalog.txt'%(path,irun))\n",
    "#             except:\n",
    "# #                 if self.verbose:\n",
    "# #                     traceback.print_exc()\n",
    "#                 continue\n",
    "                \n",
    "        \n",
    "#         self.nruns     = list(self.perAtomData.keys())\n",
    "#         self.nruns.sort()\n",
    "\n",
    "#         self.Descriptors[ 0 ] = pd.DataFrame(np.random.random(size=9876))\n",
    "#         #--- assert shape and positions are the same for all realizations\n",
    "# #         self.shape     = self.Shape[ self.nruns[ 0 ] ]\n",
    "# #         self.positions = self.Positions[ self.nruns[ 0 ] ]\n",
    "\n",
    "        \n",
    "        \n",
    "    def Parse2nd(self,path,nruns):\n",
    "        self.perAtomData = {}\n",
    "        self.Descriptors = {}\n",
    "        self.Shape       = {}\n",
    "        self.Positions   = {}\n",
    "        self.Catalogs    = {}\n",
    "        #\n",
    "        rwjs = utl.ReadWriteJson()\n",
    "        for irun in range(nruns):\n",
    "            try:\n",
    "                data_json               = rwjs.Read('%s/Run%s/descriptors.json'%(path,irun))[0]\n",
    "                self.Descriptors[irun]  = np.c_[data_json['data']]\n",
    "                self.Shape[irun]        = np.c_[data_json['shape']].flatten()\n",
    "                self.Positions[irun]    = np.c_[data_json['xyz']]\n",
    "                os.system('ln -s %s/Run%s/dumpFile/dump.xyz ./dump.%s.xyz'%(path,irun,irun))\n",
    "                data                    = np.loadtxt('%s/Run%s/perAtomData.txt'%(path,irun))\n",
    "                self.perAtomData[irun]  = pd.DataFrame(np.c_[data],\n",
    "                                                       columns ='id type x y z'.split()\n",
    "                                                      )\n",
    "                self.Catalogs[irun]     = pd.read_csv('%s/Run%s/catalog.txt'%(path,irun))\n",
    "            except:\n",
    "#                 if self.verbose:\n",
    "#                     traceback.print_exc()\n",
    "                continue\n",
    "                \n",
    "        \n",
    "        self.nruns     = list(self.perAtomData.keys())\n",
    "        self.nruns.sort()\n",
    "\n",
    "        #--- assert shape and positions are the same for all realizations\n",
    "        self.shape     = self.Shape[ self.nruns[ 0 ] ]\n",
    "        self.positions = self.Positions[ self.nruns[ 0 ] ]\n",
    "        \n",
    "    def Combine(self):\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('concatenating descriptors ...')\n",
    "#         pdb.set_trace()\n",
    "        #--- center atoms\n",
    "        center_atom_indices = list(map(lambda x:NeuralNetwork.GetCenterAtom( self.perAtomData[x])[0],self.nruns))\n",
    "        sdict = dict(zip(center_atom_indices,self.nruns))\n",
    "        \n",
    "        atom_ids = list(sdict.keys())\n",
    "        atom_ids.sort()\n",
    "        #         center_atom_indices = list( set( center_atom_indices ) )\n",
    "        data = np.concatenate(list(map(lambda x: np.c_[self.perAtomData[sdict[x]].iloc[ x ]],atom_ids)),axis=1).T\n",
    "#        descriptors_center_atoms = self.descriptors[atom_ids]\n",
    "        descriptors_center_atoms = np.c_[list(map(lambda x:self.Descriptors[sdict[x]][x], atom_ids))]\n",
    "    \n",
    "        #--- data frame\n",
    "#        print(data.shape)\n",
    "        irun = self.nruns[0]\n",
    "        df_combined = pd.DataFrame(data,columns=list(self.perAtomData[irun].keys()))\n",
    "    \n",
    "        #--- filter crystalline atoms\n",
    "        filtr = self.perAtomData[irun].defect_label == 0.0\n",
    "        df_crystalline = self.perAtomData[irun][filtr]\n",
    "        descriptors_crystalline = self.descriptors[filtr]\n",
    "\n",
    "        #--- merge\n",
    "        keys = list(df_combined.keys())\n",
    "        data_concat = np.concatenate([np.c_[df_combined[keys]],np.c_[df_crystalline[keys]]],axis=0) \n",
    "        self.perAtomData = pd.DataFrame(data_concat,\n",
    "                              columns=keys\n",
    "                             )\n",
    "\n",
    "        \n",
    "        #--- merge descriptors\n",
    "        self.descriptors = np.concatenate([descriptors_center_atoms,descriptors_crystalline],axis=0)\n",
    "\n",
    "        assert self.perAtomData.shape[ 0 ] == self.descriptors.shape[0], 'need more mc swaps: %s %s'\\\n",
    "        %(self.perAtomData.shape[ 0 ],self.descriptors.shape[0])\n",
    "                            \n",
    "    def Combine2nd(self):\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('concatenating descriptors ...')\n",
    "            \n",
    "        irun = self.nruns[0]\n",
    "        keys = list( self.perAtomData[ irun ].keys() )\n",
    "\n",
    "        #--- center atoms\n",
    "        data_concat         = np.concatenate(list(map(lambda x: np.c_[self.perAtomData[x]],self.nruns)),axis=0)\n",
    "        self.perAtomData    = pd.DataFrame(data_concat,\n",
    "                                 columns=keys\n",
    "                                )\n",
    "        self.descriptors    = np.concatenate(list(map(lambda x:self.Descriptors[x], self.nruns)))\n",
    "    \n",
    "#     def Junk2nd(self):\n",
    "        \n",
    "#         if self.verbose:\n",
    "#             print('concatenating descriptors ...')\n",
    "            \n",
    "#         irun = self.nruns[0]\n",
    "#         keys = list( self.perAtomData[ irun ].keys() )\n",
    "\n",
    "#         #--- center atoms\n",
    "#         data_concat         = np.concatenate(list(map(lambda x: np.c_[self.perAtomData[x]],self.nruns)),axis=0)\n",
    "#         self.perAtomData    = pd.DataFrame(data_concat,\n",
    "#                                  columns=keys\n",
    "#                                 )\n",
    "#         self.descriptors    = None #self.Descriptors[0]\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def GetCenterAtom(df):\n",
    "        disp_magnitude = df.ux**2+df.uy**2+df.uz**2\n",
    "        center_atom_indx = disp_magnitude.sort_values(ascending=False).index[0]\n",
    "        return center_atom_indx, int(df.iloc[ center_atom_indx ].id)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def zscore(slist):\n",
    "#         tmp = np.copy(slist)\n",
    "#         print(np.mean(tmp),np.std(tmp))\n",
    "#         tmp -= np.mean(tmp)\n",
    "#         tmp /= np.std(tmp)\n",
    "#         return tmp\n",
    "\n",
    "    def PCA(self,\n",
    "           n_components=2,\n",
    "            random_state = 1,\n",
    "           ):\n",
    "        #--- concat. data\n",
    "        X = self.descriptors\n",
    "        pca = PCA(n_components=n_components,random_state=random_state)\n",
    "        pca.fit(X)\n",
    "        X_transformed = pca.transform(X)\n",
    "\n",
    "        xdata = X_transformed[:,0]\n",
    "        ydata = X_transformed[:,1]\n",
    "        #\n",
    "        filtr_defects = self.perAtomData.defect_label == 0.0\n",
    "        #\n",
    "\n",
    "        legend = utl.Legends()\n",
    "        legend.Set(bbox_to_anchor=(1.1,.5, 0.5, 0.5))\n",
    "#         pdb.set_trace()\n",
    "        #ax = utl.PltErr(zscore(xdata)[filtr_defects],zscore(ydata)[filtr_defects],\n",
    "        ax = utl.PltErr(xdata[filtr_defects],ydata[filtr_defects],\n",
    "                  attrs={'fmt':'x','alpha':1,'label':'defect_free'},\n",
    "                        Plot = False,\n",
    "        #                 xlim=(-2,2),\n",
    "        #                 ylim=(-2,2),\n",
    "                  )\n",
    "\n",
    "        #utl.PltErr(zscore(xdata)[~filtr_defects],zscore(ydata)[~filtr_defects],\n",
    "        !mkdir png\n",
    "        utl.PltErr(xdata[~filtr_defects],ydata[~filtr_defects],\n",
    "                  attrs={'fmt':'.','color':'red','label':'defects'},\n",
    "                   ax=ax,\n",
    "                   xstr='pca_1',ystr='pca_2',\n",
    "                   legend = legend.Get(),\n",
    "                   title='png/pca.png'\n",
    "                  )\n",
    "    def Spectra(self,\n",
    "               nrows=100,\n",
    "               ):\n",
    "        assert nrows <= self.descriptors.shape[ 0 ]\n",
    "        !mkdir png\n",
    "        utl.PltBitmap(np.log10(np.abs(self.descriptors[:nrows,:])),\n",
    "                      xlabel=r'$\\mathrm{ndim}$',ylabel=r'$\\mathrm{natom}$',\n",
    "                      xlim=(0,self.descriptors.shape[1]),\n",
    "                      ylim=(0,nrows),\n",
    "                      colorbar=True,\n",
    "                      zscore=False,\n",
    "                      vminmax=(-3,3),\n",
    "                      title='png/feature_bitmap.png'\n",
    "                     )\n",
    "        \n",
    "    def SklearnMLP(self,X_train,y_train):\n",
    "        #-----------------------\n",
    "        #--- parameter grid\n",
    "        #-----------------------\n",
    "#         param_grid = {\n",
    "#                         'hidden_layer_sizes':self.hidden_layer_sizes,\n",
    "#                          #'activation' : ['tanh', 'relu'],\n",
    "#                          'learning_rate_init':self.learning_rate_init,\n",
    "# #                         'alpha':self.alpha, #--- regularization \n",
    "#                          #'learning_rate' : ['invscaling', 'adaptive'],\n",
    "#                         'n_iter_no_change':self.n_iter_no_change,\n",
    "# #                        'tol':self.tol,\n",
    "#                         'max_iter':self.max_iter,\n",
    "#                      } \n",
    "        mlp   =  MLPClassifier(random_state=1,\n",
    "                               hidden_layer_sizes = self.hidden_layer_sizes[0],\n",
    "                               learning_rate_init = self.learning_rate_init[0],\n",
    "                               n_iter_no_change   = self.n_iter_no_change[0],\n",
    "                               max_iter           = self.max_iter[0],\n",
    "                               verbose=self.verbose)\n",
    "#         clf  =  GridSearchCV(mlp, param_grid)\n",
    "#        clf.fit(X_train,y_train)\n",
    "        mlp.fit(X_train,y_train)\n",
    "        model =  mlp #clf.best_estimator_\n",
    "        loss  =  model.loss_curve_\n",
    "        val_loss = loss\n",
    "        return (model, loss, val_loss)\n",
    "\n",
    "    def KerasANN(self, X_train, y_train,X_test, y_test, ndime):\n",
    "\n",
    "        model     = keras.Sequential([ #--- The network architecture\n",
    "                                    layers.Dense(self.hidden_layer_size, activation=self.activation),\n",
    "                #                    layers.Dense(self.hidden_layer_size, activation=self.activation),\n",
    "                                    layers.Dense(ndime, activation='softmax')\n",
    "                                    ])\n",
    "        \n",
    "#         shape         =  (self.shape[0]*self.shape[1]*self.shape[2],)\n",
    "#         inputs        =  keras.Input(shape=shape)\n",
    "#         #------------------------------\n",
    "#         #--- The network architecture\n",
    "#         #------------------------------\n",
    "#         x             =  layers.Dense(   self.hidden_layer_size, activation=self.activation\n",
    "#                                        )(inputs)\n",
    "#         for i in range( self.number_hidden_layers ):\n",
    "#             x       = layers.Dense( self.hidden_layer_size, activation=self.activation\n",
    "#                                      )(x)\n",
    "#         #--- output layer\n",
    "# #         x       = layers.Flatten()(x)\n",
    "#         outputs = layers.Dense( ndime, activation=self.activation)(x)\n",
    "#         model   = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        \n",
    "        \n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate_init) #--- compilation step\n",
    "        model.compile( optimizer=optimizer,\n",
    "                       loss=\"sparse_categorical_crossentropy\",\n",
    "                       metrics=[\"mse\"]\n",
    "                     )\n",
    "        #--- save best model\n",
    "        !mkdir best_model\n",
    "#         callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnetClassifier_from_scratch.tf',  \n",
    "#                                                     monitor=\"mse\",\n",
    "#                                                     save_freq=10,\n",
    "#                                                     save_best_only=True)]\n",
    "\n",
    "        model.fit( X_train, y_train, \n",
    "           validation_data      = ( X_test, y_test ),\n",
    "#             callbacks           = callbacks,\n",
    "            epochs              = self.max_iter[0], \n",
    "            verbose             = self.verbose, \n",
    "            shuffle             = False, \n",
    "#             batch_size     = 32,\n",
    "#                     use_multiprocessing = True,\n",
    "#                     workers             = 4,\n",
    "         )    \n",
    "\n",
    "        model.save('best_model/convnetClassifier_from_scratch.tf')\n",
    "        loss      = model.history.history['loss']\n",
    "        val_loss  = model.history.history['val_loss']\n",
    "        best_model = model #keras.models.load_model(\"best_model/convnetClassifier_from_scratch.tf\")\n",
    "\n",
    "        return (best_model, loss, val_loss)\n",
    "\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def MapClassIds( y ):\n",
    "        ndime         = len(set(y.flatten()))\n",
    "        class_ids     = list(set(y.flatten()))\n",
    "        class_ids.sort()\n",
    "        map_class_ids = dict(zip(class_ids,range(ndime)))        \n",
    "        return ndime, np.c_[list(map(lambda x:[map_class_ids[x]],y.flatten()))]\n",
    "    \n",
    "    @staticmethod\n",
    "    def GetSubSetCrystallineAtoms(X,y,n_train):\n",
    "        #--- data frame\n",
    "        df = pd.DataFrame( y, columns=['topoID'] )\n",
    "        #--- groups\n",
    "        sdict = df.groupby(by='topoID').groups\n",
    "        if sdict[ 0 ].shape[ 0 ] < n_train:\n",
    "            return X, y\n",
    "        indices = np.random.choice(sdict[ 0 ], size=n_train, replace=False)\n",
    "        sdict[ 0 ] = indices\n",
    "        indices_total = np.concatenate( list(map(lambda x:sdict[x],sdict)) )\n",
    "        return X[indices_total], y[ indices_total ]\n",
    "    \n",
    "    def GetLabels( self, irun ):\n",
    "        nsize                                  = self.Descriptors[ irun ].shape[ 0 ]\n",
    "        y_labels                               = np.zeros(nsize,dtype=int)\n",
    "        nonCrystallineAtomsIndices             = self.Catalogs[ irun ].AtomIndex\n",
    "        nonCrystallineAtomsTopoIds             = self.Catalogs[ irun ].IniTopoId.astype(int)\n",
    "        y_labels[ nonCrystallineAtomsIndices ] = nonCrystallineAtomsTopoIds\n",
    "        return y_labels.reshape( ( nsize, 1 ) )\n",
    "\n",
    "    \n",
    "    def TrainClassifier(self,\n",
    "                       random_state=1,\n",
    "                       ):\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "\n",
    "        #--- get labels\n",
    "        y_labels = np.concatenate( list( map(lambda x:self.GetLabels(x), self.nruns ) ) )\n",
    "        assert self.descriptors.shape[ 0 ] == y_labels.shape[ 0 ]\n",
    "        \n",
    "        #--- map topo ids to integers 0, 1 ... ndime\n",
    "        ndime, y = NeuralNetwork.MapClassIds( y_labels )\n",
    "\n",
    "        \n",
    "        X      = np.c_[self.descriptors]\n",
    "\n",
    "        #--- filter: only train a subset of crystalline atoms\n",
    "        X, y   = NeuralNetwork.GetSubSetCrystallineAtoms( X, y, self.n_train )\n",
    "        \n",
    "        #---------------\n",
    "        #--- zscore X\n",
    "        #---------------        \n",
    "        X      = NeuralNetwork.Zscore( X, save_model = '%s/classifier.sav'%self.best_model )\n",
    "\n",
    "        #--- add noise\n",
    "        NeuralNetwork.AddGaussianNoise(X, scale = 0.001 )\n",
    "\n",
    "\n",
    "        #--- exclude void\n",
    "#        filtr  = self.perAtomData.type==2\n",
    "#         X      = X[~filtr]\n",
    "#         y      = y[~filtr]\n",
    "\n",
    "        #--- sample from crystalline atoms\n",
    "        \n",
    "        #-----------------------\n",
    "        #--- train-test split\n",
    "        #-----------------------\n",
    "#        train_size = self.n_train\n",
    "#        test_size  = int( self.n_train / 3 )\n",
    "        assert X.shape[0] >= self.n_train, 'increase nruns!' #train_size + train_size, 'increase nruns!'\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "#                                                               test_size=test_size, train_size=train_size,\n",
    "                                                              random_state=random_state)\n",
    "        if len(set(y_train.flatten())) < ndime:\n",
    "            'warning: not every class present in train set!'\n",
    "        if len(set(y_test.flatten()))  < ndime: \n",
    "            'warning: not every class present in test set!'\n",
    "        \n",
    "        #-----------------------\n",
    "        #--- train model\n",
    "        #-----------------------\n",
    "        if self.fully_connected: #--- dense nn\n",
    "            if self.implementation == 'sklearn':\n",
    "                (model, loss, val_loss) = self.SklearnMLP(X_train,y_train)\n",
    "                classes_x = model.predict(X_test) \n",
    "                \n",
    "            elif self.implementation == 'keras': #--- dense nn in keras\n",
    "                (model, loss, val_loss) = self.KerasANN(X_train, y_train,X_test, y_test, ndime)\n",
    "                predict_x = model.predict(X_test) \n",
    "                classes_x = np.argmax(predict_x,axis=1)\n",
    "                \n",
    "        elif self.cnn: #--- convolutional\n",
    "            (model, loss, val_loss), (X_train, X_test) =\\\n",
    "            self.ConvNetworkClassifier( y )\n",
    "            predict_x = model.predict(X_test) \n",
    "            classes_x = np.argmax(predict_x,axis=1)\n",
    "                    \n",
    "        #--- save loss data\n",
    "        !mkdir png\n",
    "        np.savetxt('png/val_loss_classification.txt',\n",
    "                   np.c_[range(len(loss)),loss,val_loss],\n",
    "                   header='epoch loss val_loss')\n",
    "\n",
    "        #--- confusion matrix\n",
    "        cm = confusion_matrix(y_test, classes_x,\n",
    "                         labels=range(ndime)\n",
    "                        )\n",
    "        np.savetxt('png/confusion.txt',np.c_[cm])\n",
    "\n",
    "        \n",
    "    def PrintDescriptors(self,descriptors,y,fout):\n",
    "        rwjs = utl.ReadWriteJson()\n",
    "        rwjs.Write([{'descriptors':np.c_[descriptors],\n",
    "                     'target':np.c_[y],\n",
    "                     'shape_descriptor':self.shape}],fout)\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def GetTopoIds( catalog, key ): \n",
    "        TopoIds = list( catalog.groupby( by = key ).groups.keys())\n",
    "        TopoIds.sort()\n",
    "        return TopoIds \n",
    "    \n",
    "    @staticmethod\n",
    "    def MapTopoIds( TopoIds ):\n",
    "        ndime         = len( TopoIds )\n",
    "        map_TopoIds = dict(zip(TopoIds,range(ndime)))        \n",
    "        return map_TopoIds\n",
    "\n",
    "    def GetTopoArrayIndex( self, IniTopoId, FinTopoId ):\n",
    "        mappedIniTopoId = self.mappedTopoIds[ IniTopoId ]\n",
    "        mappedFinTopoId = self.mappedTopoIds[ FinTopoId ]\n",
    "        assert mappedIniTopoId < self.n_unique_transition_paths and\\\n",
    "               mappedFinTopoId < self.n_unique_transition_paths \n",
    "        return mappedIniTopoId * self.n_unique_transition_paths * self.ndime + mappedFinTopoId * self.ndime\n",
    "\n",
    "    def GetTopoArrayIndex2nd( self, ux, uy, uz ):\n",
    "        aa = np.c_[[ux,uy,uz]].T\n",
    "        H, bin_edges = np.histogramdd(aa,bins=self.bins)        \n",
    "        assert H.sum() == 1.0\n",
    "        \n",
    "        return H.astype(int).flatten()\n",
    "        \n",
    "\n",
    "    def FillTargetMatrix( self, item ):\n",
    "#        pdb.set_trace()\n",
    "        ux = item.inifin_dr * item.DirX\n",
    "        uy = item.inifin_dr * item.DirY\n",
    "        uz = item.inifin_dr * item.DirZ\n",
    "        assert np.abs( ux ) < self.umax and\\\n",
    "                np.abs( uy ) < self.umax and\\\n",
    "                np.abs( uz ) < self.umax,'ux=%e, uy=%e, uz=%e increase self.umax!'%(ux,uy,uz)\n",
    "        irow                                   = int( item.AtomIndex )\n",
    "#        icol                                   = int( self.GetTopoArrayIndex( item.IniTopoId, item.FinTopoId ) )\n",
    "        H                                   = self.GetTopoArrayIndex2nd( ux, uy, uz )\n",
    "\n",
    "        self.y_targets[ irow ] += H           \n",
    "\n",
    "    def DiscretizeTransitionPath( self ):\n",
    "         #--- hard-coded values\n",
    "        xlin = np.arange(-self.umax,+self.umax+self.du,self.du)\n",
    "        ylin = np.arange(-self.umax,+self.umax+self.du,self.du)\n",
    "        zlin = np.arange(-self.umax,+self.umax+self.du,self.du)\n",
    "        self.nbinx = len(xlin)-1\n",
    "        self.nbiny = len(ylin)-1\n",
    "        self.nbinz = len(zlin)-1\n",
    "        self.bins = (xlin, ylin, zlin)\n",
    "        self.xv, self.yv, self.zv = np.meshgrid( self.bins[1][:-1], self.bins[0][:-1], self.bins[2][:-1] )\n",
    "\n",
    "#        print(yv[H==1],xv[H==1],zv[H==1])\n",
    "\n",
    "    def GetTargets( self, irun ):\n",
    "        #--- set-up y matrix\n",
    "#         IniTopoId                              = NeuralNetwork.GetTopoIds( self.Catalogs[ irun ], 'IniTopoId' )\n",
    "#         FinTopoId                              = NeuralNetwork.GetTopoIds( self.Catalogs[ irun ], 'FinTopoId' )\n",
    "#         TopoIds                                = list( set( FinTopoId + IniTopoId ) ) #--- transition path ids\n",
    "#         TopoIds.sort()\n",
    "#         self.mappedTopoIds                     = NeuralNetwork.MapTopoIds( TopoIds )\n",
    "        \n",
    "#         self.n_unique_transition_paths         = len( TopoIds )\n",
    "\n",
    "\n",
    "        nsize                                  = ( self.Descriptors[ irun ].shape[ 0 ], self.nbinx * self.nbiny * self.nbinz )\n",
    "        self.y_targets                         = np.zeros( nsize[ 0 ] * nsize[ 1 ], dtype=int ).reshape( nsize )\n",
    "        \n",
    "        #--- fill y matrix\n",
    "        self.Catalogs[ irun ].apply( lambda x: self.FillTargetMatrix( x ), axis = 1 )\n",
    "        \n",
    "        #--- assert self.y_targets is binary\n",
    "        assert set(self.y_targets.flatten()) == set(np.array([0,1])), 'decrease du!'\n",
    "        return self.y_targets\n",
    "        \n",
    "    def MaskCrystallineAtoms(self, irun ):\n",
    "        nsize                              = self.Descriptors[ irun ].shape[ 0 ]\n",
    "        mask                               = np.zeros(nsize,dtype=bool)\n",
    "        nonCrystallineAtomsIndices         = np.array( list( set( self.Catalogs[ irun ].AtomIndex ) ) )\n",
    "        mask[ nonCrystallineAtomsIndices ] = True\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def GetInputOutput( self, irun, indx ):\n",
    "        atomIndices     =  self.Catalogs[ irun ].AtomIndex\n",
    "        pixel_map_input =  self.Descriptors[ irun ][ atomIndices ]    \n",
    "        dr              =  np.c_[self.Catalogs[ irun ]['inifin_dr']].flatten()\n",
    "        dr_multi        = np.c_[ dr, dr, dr ]\n",
    "        vector_input    =  self.Catalogs[ irun ][ ' DirX      DirY      DirZ'.split() ] * dr_multi\n",
    "        output          =  self.Catalogs[ irun ][ 'barrier' ]\n",
    "        return [pixel_map_input, vector_input, output][ indx ]\n",
    "\n",
    "    def TrainRegressorBarriers(self,\n",
    "                       random_state=1,\n",
    "                       ):\n",
    "        '''\n",
    "        Multi-layer Perceptron regressor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        stratify : array-like, default=None\n",
    "        If not None, data is split in a stratified fashion, using this as\n",
    "        the class labels.\n",
    "        \n",
    "        y : array-like, target data\n",
    "        \n",
    "        random_state : initial seed, default=1\n",
    "        \n",
    "        printOvito : bool, default=False\n",
    "        \n",
    "        filtr : bool, default=False\n",
    "        if not None, data is filtered before calling train-test split\n",
    "        '''\n",
    "\n",
    "        \n",
    "        #--- get transition path bitmap\n",
    "#        self.DiscretizeTransitionPath()\n",
    "#        self.ndime                                  = 4 #--- hard-coded: (ux,uy,uz,E)\n",
    "        pixel_maps_input = np.concatenate( list( map(lambda x:self.GetInputOutput(x,0), self.nruns ) ) )\n",
    "        vectors_input    = np.concatenate( list( map(lambda x:self.GetInputOutput(x,1), self.nruns ) ) )\n",
    "        scalar_output    = np.concatenate( list( map(lambda x:self.GetInputOutput(x,2), self.nruns ) ) )\n",
    "\n",
    "        \n",
    "        #---------------\n",
    "        #--- zscore X\n",
    "        #---------------        \n",
    "        X      = np.c_[pixel_maps_input,vectors_input]\n",
    "        X      = NeuralNetwork.Zscore( X, save_model = '%s/scaler_regression_barriers.sav'%self.best_model )\n",
    "        y      = np.c_[ scalar_output ]\n",
    "        \n",
    "        if X.shape[ 0 ] - self.n_train < 0 :  \n",
    "            X = NeuralNetwork.Duplicate(X, new_size = self.n_train )\n",
    "            y = NeuralNetwork.Duplicate(y, new_size = self.n_train )\n",
    "\n",
    "        #--- add noise\n",
    "        NeuralNetwork.AddGaussianNoise(X, scale = 0.01 )\n",
    "        NeuralNetwork.AddGaussianNoise(y, scale = 0.01 )\n",
    "\n",
    "        #-----------------------\n",
    "        #--- train-test split\n",
    "        #-----------------------\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, #stratify=stratify,\n",
    "                                                            random_state=random_state)\n",
    "\n",
    "\n",
    "        #-----------------------\n",
    "        #--- train model\n",
    "        #-----------------------\n",
    "\n",
    "                \n",
    "        (model, loss, val_loss), (X_train, X_test) = self.ConvNetworkMixedInput(X_train, y_train, X_test, y_test )\n",
    "            \n",
    "            \n",
    "            \n",
    "            #--- validation\n",
    "        NeuralNetwork.Validation(loss, val_loss, \n",
    "                                 model, \n",
    "                                 X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "    def TrainRegressorTransitionPaths(self,#stratify,y,\n",
    "                       random_state=1,\n",
    "                       printOvito = False,\n",
    " #                      filtr = None,\n",
    "                       ):\n",
    "        '''\n",
    "        Multi-layer Perceptron regressor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        stratify : array-like, default=None\n",
    "        If not None, data is split in a stratified fashion, using this as\n",
    "        the class labels.\n",
    "        \n",
    "        y : array-like, target data\n",
    "        \n",
    "        random_state : initial seed, default=1\n",
    "        \n",
    "        printOvito : bool, default=False\n",
    "        \n",
    "        filtr : bool, default=False\n",
    "        if not None, data is filtered before calling train-test split\n",
    "        '''\n",
    "\n",
    "        \n",
    "        #--- get transition path bitmaps\n",
    "        self.DiscretizeTransitionPath()\n",
    "#        self.ndime                                  = 4 #--- hard-coded: (ux,uy,uz,E)\n",
    "        y = np.concatenate( list( map(lambda x:self.GetTargets(x), self.nruns ) ) )\n",
    "\n",
    "        \n",
    "        #--- filtr crystalline atoms\n",
    "        filtr = np.concatenate( list( map(lambda x:self.MaskCrystallineAtoms(x), self.nruns ) ) )\n",
    "\n",
    "        y     = y[ filtr ]\n",
    "        \n",
    "#        pdb.set_trace()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('dim(y)=',y.shape)\n",
    "\n",
    "        ndime  = y.shape[1] #--- dimension of the target vector\n",
    "        \n",
    "        #---------------\n",
    "        #--- zscore X\n",
    "        #---------------        \n",
    "        X      = np.c_[self.descriptors[filtr]]\n",
    "        X      = NeuralNetwork.Zscore( X, save_model = '%s/scaler_regression.sav'%self.best_model )\n",
    "        \n",
    "        \n",
    "        if X.shape[ 0 ] - self.n_train < 0 :  \n",
    "            X = NeuralNetwork.Duplicate(X, new_size = self.n_train )\n",
    "            y = NeuralNetwork.Duplicate(y, new_size = self.n_train )\n",
    "\n",
    "        #--- add noise\n",
    "        NeuralNetwork.AddGaussianNoise(X, scale = 0.1 )\n",
    "#        NeuralNetwork.AddGaussianNoise(y, scale = 0.1 )\n",
    "\n",
    "        #-----------------------\n",
    "        #--- train-test split\n",
    "        #-----------------------\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, #stratify=stratify,\n",
    "                                                            random_state=random_state)\n",
    "\n",
    "\n",
    "        #-----------------------\n",
    "        #--- train model\n",
    "        #-----------------------\n",
    "        if self.fully_connected: #--- dense nn\n",
    "            if self.implementation == 'sklearn':\n",
    "                #-----------------------\n",
    "                #--- parameter grid\n",
    "                #-----------------------\n",
    "                param_grid = {\n",
    "                                'hidden_layer_sizes':self.hidden_layer_sizes,\n",
    "                                 #'activation' : ['tanh', 'relu'],\n",
    "                                 'learning_rate_init':self.learning_rate_init,\n",
    "                                'alpha':self.alpha, #--- regularization \n",
    "                                 #'learning_rate' : ['invscaling', 'adaptive'],\n",
    "                                'n_iter_no_change':self.n_iter_no_change,\n",
    "                                'tol':self.tol,\n",
    "                                'max_iter':self.max_iter,\n",
    "                             } \n",
    "                mlp   =  MLPRegressor(random_state=random_state,verbose=self.verbose) #--- mlp regressor\n",
    "                regr  =  GridSearchCV(mlp, param_grid)\n",
    "                regr.fit(X_train,y_train)\n",
    "                model =  regr.best_estimator_\n",
    "                loss  =  model.loss_curve_\n",
    "                \n",
    "            elif self.implementation == 'keras': #--- dense nn in keras\n",
    "                model     = keras.Sequential([ #--- The network architecture\n",
    "                    layers.Dense(self.hidden_layer_size, activation=self.activation),\n",
    "#                    layers.Dense(self.hidden_layer_size, activation=self.activation),\n",
    "                    layers.Dense(ndime, activation='softmax')\n",
    "                    ])\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_init) #--- compilation step\n",
    "                model.compile( optimizer=optimizer,#\"rmsprop\",\n",
    "                               loss=\"mean_squared_error\",#\"sparse_categorical_crossentropy\",\n",
    "                               metrics=[\"mse\"]\n",
    "                             )\n",
    "                model.fit(X_train, y_train, #--- “Fitting”\n",
    "                          validation_data=(X_test, y_test),\n",
    "                          epochs=self.max_iter[0], verbose=self.verbose, batch_size=1)\n",
    "                loss      = model.history.history['loss']\n",
    "                val_loss  = model.history.history['val_loss']\n",
    "                \n",
    "        elif self.cnn: #--- convolutional\n",
    "            \n",
    "            (model, loss, val_loss), (X_train, X_test) =\\\n",
    "            self.ConvNetworkMultiLabelClassifier(X_train, y_train, X_test, y_test )\n",
    "            \n",
    "            NeuralNetwork.ValidationMultiLabelClassification(loss, val_loss, #--- validation\n",
    "                                 model, \n",
    "                                 X_train, X_test, y_train, y_test)\n",
    "            \n",
    "#             pdb.set_trace()\n",
    "#             (model, loss, val_loss), (X_train, X_test) =\\\n",
    "#             self.ConvNetwork(X_train, y_train, X_test, y_test )\n",
    "            \n",
    "            \n",
    "            \n",
    "            #--- validation\n",
    "#             NeuralNetwork.Validation(loss, val_loss, \n",
    "#                                      model, \n",
    "#                                      X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        #--- save in ovito file\n",
    "#         if printOvito:\n",
    "#             m = self.descriptors.shape[ 0 ]\n",
    "#             indices = np.arange( m )[ filtr ]\n",
    "        \n",
    "#             if indices.shape[ 0 ] - self.n_train < 0 :  \n",
    "#                 indices = NeuralNetwork.Duplicate(indices, new_size = self.n_train )\n",
    "            \n",
    "#             indices_train, indices_test, _, _ = train_test_split(indices, indices, #stratify=stratify,\n",
    "#                                                                 random_state=random_state)\n",
    "#             self.PrintOvito( indices_test, model )\n",
    "\n",
    "    @staticmethod\n",
    "    def Validation(loss, val_loss, model, X_train, X_test, y_train, y_test):\n",
    "        #-----------------------\n",
    "        #--- validation\n",
    "        #-----------------------\n",
    "        !mkdir png         #--- plot validation loss \n",
    "        ax = utl.PltErr(range(len(val_loss)), val_loss,\n",
    "                   attrs={'fmt':'-'}, Plot=False,\n",
    "                  )\n",
    "        utl.PltErr(range(len(loss)), loss,\n",
    "                   attrs={'fmt':'-'},\n",
    "                   ax=ax,\n",
    "                   yscale='log',xscale='log',\n",
    "#                   xlim=(1,self.max_iter[0]),\n",
    "                   xstr='epoch',ystr='loss',\n",
    "                   title='png/loss.png',\n",
    "                  )\n",
    "        \n",
    "        np.savetxt('png/loss.txt',np.c_[range(len(loss)),loss,val_loss],header='epoch loss val_loss')\n",
    "        \n",
    "        \n",
    "        #--- plot predictions\n",
    "        y_pred_test  = model.predict(X_test)        \n",
    "        y_pred_train = model.predict(X_train)        \n",
    "#         for idime, xstr in zip(range(3),'ux uy uz'.split()):\n",
    "#             ax = utl.PltErr(None,None,Plot=False)\n",
    "#             #\n",
    "#             utl.PltErr(y_test[:,idime],y_pred_test[:,idime],\n",
    "#                        attrs={'fmt':'x','color':'red','zorder':10,'markersize':6},\n",
    "#                        ax=ax,\n",
    "#                        Plot = False,\n",
    "\n",
    "#                       )\n",
    "#             #\n",
    "#             utl.PltErr(y_train[:,idime],y_pred_train[:,idime],\n",
    "#                        attrs={'fmt':'.','color':'blue','zorder':1,'markersize':6},\n",
    "#                        ax=ax,\n",
    "#                        Plot = False,\n",
    "\n",
    "#                       )\n",
    "#             #\n",
    "#             utl.PltErr(None,None,Plot=False,\n",
    "#                            title='png/scatter%s.png'%idime,\n",
    "#                             ax=ax,\n",
    "#                        xstr='%s actual'%xstr,ystr='%s predicted'%xstr,\n",
    "#                        xlim=(-2,2),ylim=(-2,2),\n",
    "#                            )\n",
    "\n",
    "        #--- energy\n",
    "        idime = 0 #3\n",
    "        xstr  = 'energy'\n",
    "        ax = utl.PltErr(None,None,Plot=False)\n",
    "        #\n",
    "        utl.PltErr(y_test[:,idime],y_pred_test[:,idime],\n",
    "                   attrs={'fmt':'x','color':'red','zorder':10,'markersize':6},\n",
    "                   ax=ax,\n",
    "                   Plot = False,\n",
    "\n",
    "                  )\n",
    "        utl.PltErr(y_train[:,idime],y_pred_train[:,idime],\n",
    "                   attrs={'fmt':'.','color':'blue','zorder':1,'markersize':6},\n",
    "                   ax=ax,\n",
    "                   Plot = False,\n",
    "\n",
    "                  )\n",
    "        #\n",
    "        utl.PltErr(None,None,Plot=False,\n",
    "                       title='png/scatter%s.png'%idime,\n",
    "                        ax=ax,\n",
    "                   xstr='%s actual'%xstr,ystr='%s predicted'%xstr,\n",
    "#                   xlim=(-2,2),ylim=(-2,2),\n",
    "                       )\n",
    "\n",
    "    @staticmethod\n",
    "    def ValidationMultiLabelClassification(loss, val_loss, model, X_train, X_test, y_train, y_test):\n",
    "        #-----------------------\n",
    "        #--- validation\n",
    "        #-----------------------\n",
    "        !mkdir png         #--- plot validation loss \n",
    "        ax = utl.PltErr(range(len(val_loss)), val_loss,\n",
    "                   attrs={'fmt':'-'}, Plot=False,\n",
    "                  )\n",
    "        utl.PltErr(range(len(loss)), loss,\n",
    "                   attrs={'fmt':'-'},\n",
    "                   ax=ax,\n",
    "                   yscale='log',xscale='log',\n",
    "#                   xlim=(1,self.max_iter[0]),\n",
    "                   xstr='epoch',ystr='loss',\n",
    "                   title='png/lossMultiLabelClassification.png',\n",
    "                  )\n",
    "        \n",
    "        np.savetxt('png/lossMultiLabelClassification.txt',np.c_[range(len(loss)),loss,val_loss],header='epoch loss val_loss')\n",
    "        \n",
    "        \n",
    "        #--- plot predictions\n",
    "        y_pred_test              = model.predict(X_test)        \n",
    "        y_pred_train             = model.predict(X_train)\n",
    "        \n",
    "        threshold                = 0.5 #--- hard-coded threshold\n",
    "        binary_predictions_test  = (y_pred_test > threshold).astype(int)\n",
    "        binary_predictions_train = (y_pred_train > threshold).astype(int)\n",
    "        binary_actual_test       = (y_test > threshold).astype(int)\n",
    "        binary_actual_train      = (y_train > threshold).astype(int)\n",
    "\n",
    "        \n",
    "        \n",
    "        #--- Compute the multilabel confusion matrix\n",
    "        conf_matrix = multilabel_confusion_matrix( binary_actual_test, binary_predictions_test )\n",
    "        ndime       = conf_matrix.shape[ 1 ] * conf_matrix.shape[ 2 ]\n",
    "        conf_matrix = conf_matrix.reshape((conf_matrix.shape[0],ndime))\n",
    "        np.savetxt('png/confusionMultiLabelClassification.txt',np.c_[conf_matrix])\n",
    "        \n",
    "        \n",
    "        #--- predict displacements\n",
    "        #--- reshape y_pred\n",
    "        \n",
    "            \n",
    "        \n",
    "#         disps_predictions_test = np.concatenate([list(map(lambda x: self.GetDispsFromBinaryMaps( x ) , binary_predictions_test ))])\n",
    "#         disps_actual_test      = np.concatenate([list(map(lambda x: self.GetDispsFromBinaryMaps( x ) , binary_actual_test ))])\n",
    "        \n",
    "# #         #--- plot predictions\n",
    "# #         y_pred_test  = model.predict(X_test)        \n",
    "# #         y_pred_train = model.predict(X_train)        \n",
    "#         for idime, xstr in zip(range(3),'ux uy uz'.split()):\n",
    "#             ax = utl.PltErr(None,None,Plot=False)\n",
    "#             #\n",
    "#             utl.PltErr(disps_actual_test[:,idime],disps_predictions_test[:,idime],\n",
    "#                        attrs={'fmt':'x','color':'red','zorder':10,'markersize':6},\n",
    "#                        ax=ax,\n",
    "#                        Plot = False,\n",
    "\n",
    "#                       )\n",
    "#             #\n",
    "# #             utl.PltErr(y_train[:,idime],y_pred_train[:,idime],\n",
    "# #                        attrs={'fmt':'.','color':'blue','zorder':1,'markersize':6},\n",
    "# #                        ax=ax,\n",
    "# #                        Plot = False,\n",
    "\n",
    "# #                       )\n",
    "#             #\n",
    "#             utl.PltErr(None,None,Plot=False,\n",
    "#                            title='png/scatter%s.png'%idime,\n",
    "#                             ax=ax,\n",
    "#                        xstr='%s actual'%xstr,ystr='%s predicted'%xstr,\n",
    "#                        xlim=(-3,3),ylim=(-3,3),\n",
    "#                            )\n",
    "\n",
    "    def GetDispsFromBinaryMaps( self, binaryMap ):\n",
    "        binaryMapReshaped = binaryMap.reshape((self.nbinx, self.nbiny, self.nbinz ))\n",
    "        filtr = binaryMapReshaped == 1\n",
    "        return np.c_[self.yv[filtr],self.xv[filtr],self.zv[filtr]]\n",
    "\n",
    "        \n",
    "    def PrintOvito( self, filtr, model ):\n",
    "        #--- save in ovito\n",
    "        X          = np.c_[self.descriptors[filtr]]\n",
    "        X          = NeuralNetwork.Zscore( X )\n",
    "        X_reshaped =  X.reshape((X.shape[0],self.shape[0],self.shape[1],self.shape[2],1))\n",
    "        y_pred     = model.predict( X_reshaped )\n",
    "        with open('original.xyz','w') as fp:\n",
    "            utl.PrintOvito(self.perAtomData.iloc[filtr], fp, '0', attr_list='id type x y z ux uy uz'.split())\n",
    "        with open('test.xyz','w') as fp:\n",
    "            xyz = self.perAtomData.iloc[filtr]['id type x y z'.split()]\n",
    "            cordc = pd.DataFrame(np.c_[xyz,y_pred[:,:3]],columns='id type x y z ux uy uz'.split())\n",
    "            utl.PrintOvito(cordc, fp, '0', attr_list='id type x y z ux uy uz'.split())\n",
    "                \n",
    "\n",
    "    def ConvNetworkMixedInput(self,X_train, y_train, X_test, y_test):\n",
    "        '''\n",
    "        Convolutional neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : array-like training x input\n",
    "        \n",
    "        y_train : array-like, training y input\n",
    "        \n",
    "        X_test : array-like test x input\n",
    "        \n",
    "        y_test : array-like, training y input\n",
    "\n",
    "        Return\n",
    "        ---------- ( , loss,  )\n",
    "        best_model : cnn object, best trained model based on on the validation loss\n",
    "        \n",
    "        loss : array-like, mse loss\n",
    "\n",
    "        val_loss : array-like, validation loss\n",
    "\n",
    "        '''\n",
    "#         tf.random.set_random_seed(812)\n",
    "\n",
    "        shape         =  (self.shape[0],self.shape[1],self.shape[2],1) #--- rows, cols, thickness, channels: pixel map\n",
    "        shape_vector_input = 3\n",
    "        \n",
    "        kernel_size   =  self.kernel_size \n",
    "        epochs        =  self.max_iter[0]\n",
    "        activation    =  self.activation\n",
    "        padding       = 'same'\n",
    "        filters       =  self.n_channels\n",
    "        learning_rate = self.learning_rate_init[0]\n",
    "        #\n",
    "        ndime         =  y_train.shape[1]\n",
    "        n_train       =  X_train.shape[0]\n",
    "        n_test        =  X_test.shape[0]\n",
    "        assert        shape[0] * shape[1] * shape[2] == X_train.shape[ 1 ] - shape_vector_input\n",
    "        pixel_map_input        =  keras.Input(shape=shape)\n",
    "        vector_input           =  keras.Input(shape=(shape_vector_input,))\n",
    "        #\n",
    "\n",
    "        #------------------------------\n",
    "        #--- The network architecture\n",
    "        #------------------------------\n",
    "        x             =  layers.Conv3D(   filters     =  filters, \n",
    "                                          kernel_size =  kernel_size,\n",
    "                                          activation  =  activation,\n",
    "                                          padding     =  padding\n",
    "                                       )(pixel_map_input)\n",
    "        filters       *=  2\n",
    "        for i in range( self.number_hidden_layers ):\n",
    "            x       = layers.AveragePooling3D( pool_size = 2 )( x )\n",
    "            x       = layers.Conv3D( filters       =  filters, \n",
    "                                     kernel_size   =  kernel_size,\n",
    "                                     activation    =  activation,\n",
    "                                     padding       =  padding\n",
    "                                     )(x)\n",
    "            filters *= 2\n",
    "        x       = layers.Flatten()(x)\n",
    "            \n",
    "        #--- concatenate flattened map with vector\n",
    "        combined = keras.layers.concatenate( [ x, vector_input ] )\n",
    "        \n",
    "        #--- output layer\n",
    "        outputs = layers.Dense( ndime )( combined ) #, activation=activation)(x)\n",
    "        model   = keras.Model(inputs=[pixel_map_input,vector_input], outputs=outputs)\n",
    "        if self.verbose:\n",
    "            print('cnn model summary:',model.summary())\n",
    "\n",
    "        #--- The compilation step\n",
    "        optimizer = tf.keras.optimizers.Adam( learning_rate = learning_rate )\n",
    "        model.compile( optimizer =  optimizer,\n",
    "                       loss      =  \"mean_squared_error\",\n",
    "                       metrics   =  [\"mse\"]\n",
    "                     )\n",
    "\n",
    "        model.summary()\n",
    "        #--- save best model\n",
    "#         callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnet_from_scratch.tf',  \n",
    "#                                                    monitor=\"loss\",\n",
    "#                                                   save_freq=10,\n",
    "#                                                     save_best_only=True)]\n",
    "\n",
    "        #--- “Fitting” the model X_train_transfrmd, y_train\n",
    "        nn = X_train.shape[ 1 ]\n",
    "        X_train_pixels = X_train[:,0:nn-shape_vector_input]\n",
    "        X_test_pixels  = X_test[:,0:nn-shape_vector_input]\n",
    "        assert    X_train_pixels.shape[ 1 ] ==    shape[0] * shape[1] * shape[2]\n",
    "        X_train_vector = X_train[:,nn-shape_vector_input:nn]\n",
    "        X_test_vector  = X_test[:,nn-shape_vector_input:nn]\n",
    "        assert    X_train_vector.shape[ 1 ] ==   shape_vector_input\n",
    "        \n",
    "    \n",
    "        X_train_pixels =  X_train_pixels.reshape((n_train,shape[0],shape[1],shape[2],1))\n",
    "        X_test_pixels  =  X_test_pixels.reshape((n_test,shape[0],shape[1],shape[2],1))\n",
    "        \n",
    "        model.fit( [X_train_pixels,X_train_vector], y_train, \n",
    "                   validation_data      = ( [X_test_pixels,X_test_vector], y_test ),\n",
    "#                    callbacks            = callbacks,\n",
    "                    epochs              = epochs, \n",
    "                    verbose             = self.verbose, \n",
    "                    shuffle             = False, \n",
    "#                    batch_size          = 1,\n",
    "#                     batch_size     = 128,\n",
    "#                     use_multiprocessing = True,\n",
    "#                     workers             = 4,\n",
    "                 )\n",
    "\n",
    "        #--- validation loss\n",
    "        model.save('best_model/convnetRegressorMixedInput_from_scratch.tf')\n",
    "        loss       = model.history.history['loss']\n",
    "        val_loss   = model.history.history['val_loss']\n",
    "        best_model =model #keras.models.load_model(\"best_model/convnet_from_scratch.tf\")\n",
    "\n",
    "        \n",
    "        return ( best_model, loss, val_loss ), ([X_train_pixels,X_train_vector], [X_test_pixels,X_test_vector])\n",
    "    \n",
    "    def ConvNetwork(self,X_train, y_train, X_test, y_test):\n",
    "        '''\n",
    "        Convolutional neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : array-like training x input\n",
    "        \n",
    "        y_train : array-like, training y input\n",
    "        \n",
    "        X_test : array-like test x input\n",
    "        \n",
    "        y_test : array-like, training y input\n",
    "\n",
    "        Return\n",
    "        ---------- ( , loss,  )\n",
    "        best_model : cnn object, best trained model based on on the validation loss\n",
    "        \n",
    "        loss : array-like, mse loss\n",
    "\n",
    "        val_loss : array-like, validation loss\n",
    "\n",
    "        '''\n",
    "#         tf.random.set_random_seed(812)\n",
    "\n",
    "        shape         =  (self.shape[0],self.shape[1],self.shape[2],1) #--- rows, cols, thickness, channels\n",
    "        kernel_size   =  self.kernel_size \n",
    "        epochs        =  self.max_iter[0]\n",
    "        activation    =  self.activation\n",
    "        padding       = 'same'\n",
    "        filters       =  self.n_channels\n",
    "        learning_rate = self.learning_rate_init[0]\n",
    "        #\n",
    "        ndime         =  y_train.shape[1]\n",
    "        n_train       =  X_train.shape[0]\n",
    "        n_test        =  X_test.shape[0]\n",
    "        assert        shape[0] * shape[1] * shape[2] == X_train.shape[ 1 ]\n",
    "        inputs        =  keras.Input(shape=shape)\n",
    "        #\n",
    "\n",
    "        #------------------------------\n",
    "        #--- The network architecture\n",
    "        #------------------------------\n",
    "        x             =  layers.Conv3D(   filters     =  filters, \n",
    "                                          kernel_size =  kernel_size,\n",
    "                                          activation  =  activation,\n",
    "                                          padding     =  padding\n",
    "                                       )(inputs)\n",
    "        filters       *=  2\n",
    "        for i in range( self.number_hidden_layers ):\n",
    "            x       = layers.AveragePooling3D( pool_size = 2 )( x )\n",
    "            x       = layers.Conv3D( filters       =  filters, \n",
    "                                     kernel_size   =  kernel_size,\n",
    "                                     activation    =  activation,\n",
    "                                     padding       =  padding\n",
    "                                     )(x)\n",
    "            filters *= 2\n",
    "            \n",
    "        #--- output layer\n",
    "        x       = layers.Flatten()(x)\n",
    "        outputs = layers.Dense( ndime )( x ) #, activation=activation)(x)\n",
    "        model   = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        if self.verbose:\n",
    "            print('cnn model summary:',model.summary())\n",
    "\n",
    "        #--- The compilation step\n",
    "        optimizer = tf.keras.optimizers.Adam( learning_rate = learning_rate )\n",
    "        model.compile( optimizer =  optimizer,\n",
    "                       loss      =  \"mean_squared_error\",\n",
    "                       metrics   =  [\"mse\"]\n",
    "                     )\n",
    "\n",
    "        #--- save best model\n",
    "#         callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnet_from_scratch.tf',  \n",
    "#                                                    monitor=\"loss\",\n",
    "#                                                   save_freq=10,\n",
    "#                                                     save_best_only=True)]\n",
    "\n",
    "        #--- “Fitting” the model X_train_transfrmd, y_train\n",
    "        X_train_reshaped =  X_train.reshape((n_train,shape[0],shape[1],shape[2],1))\n",
    "        X_test_reshaped  =  X_test.reshape((n_test,shape[0],shape[1],shape[2],1))\n",
    "        model.fit( X_train_reshaped, y_train, \n",
    "                   validation_data      = ( X_test_reshaped, y_test ),\n",
    "#                    callbacks            = callbacks,\n",
    "                    epochs              = epochs, \n",
    "                    verbose             = self.verbose, \n",
    "                    shuffle             = False, \n",
    "#                    batch_size          = 1,\n",
    "#                     batch_size     = 128,\n",
    "#                     use_multiprocessing = True,\n",
    "#                     workers             = 4,\n",
    "                 )\n",
    "\n",
    "        #--- validation loss\n",
    "        model.save('best_model/convnetRegressor_from_scratch.tf')\n",
    "        loss       = model.history.history['loss']\n",
    "        val_loss   = model.history.history['val_loss']\n",
    "        best_model =model #keras.models.load_model(\"best_model/convnet_from_scratch.tf\")\n",
    "\n",
    "        \n",
    "        return ( best_model, loss, val_loss ), (X_train_reshaped, X_test_reshaped)\n",
    "    \n",
    "    def ConvNetworkMultiLabelClassifier(self,X_train, y_train, X_test, y_test):\n",
    "        '''\n",
    "        Convolutional neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : array-like training x input\n",
    "        \n",
    "        y_train : array-like, training y input\n",
    "        \n",
    "        X_test : array-like test x input\n",
    "        \n",
    "        y_test : array-like, training y input\n",
    "\n",
    "        Return\n",
    "        ---------- ( , loss,  )\n",
    "        best_model : cnn object, best trained model based on on the validation loss\n",
    "        \n",
    "        loss : array-like, mse loss\n",
    "\n",
    "        val_loss : array-like, validation loss\n",
    "\n",
    "        '''\n",
    "#         tf.random.set_random_seed(812)\n",
    "\n",
    "        shape         =  (self.shape[0],self.shape[1],self.shape[2],1) #--- rows, cols, thickness, channels\n",
    "        kernel_size   =  self.kernel_size \n",
    "        epochs        =  self.max_iter[0]\n",
    "        activation    =  'relu' #self.activation\n",
    "        padding       = 'same'\n",
    "        filters       =  self.n_channels\n",
    "        learning_rate = self.learning_rate_init[0]\n",
    "        #\n",
    "        ndime         =  y_train.shape[1]\n",
    "        n_train       =  X_train.shape[0]\n",
    "        n_test        =  X_test.shape[0]\n",
    "        assert        shape[0] * shape[1] * shape[2] == X_train.shape[ 1 ]\n",
    "        inputs        =  keras.Input(shape=shape)\n",
    "        #\n",
    "\n",
    "        #------------------------------\n",
    "        #--- The network architecture\n",
    "        #------------------------------\n",
    "        x             =  layers.Conv3D(   filters     =  filters, \n",
    "                                          kernel_size =  kernel_size,\n",
    "                                          activation  =  activation,\n",
    "                                          padding     =  padding\n",
    "                                       )(inputs)\n",
    "        filters       *=  2\n",
    "        for i in range( self.number_hidden_layers ):\n",
    "            x       = layers.AveragePooling3D( pool_size = 2 )( x )\n",
    "            x       = layers.Conv3D( filters       =  filters, \n",
    "                                     kernel_size   =  kernel_size,\n",
    "                                     activation    =  activation,\n",
    "                                     padding       =  padding\n",
    "                                     )(x)\n",
    "            filters *= 2\n",
    "            \n",
    "        #--- output layer\n",
    "        x       = layers.Flatten()(x)\n",
    "        outputs = layers.Dense( ndime, activation='sigmoid' )( x ) #, activation=activation)(x)\n",
    "        model   = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        if self.verbose:\n",
    "            print('cnn model summary:',model.summary())\n",
    "\n",
    "        #--- The compilation step\n",
    "        optimizer = tf.keras.optimizers.Adam( learning_rate = learning_rate )\n",
    "        model.compile( optimizer =  optimizer,\n",
    "                       loss      =  \"binary_crossentropy\",\n",
    "                       metrics   =  [\"mse\"]\n",
    "                     )\n",
    "\n",
    "        #--- save best model\n",
    "#         callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnet_from_scratch.tf',  \n",
    "#                                                    monitor=\"loss\",\n",
    "#                                                   save_freq=10,\n",
    "#                                                     save_best_only=True)]\n",
    "\n",
    "        #--- “Fitting” the model X_train_transfrmd, y_train\n",
    "        X_train_reshaped =  X_train.reshape((n_train,shape[0],shape[1],shape[2],1))\n",
    "        X_test_reshaped  =  X_test.reshape((n_test,shape[0],shape[1],shape[2],1))\n",
    "        model.fit( X_train_reshaped, y_train, \n",
    "                   validation_data      = ( X_test_reshaped, y_test ),\n",
    "#                    callbacks            = callbacks,\n",
    "                    epochs              = epochs, \n",
    "                    verbose             = self.verbose, \n",
    "                    shuffle             = False, \n",
    "#                    batch_size          = 1,\n",
    "#                     batch_size     = 128,\n",
    "#                     use_multiprocessing = True,\n",
    "#                     workers             = 4,\n",
    "                 )\n",
    "\n",
    "        #--- validation loss\n",
    "        model.save('best_model/convnetMultiLabelClassifier_from_scratch.tf')\n",
    "        loss       = model.history.history['loss']\n",
    "        val_loss   = model.history.history['val_loss']\n",
    "        best_model =model #keras.models.load_model(\"best_model/convnet_from_scratch.tf\")\n",
    "\n",
    "        \n",
    "        return ( best_model, loss, val_loss ), (X_train_reshaped, X_test_reshaped)\n",
    "    \n",
    "    def ConvNetworkClassifier(self,y,\n",
    "                               random_state=1\n",
    "                               ):\n",
    "        '''\n",
    "        Convolutional neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : array-like training x input\n",
    "        \n",
    "        y_train : array-like, training y input\n",
    "        \n",
    "        X_test : array-like test x input\n",
    "        \n",
    "        y_test : array-like, training y input\n",
    "\n",
    "        Return\n",
    "        ---------- ( , loss,  )\n",
    "        best_model : cnn object, best trained model based on on the validation loss\n",
    "        \n",
    "        loss : array-like, mse loss\n",
    "\n",
    "        val_loss : array-like, validation loss\n",
    "\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('dim(y)=',y.shape)\n",
    "        \n",
    "\n",
    "        #---------------\n",
    "        #--- zscore X\n",
    "        #---------------        \n",
    "        X      = np.c_[self.descriptors ]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "        X      = scaler.transform( X )\n",
    "    \n",
    "        if self.verbose:\n",
    "            print('X.shape:=',X.shape)\n",
    "            \n",
    "            \n",
    "            \n",
    "        #-----------------------\n",
    "        #--- train-test split\n",
    "        #-----------------------\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                            random_state=random_state)\n",
    "\n",
    "        \n",
    "        \n",
    "        #---- set model parameters\n",
    "        shape         =  (self.shape[0],self.shape[1],self.shape[2],1) #--- rows, cols, thickness, channels\n",
    "        kernel_size   =  self.kernel_size \n",
    "        epochs        =  self.max_iter[0]\n",
    "        activation    =  self.activation\n",
    "        padding       = 'same'\n",
    "        filters       =  self.n_channels\n",
    "        learning_rate = self.learning_rate_init[0]\n",
    "        #\n",
    "        ndime         =  y_train.shape[1]\n",
    "        n_train       =  X_train.shape[0]\n",
    "        n_test        =  X_test.shape[0]\n",
    "        assert        shape[0] * shape[1] * shape[2] == X_train.shape[ 1 ]\n",
    "        inputs        =  keras.Input(shape=shape)\n",
    "\n",
    "        #------------------------------\n",
    "        #--- The network architecture\n",
    "        #------------------------------\n",
    "        x             =  layers.Conv3D(   filters     =  filters, \n",
    "                                          kernel_size =  kernel_size,\n",
    "                                          activation  =  activation,\n",
    "                                          padding     =  padding\n",
    "                                       )(inputs)\n",
    "        filters       *=  2\n",
    "        for i in range( self.number_hidden_layers ):\n",
    "            x       = layers.AveragePooling3D( pool_size = 2 )( x )\n",
    "            x       = layers.Conv3D( filters       =  filters, \n",
    "                                     kernel_size   =  kernel_size,\n",
    "                                     activation    =  activation,\n",
    "                                     padding       =  padding\n",
    "                                     )(x)\n",
    "            filters *= 2\n",
    "            \n",
    "        #--- output layer\n",
    "        x       = layers.Flatten()(x)\n",
    "        outputs = layers.Dense( ndime, activation=activation)(x)\n",
    "        model   = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        if self.verbose:\n",
    "            print('cnn model summary:',model.summary())\n",
    "\n",
    "        #--- The compilation step\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate) #--- compilation step\n",
    "        model.compile( optimizer =  optimizer,\n",
    "                       loss=[\"binary_crossentropy\",\"sparse_categorical_crossentropy\"][1],\n",
    "                       metrics   =  [\"mse\"]\n",
    "                     )\n",
    "\n",
    "        #--- save best model\n",
    "        !mkdir best_model\n",
    "#         callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnetClassifier_from_scratch.tf',  \n",
    "#                                                     monitor=\"accuracy\",\n",
    "#                                                     save_freq=10,\n",
    "#                                                     save_best_only=True)]\n",
    "\n",
    "        #--- “Fitting” the model X_train_transfrmd, y_train\n",
    "        X_train_reshaped =  X_train.reshape((n_train,shape[0],shape[1],shape[2],1))\n",
    "        X_test_reshaped  =  X_test.reshape((n_test,shape[0],shape[1],shape[2],1))\n",
    "        model.fit( X_train_reshaped, y_train, \n",
    "                   validation_data      = ( X_test_reshaped, y_test ),\n",
    "                   callbacks            = callbacks,\n",
    "                    epochs              = epochs, \n",
    "                    verbose             = self.verbose, \n",
    "                    shuffle             = False, \n",
    "#                     batch_size     = 128,\n",
    "#                     use_multiprocessing = True,\n",
    "#                     workers             = 4,\n",
    "                 )\n",
    "\n",
    "        #--- validation loss\n",
    "        model.save('best_model/convnetClassifier_from_scratch.tf')\n",
    "        loss       = model.history.history['loss']\n",
    "        val_loss   = model.history.history['val_loss']\n",
    "        best_model = model #keras.models.load_model(\"best_model/convnetClassifier_from_scratch.tf\")\n",
    "\n",
    "        \n",
    "        return ( best_model, loss, val_loss ), (X_train_reshaped, X_test_reshaped)\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def Duplicate(X, new_size = 100 ):\n",
    "        m = m0 = X.shape[ 0 ]\n",
    "#        n = X.shape[ 1 ]\n",
    "        augmented_x = np.copy( X )\n",
    "\n",
    "        while m <= new_size:\n",
    "            augmented_x = np.concatenate([augmented_x,X],axis = 0)\n",
    "            #\n",
    "            m = augmented_x.shape[ 0 ]\n",
    "\n",
    "        assert m > new_size\n",
    "\n",
    "        return augmented_x[:new_size]\n",
    "\n",
    "    @staticmethod    \n",
    "    def AddGaussianNoise(X,scale = 0.1):\n",
    "\n",
    "        epsilon_x = np.random.normal(scale=scale,size=X.size).reshape(X.shape)\n",
    "        X += epsilon_x\n",
    "        \n",
    "    \n",
    "    def PrintDensityMap(self, atomIndx, irun, fout):\n",
    "        with open(fout,'w') as fp:\n",
    "                    disp           = np.c_[self.perAtomData[ irun ].iloc[atomIndx]['ux uy uz'.split()]].flatten()\n",
    "                    df             = pd.DataFrame(np.c_[self.Positions[ irun ].T,self.Descriptors[ irun ][atomIndx]],\n",
    "                                                  columns='x y z mass'.split())\n",
    "                    utl.PrintOvito(df, fp, 'disp = %s'%disp, attr_list='x y z mass'.split())\n",
    "                    \n",
    "    @staticmethod\n",
    "    def Zscore( X, **kwargs ):\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "        \n",
    "        if 'save_model' in kwargs:\n",
    "            pickle.dump( scaler, open( kwargs[ 'save_model' ], 'wb' ) )\n",
    "        return scaler.transform( X )\n",
    "\n",
    "#     def SaveConf(self,fout):\n",
    "#         with open(fout,'w') as fp:\n",
    "#             np.savetxt(fp,np.c_[self.perAtomData],header=' '.join(list(self.perAtomData.keys())))\n",
    "\n",
    "#     def Test(self,y,\n",
    "#                                    random_state=1\n",
    "#                                    ):\n",
    "#             '''\n",
    "#             Convolutional neural network.\n",
    "\n",
    "#             Parameters\n",
    "#             ----------\n",
    "#             X_train : array-like training x input\n",
    "\n",
    "#             y_train : array-like, training y input\n",
    "\n",
    "#             X_test : array-like test x input\n",
    "\n",
    "#             y_test : array-like, training y input\n",
    "\n",
    "#             Return\n",
    "#             ---------- ( , loss,  )\n",
    "#             best_model : cnn object, best trained model based on on the validation loss\n",
    "\n",
    "#             loss : array-like, mse loss\n",
    "\n",
    "#             val_loss : array-like, validation loss\n",
    "\n",
    "#             '''\n",
    "\n",
    "#             if self.verbose:\n",
    "#                 print('dim(y)=',y.shape)\n",
    "\n",
    "#             ndime  = y.shape[1] #--- dimension of the target vector\n",
    "\n",
    "\n",
    "#             #---------------\n",
    "#             #--- zscore X\n",
    "#             #---------------        \n",
    "#             X      = np.c_[self.descriptors ]\n",
    "#             scaler = StandardScaler()\n",
    "#             scaler.fit(X)\n",
    "#             X      = scaler.transform( X )\n",
    "\n",
    "#             if self.verbose:\n",
    "#                 print('X.shape:=',X.shape)\n",
    "\n",
    "\n",
    "\n",
    "#             #-----------------------\n",
    "#             #--- train-test split\n",
    "#             #-----------------------\n",
    "#             X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "#                                                                 random_state=random_state)\n",
    "\n",
    "\n",
    "\n",
    "#             #---- set model parameters\n",
    "#             shape         =  (self.shape[0],self.shape[1],self.shape[2],1) #--- rows, cols, thickness, channels\n",
    "#             kernel_size   =  self.kernel_size \n",
    "#             epochs        =  self.max_iter[0]\n",
    "#             activation    =  self.activation\n",
    "#             padding       = 'same'\n",
    "#             filters       =  self.n_channels\n",
    "#             learning_rate = self.learning_rate_init[0]\n",
    "#             #\n",
    "#             ndime         =  y_train.shape[1]\n",
    "#             n_train       =  X_train.shape[0]\n",
    "#             n_test        =  X_test.shape[0]\n",
    "#             assert        shape[0] * shape[1] * shape[2] == X_train.shape[ 1 ]\n",
    "#             inputs        =  keras.Input(shape=shape)\n",
    "#             #\n",
    "#     #         pdb.set_trace()\n",
    "#             #------------------------------\n",
    "#             #--- The network architecture\n",
    "#             #------------------------------\n",
    "#             model     = keras.Sequential([\n",
    "#                 layers.Dense(self.hidden_layer_size, activation=\"relu\"),\n",
    "#     #             layers.Dense(self.hidden_layer_size), #activation=\"relu\"),\n",
    "#                 layers.Dense(2, activation=\"softmax\")\n",
    "#                 ])\n",
    "#             optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate) #--- compilation step\n",
    "\n",
    "#             model.compile( optimizer=\"rmsprop\",\n",
    "#                            loss=\"sparse_categorical_crossentropy\",\n",
    "#                            metrics=[\"mse\"])\n",
    "\n",
    "\n",
    "#             #--- “Fitting” the model X_train_transfrmd, y_train\n",
    "#             X_train_reshaped =  X_train \n",
    "#             X_test_reshaped  =  X_test\n",
    "#             model.fit( X_train_reshaped, y_train, \n",
    "#                        validation_data      = ( X_test_reshaped, y_test ),\n",
    "#     #                     callbacks=callbacks,\n",
    "#                         epochs              = epochs, \n",
    "#                         verbose             = self.verbose, \n",
    "#                         shuffle             = False, \n",
    "#     #                     batch_size     = 128,\n",
    "#                         use_multiprocessing = True,\n",
    "#                         workers             = 4,\n",
    "#                      )        \n",
    "\n",
    "\n",
    "#             #--- save best model\n",
    "#             !mkdir best_model\n",
    "#     #         callbacks=[keras.callbacks.ModelCheckpoint( filepath='best_model/convnetClassifier_from_scratch.tf',  \n",
    "#     #                                                    monitor=\"val_loss\",\n",
    "#     #                                                   save_freq=10,\n",
    "#     #                                                     save_best_only=True)]\n",
    "\n",
    "\n",
    "#             #--- validation loss\n",
    "#             loss       = model.history.history['loss']\n",
    "#             val_loss   = model.history.history['val_loss']\n",
    "#             best_model = model #keras.models.load_model(\"best_model/convnet_from_scratch.tf\")\n",
    "\n",
    "\n",
    "#             !mkdir png\n",
    "#             utl.PltErr(range(len(loss)), loss,\n",
    "#                        yscale='log',\n",
    "#                        xstr='epoch',ystr='loss',\n",
    "#                        title='png/loss_classification.png',\n",
    "#                       )\n",
    "\n",
    "#     #         pdb.set_trace()\n",
    "#             #--- confusion matrix\n",
    "#             cm = confusion_matrix(y_test, model.predict_classes(X_test),\n",
    "#                              labels=[0, 1]\n",
    "#                             )\n",
    "#             print('cm=',cm)\n",
    "#             np.savetxt('png/confusion.txt',np.c_[cm])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c303869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.Catalogs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f8fc70",
   "metadata": {},
   "source": [
    "## main(): classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe6a63d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " def main():\n",
    " \n",
    "    if not eval(confParser['neural net classification']['classification']):\n",
    "        return\n",
    "    \n",
    "    nn = NeuralNetwork(\n",
    "                        hidden_layer_sizes   = eval(confParser['neural net classification']['hidden_layer_sizes']),\n",
    "                        learning_rate_init   = eval(confParser['neural net classification']['learning_rate_init']),\n",
    "                        n_iter_no_change     = eval(confParser['neural net classification']['n_iter_no_change']),\n",
    "                        tol                  = eval(confParser['neural net classification']['tol']),\n",
    "                        max_iter             = eval(confParser['neural net classification']['max_iter']),\n",
    "                        alpha                = eval(confParser['neural net classification']['alpha']),\n",
    "                        hidden_layer_size    = eval(confParser['neural net classification']['hidden_layer_size']),\n",
    "                        fully_connected      = eval(confParser['neural net classification']['fully_connected']),\n",
    "                        implementation       = eval(confParser['neural net classification']['implementation']),\n",
    "                        cnn                  = eval(confParser['neural net classification']['cnn']),\n",
    "                        n_channels           = eval(confParser['neural net classification']['n_channels']),\n",
    "                        kernel_size          = eval(confParser['neural net classification']['kernel_size']),\n",
    "                        activation           = eval(confParser['neural net classification']['activation']),\n",
    "                        number_hidden_layers = eval(confParser['neural net classification']['number_hidden_layers']),\n",
    "                        n_train              = eval(confParser['neural net classification']['n_train']),\n",
    "                        best_model           = 'best_model',\n",
    "                        verbose              = True \n",
    "                    )\n",
    "    \n",
    "    nn.Parse2nd( path  = confParser['neural net']['input_path'],\n",
    "              nruns = eval(confParser['neural net classification']['nruns']))\n",
    "\n",
    "    nn.Combine2nd() #--- concat. descriptors\n",
    "\n",
    "    #--- classifier\n",
    "    nn.TrainClassifier()\n",
    "#        nn.Test(np.c_[nn.perAtomData.defect_label].astype(int))\n",
    "    \n",
    "    \n",
    "    return nn\n",
    "\n",
    "#model_clf = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0db348",
   "metadata": {},
   "source": [
    "## main(): regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01971ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " def main():\n",
    " \n",
    "    if not eval(confParser['neural net regression']['regression']):\n",
    "        return\n",
    "\n",
    "    nn = NeuralNetwork(\n",
    "                        hidden_layer_sizes   = eval(confParser['neural net regression']['hidden_layer_sizes']),\n",
    "                        learning_rate_init   = eval(confParser['neural net regression']['learning_rate_init']),\n",
    "                        n_iter_no_change     = eval(confParser['neural net regression']['n_iter_no_change']),\n",
    "                        tol                  = eval(confParser['neural net regression']['tol']),\n",
    "                        max_iter             = eval(confParser['neural net regression']['max_iter']),\n",
    "                        alpha                = eval(confParser['neural net regression']['alpha']),\n",
    "                        hidden_layer_size    = eval(confParser['neural net regression']['hidden_layer_size']),\n",
    "                        fully_connected      = eval(confParser['neural net regression']['fully_connected']),\n",
    "                        implementation       = eval(confParser['neural net regression']['implementation']),\n",
    "                        cnn                  = eval(confParser['neural net regression']['cnn']),\n",
    "                        n_channels           = eval(confParser['neural net regression']['n_channels']),\n",
    "                        kernel_size          = eval(confParser['neural net regression']['kernel_size']),\n",
    "                        activation           = eval(confParser['neural net regression']['activation']),\n",
    "                        number_hidden_layers = eval(confParser['neural net regression']['number_hidden_layers']),\n",
    "                        n_train              = eval(confParser['neural net regression']['n_train']),\n",
    "                        du                   = eval(confParser['neural net regression']['du']),\n",
    "                        umax                 = eval(confParser['neural net regression']['umax']),\n",
    "                        best_model           = 'best_model',\n",
    "                        verbose              = True \n",
    "                    )\n",
    "    \n",
    "    nn.Parse2nd( path  = confParser['neural net']['input_path'],\n",
    "              nruns = eval(confParser['neural net regression']['nruns']))\n",
    "\n",
    "    nn.Combine2nd() #--- concat. descriptors\n",
    "\n",
    "    nn.TrainRegressorTransitionPaths()\n",
    "    nn.TrainRegressorBarriers()\n",
    "    \n",
    "    return nn\n",
    "\n",
    "#model_regr = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864aefa4",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a5a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if eval(confParser['flags']['RemoteMachine']):\n",
    "        return\n",
    "    \n",
    "\n",
    "    \n",
    "    #--- ann\n",
    "    number_hidden_layers  = dict(zip(range(4),[1]))\n",
    "    hidden_layer_size     = dict(zip(range(4),[1]))\n",
    "    n_channels            = dict(zip(range(4),[1]))\n",
    "    activations           = dict(zip(range(20),['relu']))\n",
    "#     string[ inums ] = \"\\t\\'5\\':\\'neuralNet/20x20/ann/classifier/layer%s/channel%s/activation%s/layer_size%s\\',\\n\" % (key_n,key_c,key_a,key_h) #--- change job name\n",
    "    \n",
    "    #--- cnn\n",
    "#     number_hidden_layers  = dict(zip(range(4),[1,2,3]))\n",
    "#     hidden_layer_size     = dict(zip(range(4),[1]))\n",
    "#     n_channels            = dict(zip(range(4),[8,16,32,64]))\n",
    "#     activations           = dict(zip(range(20),['linear']))\n",
    "\n",
    "    runs = range(8)\n",
    "    \n",
    "    legend = utl.Legends()\n",
    "    legend.Set(fontsize=14,bbox_to_anchor=(1.5, 0.3, 0.5, 0.5))\n",
    "    symbols = utl.Symbols()\n",
    "    \n",
    "    nphi = len(number_hidden_layers)\n",
    "    #---\n",
    "    count = 0\n",
    "    ax = utl.PltErr(None, None, Plot=False )\n",
    "    for key_n in number_hidden_layers:\n",
    "        number_hidden_layer = number_hidden_layers[key_n]\n",
    "#         if number_hidden_layer != 2:\n",
    "#             continue\n",
    "        for key_c in n_channels:\n",
    "            n_channel = n_channels[key_c]\n",
    "#             if n_channel != 16:\n",
    "#                 continue\n",
    "            for key_a in activations:\n",
    "                activation = activations[key_a]\n",
    "                for key_h in hidden_layer_size:\n",
    "                    nsize = hidden_layer_size[key_h]\n",
    "\n",
    "        #---\t\n",
    "#                    path = 'neuralNet/20x20/cnn/classifier/layer%s/channel%s/activation%s/layer_size%s'%(key_n,key_c,key_a,key_h) #--- change job name\n",
    "                    path = 'neuralNet/ni/interestitials/test2nd' #--- change job name\n",
    "                    fp = ['confusion.txt', 'val_loss_classification.txt','loss.txt'][ 2 ]\n",
    "                    for irun in runs:\n",
    "                        try:\n",
    "                            data = np.loadtxt('%s/Run%s/png/%s'%(path,irun,fp))\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                        if fp == 'confusion.txt':\n",
    "                            accuracy_crystals = data[0,0]/np.sum(data[0,:])\n",
    "                            accuracy_defects = data[1,1]/np.sum(data[1,:])\n",
    "                            print(data)\n",
    "                            utl.PltErr(accuracy_crystals, accuracy_defects,\n",
    "                               attrs=symbols.GetAttrs(count=count%7,nevery=800,\n",
    "                                    label='%s layers, %s channels, act. %s'%(number_hidden_layer,n_channel,activation)), \n",
    "                                       Plot=False,\n",
    "                                       ax=ax,\n",
    "                                       )\n",
    "                        else:\n",
    "                            epoch = data[:,0]\n",
    "                            loss = data[:,1]\n",
    "                            val_loss = data[:,2]\n",
    "\n",
    "                            utl.PltErr(epoch, loss,\n",
    "                               attrs=symbols.GetAttrs(count=count%7,nevery=10,\n",
    "                                    label='train:%s layers, %s channels, act. %s'%(number_hidden_layer,n_channel,activation)), \n",
    "                                       Plot=False,\n",
    "                                       ax=ax,\n",
    "                                       )\n",
    "                            utl.PltErr(epoch, val_loss,\n",
    "                               attrs=symbols.GetAttrs(count=(count+1)%7,nevery=10,\n",
    "                                    label='test:%s layers, %s channels, act. %s'%(number_hidden_layer,n_channel,activation)), \n",
    "                                       Plot=False,\n",
    "                                       ax=ax,\n",
    "                                       )\n",
    "                    count += 1\n",
    "    ax = utl.PltErr(None, None,\n",
    "                        yscale='log',xscale='log',\n",
    "                       xstr='epoch',ystr='validation loss',\n",
    "#                     ylim=(1e-1,1e1),\n",
    "                    ax=ax,\n",
    "                    legend=legend.Get(),\n",
    "                       title='png/training_loss.png',\n",
    "                   )\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6dc865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm=np.loadtxt('neuralNet/ni/interestitials/test2nd/Run0/png/confusionMultiLabelClassification.txt').astype(int)\n",
    "# falseNegative = list(map(lambda x: 1.0*x[0]/(x[0]+x[1]), cm))\n",
    "# truePositive  = list(map(lambda x: 1.0*x[3]/(x[2]+x[3]), cm))\n",
    "# filtr  = cm[:,3] > 0\n",
    "# cm[filtr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74daa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# legend = utl.Legends()\n",
    "# legend.Set(fontsize=14,bbox_to_anchor=(1.5, 0.3, 0.5, 0.5))\n",
    "# symbols = utl.Symbols()\n",
    "\n",
    "# fp = ['confusion.txt', 'val_loss_classification.txt'][0]\n",
    "# data = np.loadtxt('neuralNet/ni/kmc/inactive/Run0/png/%s'%(fp))\n",
    "# ax = utl.PltErr(None, None, Plot=False )\n",
    "# if fp == 'confusion.txt':\n",
    "#     accuracy_crystals = data[0,0]/np.sum(data[0,:])\n",
    "#     accuracy_defects = data[1,1]/np.sum(data[1,:])\n",
    "#     print(data)\n",
    "#     utl.PltErr(accuracy_crystals, accuracy_defects,\n",
    "#        attrs=symbols.GetAttrs(count=0,nevery=800,\n",
    "#             ), \n",
    "#                Plot=False,\n",
    "#                ax=ax,\n",
    "#                )\n",
    "# else:\n",
    "#     epoch = data[:,0]\n",
    "#     loss = data[:,1]\n",
    "#     val_loss = data[:,2]\n",
    "\n",
    "#     utl.PltErr(epoch, val_loss,\n",
    "#        attrs=symbols.GetAttrs(count=0,nevery=10,\n",
    "#             ), \n",
    "#                Plot=False,\n",
    "#                ax=ax,\n",
    "#                )\n",
    "    \n",
    "# ax = utl.PltErr(None, None,\n",
    "# yscale='log',xscale='log',\n",
    "# xstr='epoch',ystr='validation loss',\n",
    "# #                     ylim=(1e-1,1e1),\n",
    "# ax=ax,\n",
    "# # legend=legend.Get(),\n",
    "# title='png/training_loss.png',\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f506974a",
   "metadata": {},
   "source": [
    "## test example: 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be92045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# y=np.c_[[1.725966,1.725967],\n",
    "#             [-1.725966,1.725967],\n",
    "#             [-1.725966,-1.725967],\n",
    "#             [1.725966,-1.725967],\n",
    "#            ].T\n",
    "\n",
    "# X=np.concatenate([list(map(lambda x:np.load('png/descriptor%s.npy'%x).flatten(),range(4)))],axis=1)\n",
    "\n",
    "# #--- zscore\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X)\n",
    "# X_transfrmd = scaler.transform( X )\n",
    "\n",
    "# X_train_transfrmd, X_test_transfrmd, y_train, y_test = train_test_split(X_transfrmd, y, test_size=0.25)\n",
    "# print(y_test)\n",
    "\n",
    "\n",
    "# print(X_train_transfrmd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69377834",
   "metadata": {},
   "source": [
    "### fully connected in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c56d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #        pdb.set_trace()\n",
    "# #--- tune parameters\n",
    "\n",
    "# #--- train\n",
    "# mlp = MLPRegressor(random_state=1,\n",
    "#                     verbose=True,\n",
    "#                    n_iter_no_change=100000,\n",
    "#                     max_iter=100,#00,\n",
    "#                    hidden_layer_sizes=(1000,1000),\n",
    "# #                    shuffle=False,\n",
    "# #                     alpha=1e-1,\n",
    "\n",
    "#                   )\n",
    "# mlp.fit(X_train_transfrmd,y_train)\n",
    "\n",
    "# #--- validate\n",
    "# !mkdir png\n",
    "# utl.PltErr(range(len(mlp.loss_curve_)), mlp.loss_curve_,\n",
    "#            attrs={'fmt':'-'},\n",
    "#            yscale='log',xscale='log',\n",
    "# #           xlim=(1,self.max_iter[0]),\n",
    "#            xstr='epoch',ystr='loss',\n",
    "#            title='png/loss.png',\n",
    "#           )\n",
    "\n",
    "# # #         pdb.set_trace()\n",
    "# y_pred =mlp.predict(X_test_transfrmd)        \n",
    "# y_pred_train = mlp.predict(X_train_transfrmd)        \n",
    "# for idime, xstr in zip(range(2),'ux uy'.split()):\n",
    "#     ax = utl.PltErr(None,None,Plot=False)\n",
    "#     #\n",
    "#     utl.PltErr(y_test[:,idime],y_pred[:,idime],\n",
    "#                attrs={'fmt':'x','color':'red','zorder':10,'markersize':6},\n",
    "#                ax=ax,\n",
    "#                Plot = False,\n",
    "\n",
    "#               )\n",
    "#     #\n",
    "#     utl.PltErr(y_train[:,idime],y_pred_train[:,idime],\n",
    "#                attrs={'fmt':'.','color':'blue','zorder':1,'markersize':6},\n",
    "#                ax=ax,\n",
    "#                Plot = False,\n",
    "\n",
    "#               )\n",
    "#     #\n",
    "#     utl.PltErr(None,None,Plot=False,\n",
    "#                    title='png/scatter%s.png'%idime,\n",
    "#                     ax=ax,\n",
    "#                xstr='%s actual'%xstr,ystr='%s predicted'%xstr,\n",
    "#                xlim=(-3,3),ylim=(-3,3),\n",
    "#                    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1353cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp.best_loss_, mlp.loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ux,uy=mlp.predict(X_test_transfrmd)[0]\n",
    "# ax=utl.PltErr([0,ux],[0,uy],\n",
    "#               Plot=False\n",
    "#           )\n",
    "# utl.PltErr([0,y_test[0][0]],[0,y_test[0][1]],\n",
    "#            xlim=(-3,3),ylim=(-3,3),\n",
    "#             ax=ax\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ux,uy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce616c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = utl.PltErr(None,None,Plot=False)\n",
    "\n",
    "# for i in range(2):\n",
    "#     utl.PltErr(range(data.descriptors[0,:].shape[0]),data.descriptors[i,:],\n",
    "#               attrs={'fmt':'-'},#,'color':'C0'},\n",
    "#                xscale='log',yscale='log',\n",
    "#                ax=ax,\n",
    "#                Plot=False,\n",
    "#               )\n",
    "\n",
    "# utl.PltErr(range(data.descriptors[100,:].shape[0]),data.descriptors[100,:],\n",
    "#           attrs={'fmt':'-','color':'C0'},\n",
    "#            xscale='log',yscale='log',\n",
    "#            ax=ax,\n",
    "#            Plot=False,\n",
    "#           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5598bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.Spectra(nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d061978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPRegressor\n",
    "# from sklearn.datasets import make_regression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X, y = make_regression(n_samples=200, random_state=1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "#                                                     random_state=1)\n",
    "# regr = MLPRegressor(verbose=False,\n",
    "#                     random_state=1, \n",
    "# #                     learning_rate='adaptive',\n",
    "# #                    early_stopping=True, \n",
    "#                      n_iter_no_change=1, \n",
    "#                     tol=1e-2,\n",
    "#                      max_iter=10000000,\n",
    "# #                     solver='sgd',\n",
    "#                    ).fit(X_train, y_train)\n",
    "# regr.tol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5eecba",
   "metadata": {},
   "source": [
    "### fully connected in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a27a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #--- The network architecture\n",
    "# model = keras.Sequential([\n",
    "#     layers.Dense(512), #activation=\"relu\"),\n",
    "# #     layers.Dense(1000), #activation=\"relu\"),\n",
    "#     layers.Dense(2) #, activation=\"relu\")\n",
    "#     ])\n",
    "\n",
    "# #--- The compilation step\n",
    "# optimizer = tf.keras.optimizers.Adam() #learning_rate=1e-4)\n",
    "# model.compile( optimizer=optimizer,#\"rmsprop\",\n",
    "#                loss=\"mean_squared_error\",#\"sparse_categorical_crossentropy\",\n",
    "#                metrics=[\"mse\"]\n",
    "#              )\n",
    "\n",
    "# #--- Preparing the image data\n",
    "# # train_images = train_images.reshape((60000, 28 * 28))\n",
    "# # train_images = train_images.astype(\"float32\") / 255\n",
    "# # test_images = test_images.reshape((10000, 28 * 28))\n",
    "# # test_images = test_images.astype(\"float32\") / 255\n",
    "\n",
    "# #--- “Fitting” the model X_train_transfrmd,y_train\n",
    "# model.fit(X_train_transfrmd, y_train, \n",
    "#             validation_data=(X_test_transfrmd, y_test),\n",
    "\n",
    "#           epochs=100, verbose=False)#, batch_size=128)\n",
    "\n",
    "# loss = model.history.history['loss']\n",
    "# val_loss = model.history.history['val_loss']\n",
    "# #--- validate\n",
    "\n",
    "# ax = utl.PltErr(range(len(val_loss)), val_loss,\n",
    "#            attrs={'fmt':'-'}, Plot=False,\n",
    "#           )\n",
    "# utl.PltErr(range(len(loss)), loss,\n",
    "#            attrs={'fmt':'-'},\n",
    "#            ax=ax,\n",
    "#            yscale='log',xscale='log',\n",
    "#            xlim=(1,100),\n",
    "#            xstr='epoch',ystr='loss',\n",
    "#            title='png/loss.png',\n",
    "#           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262922c9",
   "metadata": {},
   "source": [
    "### cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f22b71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tf.random.set_random_seed(812)\n",
    "\n",
    "# shape=(300,300,1)\n",
    "# kernel_size = (3,3)\n",
    "# epochs = 1000\n",
    "# activation = ['linear','sigmoid','relu'][0]\n",
    "# padding='same'\n",
    "# filters = 1\n",
    "# #\n",
    "# ndime = y_train.shape[1]\n",
    "# n_train = X_train_transfrmd.shape[0]\n",
    "# n_test = X_test_transfrmd.shape[0]\n",
    "# assert shape[0]*shape[1]*shape[2] == X_train_transfrmd.shape[1]\n",
    "# inputs = keras.Input(shape=shape)\n",
    "# #\n",
    "# x = layers.Conv2D(filters=filters, kernel_size=kernel_size,activation=activation,padding=padding)(inputs)\n",
    "# # x = layers.AveragePooling2D(pool_size=2)(x)\n",
    "# # x = layers.Conv2D(filters=2*filters, kernel_size=kernel_size,activation=activation,padding=padding)(x)\n",
    "# # x = layers.AveragePooling2D(pool_size=2)(x)\n",
    "# # x = layers.Conv2D(filters=4*filters, kernel_size=kernel_size,activation=activation,padding=padding)(x)\n",
    "# # x = layers.AveragePooling2D(pool_size=2)(x)\n",
    "# # x = layers.Conv2D(filters=8*filters, kernel_size=kernel_size,activation=activation,padding=padding)(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# outputs = layers.Dense( ndime, activation=activation)(x)\n",
    "\n",
    "# #--- The network architecture\n",
    "# model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "# #--- The compilation step\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5,epsilon=1e-08)\n",
    "# model.compile( optimizer=optimizer,#\"rmsprop\",\n",
    "#                loss=\"mean_squared_error\",#\"sparse_categorical_crossentropy\",\n",
    "#                metrics=[\"mse\"]\n",
    "#              )\n",
    "\n",
    "# #--- save best model \n",
    "# callbacks=[keras.callbacks.ModelCheckpoint( filepath='png/convnet_from_scratch.keras',  \n",
    "#                                            monitor=\"val_loss\",\n",
    "#                                            save_freq=10,\n",
    "#                                             save_best_only=True)]\n",
    "\n",
    "# #--- “Fitting” the model X_train_transfrmd,y_train\n",
    "# X_train_reshaped = X_train_transfrmd.reshape((n_train,shape[0],shape[1],1))\n",
    "# X_test_reshaped = X_test_transfrmd.reshape((n_test,shape[0],shape[1],1))\n",
    "# model.fit(X_train_reshaped, y_train, \n",
    "#             validation_data=(X_test_reshaped, y_test),\n",
    "#             #callbacks=callbacks,\n",
    "#           epochs=epochs, verbose=False, shuffle=False)#, batch_size=128)\n",
    "\n",
    "# loss = model.history.history['loss']\n",
    "# val_loss = model.history.history['val_loss']\n",
    "# #--- validate\n",
    "\n",
    "# ax = utl.PltErr(range(len(val_loss)), val_loss,\n",
    "#            attrs={'fmt':'-'}, Plot=False,\n",
    "#           )\n",
    "# utl.PltErr(range(len(loss)), loss,\n",
    "#            attrs={'fmt':'-'},\n",
    "#            ax=ax,\n",
    "#            yscale='log',xscale='log',\n",
    "#            xlim=(1,epochs),\n",
    "#            xstr='epoch',ystr='loss',\n",
    "#            title='png/loss.png',\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ae920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model = keras.models.load_model(\"png/convnet_from_scratch.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa9c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ux,uy=best_model.predict(X_test_reshaped)[0]\n",
    "# ax=utl.PltErr([0,ux],[0,uy],\n",
    "#               Plot=False\n",
    "#           )\n",
    "# utl.PltErr([0,y_test[0][0]],[0,y_test[0][1]],\n",
    "#            xlim=(-3,3),ylim=(-3,3),\n",
    "#             ax=ax\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf166dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (ux,uy), y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a451788",
   "metadata": {},
   "source": [
    "# gnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7362910",
   "metadata": {},
   "source": [
    "# fixed output size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ecb129",
   "metadata": {},
   "source": [
    "## build catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb50389",
   "metadata": {},
   "outputs": [],
   "source": [
    "evlist_dir = '%s/EVLIST_DIR'%confParser['input files']['input_path']\n",
    "events_dir = '%s/SPEC_EVENTS_DIR'%confParser['input files']['input_path']\n",
    "lib_path   = confParser['input files']['lib_path'].split()[0] #'../../HeaDef/postprocess'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf6e5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def ParseEvList_dir():\n",
    "    files = os.listdir(evlist_dir)\n",
    "    events={}\n",
    "    for sfile in files:\n",
    "        try:\n",
    "            kmc_step = int(sfile.split('_')[-1])\n",
    "    #        print(kmc_step)\n",
    "            filee=open('%s/%s'%(evlist_dir,sfile)) #--- open file\n",
    "            events[kmc_step] = pd.read_csv(filee,delim_whitespace=True).iloc[1:]#delimiter='')\n",
    "        except:\n",
    "            continue\n",
    "    return events\n",
    "\n",
    "def ModifyCatalog(  ):\n",
    "    '''\n",
    "    Return a dataframe including per-atom energy and defect type \n",
    "    '''        \n",
    "    kmc_step     = 1\n",
    "\n",
    "    #--- discard refined == False\n",
    "    filtr = catalog[ kmc_step ].refined == 'T'\n",
    "    catalog[ kmc_step ] = catalog[ kmc_step ][ filtr ]\n",
    "\n",
    "    #--- displacements\n",
    "    fout = 'transition_paths.json'\n",
    "    os.system('rm %s'%fout)\n",
    "    catalog[ kmc_step ].apply(lambda x:GetDispForEvents(x,fout),axis=1)\n",
    "\n",
    "    #--- modify catalog\n",
    "#     self.catalog[ kmc_step ]['refined'] = 1 #--- add column: 1\n",
    "#     self.catalog[ kmc_step ][ 'DirX DirY DirZ'.split() ] = np.c_[ list( disps_events ) ] #--- add columns: ux, uy, uz\n",
    "#     self.catalog[ kmc_step ].reset_index(drop = True, inplace=True) #--- reset index\n",
    "\n",
    "#     #--- add atom indices\n",
    "#     AtomIds     = np.c_[self.catalog[ kmc_step ].AtomId].astype(int).flatten() #--- add atom index\n",
    "#     AtomIndices = EnergyBarrier.GetDataFrameIndex(self.perAtomData, key='id', val=AtomIds)\n",
    "#     self.catalog[ kmc_step ]['AtomIndex'] = AtomIndices\n",
    "\n",
    "def GetDispForEvents( item, fout ):\n",
    "    #--- parse spec_event file including the diffusion path\n",
    "    try:\n",
    "        diffusion_path_dict = ParseSpecEvents_dir( '%s/spec_event_%s_%s_0'\\\n",
    "                                                                %( events_dir, item.AtomId, item.Spec_id ) ) \n",
    "        diffusion_path_xyz = Displacement( '%s/spec_event_%s_%s_0.xyz'\\\n",
    "                                                                 %( events_dir, item.AtomId, item.Spec_id ), 'junk.xyz' ) \n",
    "        assert diffusion_path_dict[ 'active_atom_label' ] == int( item.AtomId )\n",
    "        assert diffusion_path_dict[ 'Spec_eventId' ]      == int( item.Spec_id )\n",
    "        assert diffusion_path_dict[ 'Original_eventId' ]  == int( item.eventId )\n",
    "        assert diffusion_path_dict[ 'event_label' ][ 0 ]  == int( item.IniTopoId ) and\\\n",
    "               diffusion_path_dict[ 'event_label' ][ 1 ]  == int( item.SadTopoId ) and\\\n",
    "               diffusion_path_dict[ 'event_label' ][ 2 ]  == int( item.FinTopoId ) \n",
    "\n",
    "        #--- save as json\n",
    "        with open(fout,'a') as fp:\n",
    "            rwjs = utl.ReadWriteJson(append=True)\n",
    "            rwjs.Write( [diffusion_path_xyz.to_dict()], fp )\n",
    "        \n",
    "        return diffusion_path_dict[ 'delr_main_atom' ]\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        return [np.nan, np.nan, np.nan ]\n",
    "\n",
    "def ParseSpecEvents_dir(fp):\n",
    "    '''\n",
    "    return energy barriers associated with hopping events\n",
    "    '''\n",
    "    with open( fp ) as filee: #'%s/%s'%(self.events_dir,sfile)) #--- open file\n",
    "        xstrs             = filee.readlines()\n",
    "        Spec_eventId      = int(xstrs[0].split()[-1]) #--- specific event id\n",
    "        Original_eventId  = int(xstrs[1].split()[-1]) #--- Original eventId\n",
    "        Refinement_step   = int(xstrs[2].split()[-1]) #--- Original eventId\n",
    "        event_label       = list(map(int,xstrs[3].split()[-1:-4:-1])) #--- Original eventId\n",
    "        energy_barrier    = float(xstrs[4].split()[-1]) #--- energy\n",
    "        delr_main_atom    = list(map(float,xstrs[12].split()[-1:-4:-1]))                 \n",
    "        active_atom_label = int(xstrs[15].split()[-1])\n",
    "\n",
    "    event_label.reverse()\n",
    "    delr_main_atom.reverse()\n",
    "    return {'Spec_eventId':Spec_eventId, 'Original_eventId':Original_eventId, 'Refinement_step':Refinement_step,\\\n",
    "            'event_label':event_label,   'energy_barrier':energy_barrier,     'delr_main_atom':delr_main_atom,\\\n",
    "            'active_atom_label':active_atom_label}\n",
    "        \n",
    "def Displacement(fp, fout,verbose=True):\n",
    "    '''\n",
    "    Return total displacements \n",
    "    '''\n",
    "    !rm $fout\n",
    "#    pdb.set_trace()\n",
    "    #--- fetch parameters\n",
    "\n",
    "    #--- call ovito\n",
    "    t0 = time.time()\n",
    "    if verbose:\n",
    "        print('input :%s'%fp)\n",
    "    os.system('ovitos %s/OvitosCna.py %s %s 1 7 %s'%(lib_path,fp,fout,'header.txt'))\n",
    "    if verbose:\n",
    "        print('output disp:%s s'%(time.time()-t0))\n",
    "\n",
    "    #--- parse disp files\n",
    "    if verbose:\n",
    "        print('parsing %s'%fout)\n",
    "    t0 = time.time()\n",
    "    lmpDisp = lp.ReadDumpFile( fout )\n",
    "    lmpDisp.GetCords( ncount = sys.maxsize )\n",
    "    if verbose:\n",
    "        print('elapsed time %s s'%(time.time()-t0))\n",
    "        display(lmpDisp.coord_atoms_broken[0].head())\n",
    "    \n",
    "    dispSad = lmpDisp.coord_atoms_broken[1]['x y z'.split()] - lmpDisp.coord_atoms_broken[0]['x y z'.split()]\n",
    "    dispFin = lmpDisp.coord_atoms_broken[2]['x y z'.split()] - lmpDisp.coord_atoms_broken[0]['x y z'.split()]\n",
    "    idType_xyz = lmpDisp.coord_atoms_broken[0]['id type x y z'.split()]\n",
    "    df = pd.DataFrame(np.c_[idType_xyz, dispSad,dispFin],columns='id type x y z ux_sad uy_sad uz_sad ux_fin uy_fin uz_fin'.split())\n",
    "    \n",
    "    return df\n",
    "            \n",
    "catalog          = ParseEvList_dir()\n",
    "ModifyCatalog()\n",
    "#catalog[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "rwjs = utl.ReadWriteJson()\n",
    "transition_paths = rwjs.Read( 'transition_paths.json' )\n",
    "pd.DataFrame(transition_paths[ 1 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d74af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm 'ovito.xyz'\n",
    "for indx, item in enumerate( transition_paths ):\n",
    "    cordc = pd.DataFrame(item)\n",
    "    with open('ovito.xyz','a') as fp: \n",
    "        utl.PrintOvito(cordc, fp, 'itime=%s'%indx, attr_list='id type x y z ux_fin uy_fin uz_fin'.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2142613",
   "metadata": {},
   "source": [
    "## graph net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bdc7e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class GraphNeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dims, activation):\n",
    "        super(GraphNeuralNet, self).__init__()\n",
    "        self.output_dims = output_dims\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "        self.fc_out = nn.ModuleList([nn.Linear(hidden_dims[-1], dim) for dim in output_dims])\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x, adj_matrices):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation(hidden_layer(x))\n",
    "        outputs = []\n",
    "        for adj_matrix in adj_matrices:\n",
    "            hidden = torch.matmul(adj_matrix, x)\n",
    "            for fc_out in self.fc_out:\n",
    "                outputs.append(fc_out(hidden))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def compute_adjacency_matrices(input_data, rcut):\n",
    "    adj_matrices = []\n",
    "    \n",
    "    for positions in input_data:\n",
    "        num_atoms = positions.shape[0]\n",
    "        adj_matrix = torch.zeros((num_atoms, num_atoms), dtype=torch.float)\n",
    "        \n",
    "        for i in range(num_atoms):\n",
    "            for j in range(i + 1, num_atoms):\n",
    "                distance = torch.norm(positions[i] - positions[j])\n",
    "                if distance <= rcut:\n",
    "                    adj_matrix[i, j] = 1\n",
    "                    adj_matrix[j, i] = 1\n",
    "            assert adj_matrix[i,:].sum() > 0, 'dangling node : increase the cutoff!'\n",
    "        adj_matrices.append(adj_matrix)\n",
    "    \n",
    "    #--- assert no \n",
    "    return adj_matrices\n",
    "\n",
    "\n",
    "def augment_data(input_data, target_displacements, noise_std):\n",
    "    augmented_input_data = []\n",
    "    augmented_target_displacements = []\n",
    "\n",
    "    for data, target in zip(input_data, target_displacements):\n",
    "        # Add Gaussian noise to input data\n",
    "        noisy_data = data + torch.randn_like(data) * noise_std\n",
    "        augmented_input_data.append(noisy_data)\n",
    "\n",
    "        # Add Gaussian noise to target displacements\n",
    "        noisy_target = target + torch.randn_like(target) * noise_std\n",
    "        augmented_target_displacements.append(noisy_target)\n",
    "\n",
    "    return augmented_input_data, augmented_target_displacements\n",
    "\n",
    "def standardize_data(data, mean, std):\n",
    "    return (data - mean) / std\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e71168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(): # Example usage\n",
    "    input_dim = eval(confParser['gnn']['input_dim']) #3  # Dimensionality of atom positions (e.g., x, y, z coordinates)\n",
    "    hidden_dim = eval(confParser['gnn']['hidden_dim']) #[64]  # Dimensionality of hidden layers\n",
    "    output_dims = eval(confParser['gnn']['output_dims']) #[3]  # Dimensionality of displacement vectors for each snapshot\n",
    "    activation = eval(confParser['gnn']['activation']) #nn.ReLU() #nn.Identity()) #F.relu)\n",
    "    lr = eval(confParser['gnn']['lr'])# 1.0e-4\n",
    "    ntrain = eval(confParser['gnn']['ntrain'])#100\n",
    "    num_epochs = eval(confParser['gnn']['num_epochs'])#20000\n",
    "    noise_std   = eval(confParser['gnn']['noise_std'])#0.1\n",
    "\n",
    "    num_snapshots = len( transition_paths )\n",
    "    snapshots     = range(num_snapshots)\n",
    "\n",
    "    model = GraphNeuralNet(input_dim, hidden_dim, output_dims,activation) #\n",
    "\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Example training data\n",
    "    num_atoms = [ len(transition_paths[ i ]['id']) for i in snapshots ]\n",
    "    input_data = [torch.from_numpy( np.c_[pd.DataFrame(transition_paths[ i ])['x y z'.split()]] ).float() for i in snapshots]  \n",
    "\n",
    "\n",
    "\n",
    "    # Example target data (displacement vectors for each snapshot and each path)\n",
    "    target_displacements = [torch.from_numpy( np.c_[pd.DataFrame(transition_paths[ i ])['ux_fin uy_fin uz_fin'.split()]] ).float() for i in snapshots for dim in output_dims]\n",
    "\n",
    "\n",
    "    # Augment the dataset to have order 100 snapshots\n",
    "    augmented_input_data = []\n",
    "    augmented_target_displacements = []\n",
    "    input_data_tensor = torch.stack(input_data)\n",
    "    ntrain_initial = input_data_tensor.shape[0]*input_data_tensor.shape[1]\n",
    "    n_repeat = np.max([1,int(ntrain/ntrain_initial)])\n",
    "\n",
    "    for _ in range(n_repeat):  # Repeat the augmentation process 10 times\n",
    "        augmented_input, augmented_target = augment_data(input_data, target_displacements, noise_std)\n",
    "        augmented_input_data.extend(augmented_input)\n",
    "        augmented_target_displacements.extend(augmented_target)\n",
    "\n",
    "    adj_matrices = compute_adjacency_matrices(augmented_input_data, rcut=3.0) #[torch.randint(0, 2, (num_atoms[i], num_atoms[i])).float() for i in range(num_snapshots)]  # Random adjacency matrices for each snapshot\n",
    "\n",
    "\n",
    "\n",
    "    # Concatenate input data along a new dimension to form a single tensor\n",
    "    input_data_tensor = torch.stack(augmented_input_data)\n",
    "    print('input_data_tensor.shape:',input_data_tensor.shape)\n",
    "\n",
    "    # Standardize the augmented input data\n",
    "    mean = input_data_tensor.mean(dim=(0, 1))\n",
    "    std = input_data_tensor.std(dim=(0, 1))\n",
    "    standardized_input_data = [standardize_data(data, mean, std) for data in augmented_input_data]\n",
    "\n",
    "\n",
    "    # Convert input data to tensors\n",
    "    #input_data_tensor = torch.stack(augmented_input_data)\n",
    "    target_displacements_tensor = torch.stack(augmented_target_displacements)\n",
    "\n",
    "    total_loss_hist = []\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_displacements = model(input_data_tensor, adj_matrices)\n",
    "    #    predicted_displacements_tensor = torch.stack( predicted_displacements )\n",
    "        losses = []\n",
    "        for indx, i in enumerate(snapshots):\n",
    "            pred = predicted_displacements[ indx ][ indx ]\n",
    "            snapshot_losses = criterion(pred, augmented_target_displacements[indx])\n",
    "    #        snapshot_losses = [criterion(pred, augmented_target_displacements[indx]) for pred in predicted_displacements[indx]]\n",
    "    #        pdb.set_trace()\n",
    "            losses.append(snapshot_losses)\n",
    "    #        losses.extend(snapshot_losses)\n",
    "    #    loss = criterion(predicted_displacements_tensor, target_displacements_tensor)\n",
    "        total_loss = sum(losses)\n",
    "        total_loss.backward()\n",
    "    #    loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_hist += [total_loss.detach().numpy()]\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Total Loss: {total_loss.item()}')\n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16c5e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in model.parameters():\n",
    "#     print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09399b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = plt.figure(figsize=(10,10)).add_subplot(projection='3d',)\n",
    "\n",
    "# irun=6\n",
    "# xyz = input_data_tensor[irun] \n",
    "# adj_mat = adj_matrices[irun]\n",
    "\n",
    "\n",
    "# ax.plot(xyz[:,0],xyz[:,1],xyz[:,2],\n",
    "#         '.', ms=20,\n",
    "#         )\n",
    "\n",
    "# n=adj_mat.shape[0]\n",
    "# for i,item in enumerate(adj_mat):\n",
    "#     for j in range(i,n):\n",
    "#         if item[j]:\n",
    "#             ax.plot([xyz[i,0],xyz[j,0]],[xyz[i,1],xyz[j,1]],[xyz[i,2],xyz[j,2]],\n",
    "#                     '-', color='black',\n",
    "#                     )\n",
    "\n",
    "# u = augmented_target_displacements[irun][-1]\n",
    "\n",
    "# ax.plot([xyz[-1,0],u[0]],[xyz[-1,1],u[1]],[xyz[-1,2],u[2]],\n",
    "#         '-', color='red',\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d3c829",
   "metadata": {},
   "source": [
    "## mpnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c862a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# class MPNN(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super(MPNN, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "#         self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "#     def message_passing_layer(self, x, adj_matrices):\n",
    "#         # Ensure adj_matrices is a list of tensors\n",
    "#         adj_matrices = [adj.unsqueeze(0) for adj in adj_matrices]\n",
    "\n",
    "#         # Expand node features to include neighboring node features\n",
    "# #        expanded_x = [torch.matmul(adj.unsqueeze(0), x.unsqueeze(0)).squeeze(0) for adj in adj_matrices]\n",
    "#         expanded_x = [torch.matmul(adj,yy) for adj, yy in zip(adj_matrices,x)]\n",
    "# #        expanded_x = [torch.matmul(adj.unsqueeze(0), adj).squeeze(0) for adj in adj_matrices]\n",
    "\n",
    "#         # Concatenate node features with neighboring node features\n",
    "#         pdb.set_trace()\n",
    "#         concatenated_x = [torch.cat((x, exp_x), dim=1) for exp_x in expanded_x]\n",
    "\n",
    "#         # Apply linear transformation\n",
    "#         transformed_x = [self.fc1(cat_x) for cat_x in concatenated_x]\n",
    "\n",
    "#         # Apply activation function\n",
    "#         x = [F.relu(trans_x) for trans_x in transformed_x]\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "#     def readout_layer(self, x):\n",
    "#         # Concatenate tensors in the list along the batch dimension\n",
    "#         concatenated_x = torch.stack(x, dim=0)\n",
    "\n",
    "#         # Apply global pooling operation (e.g., mean or sum) along the batch dimension\n",
    "#         return torch.mean(concatenated_x, dim=0)\n",
    "\n",
    "#     def forward(self, x, adj_matrices):\n",
    "#         x = self.message_passing_layer(x, adj_matrices)\n",
    "#         output = self.readout_layer(x)\n",
    "#         return output\n",
    "\n",
    "# # Example usage\n",
    "# input_dim = 3  # Dimensionality of atom positions (e.g., x, y, z coordinates)\n",
    "# hidden_dim = 64  # Dimensionality of hidden layers\n",
    "# output_dim = 3  # Dimensionality of displacement vectors for each snapshot\n",
    "\n",
    "# # Create model instance\n",
    "# model = MPNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# # Define optimizer and loss function\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1.0e-4)\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# # Example training data\n",
    "# # Assuming input_data is a list of tensors containing initial positions for each snapshot\n",
    "# # Assuming adj_matrices is a list of adjacency matrices for each snapshot\n",
    "# # Assuming target_displacements is a list of tensors containing displacement vectors for each snapshot\n",
    "# # You need to replace these with your actual data\n",
    "# input_data = [torch.randn(10, input_dim).float() for _ in range(10)]  # Example random initial positions\n",
    "# adj_matrices = [torch.randint(0, 2, (10, 10)).float() for _ in range(10)]  # Example random adjacency matrices\n",
    "# target_displacements = [torch.randn(10, output_dim) for _ in range(10)]  # Example random target displacements\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 1000\n",
    "# for epoch in range(num_epochs):\n",
    "#     optimizer.zero_grad()\n",
    "#     predicted_displacements = model(input_data, adj_matrices)\n",
    "#     loss = criterion(predicted_displacements, target_displacements)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     if epoch % 100 == 0:\n",
    "#         print(f'Epoch {epoch}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d47449",
   "metadata": {},
   "source": [
    "## tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7268a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class GraphNeuralNetwork(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(GraphNeuralNetwork, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "        self.fc_out = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, x, adj_matrices):\n",
    "        # x: Node features (batch_size, num_nodes, input_dim)\n",
    "        # adj_matrices: Adjacency matrices (batch_size, num_nodes, num_nodes)\n",
    "        \n",
    "        # Apply first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # Iterate over adjacency matrices and update node features\n",
    "        for adj_matrix in adj_matrices:\n",
    "            x = tf.matmul(adj_matrix, x)\n",
    "            x = self.fc2(x)\n",
    "        \n",
    "        # Apply output fully connected layer\n",
    "        output = self.fc_out(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "def compute_adjacency_matrices(input_data, rcut):\n",
    "    adj_matrices = []\n",
    "    \n",
    "    for positions in input_data:\n",
    "        num_atoms = positions.shape[0]\n",
    "        adj_matrix = np.zeros(num_atoms*num_atoms).reshape((num_atoms, num_atoms))\n",
    "        \n",
    "        for i in range(num_atoms):\n",
    "            for j in range(i + 1, num_atoms):\n",
    "                distance = torch.norm(positions[i] - positions[j])\n",
    "                if distance <= rcut:\n",
    "                    adj_matrix[i, j] = 1\n",
    "                    adj_matrix[j, i] = 1\n",
    "            assert adj_matrix[i,:].sum() > 0, 'dangling node : increase the cutoff!'\n",
    "        adj_matrices.append(adj_matrix)\n",
    "    \n",
    "    #--- assert no \n",
    "    return np.c_[adj_matrices]\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    hidden_dim = 32  # Dimensionality of hidden layers\n",
    "    output_dim = 3  # Dimensionality of output vectors\n",
    "    snapshots = [0] #range(len(transition_paths))\n",
    "\n",
    "    model = GraphNeuralNetwork(hidden_dim, output_dim)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    # Generate dummy input data and adjacency matrices\n",
    "#     batch_size = 2\n",
    "#     num_nodes = 10\n",
    "    input_dim = 3\n",
    "\n",
    "    #x = tf.random.normal((batch_size, num_nodes, input_dim))\n",
    "    x = np.c_[[np.c_[pd.DataFrame(transition_paths[ i ])['x y z'.split()]] for i in snapshots]]\n",
    "    x = tf.convert_to_tensor(x,dtype=tf.float32)\n",
    "\n",
    "    #adj_matrices = tf.random.uniform((batch_size, num_nodes, num_nodes)) # for _ in range(2)]  # Example with 2 adjacency matrices\n",
    "    input_data = [torch.from_numpy( np.c_[pd.DataFrame(transition_paths[ i ])['x y z'.split()]] ).float() for i in snapshots]  \n",
    "    adj_matrices = compute_adjacency_matrices(torch.stack(input_data), rcut=3.0)\n",
    "    adj_matrices = tf.convert_to_tensor(adj_matrices,dtype=tf.float32)\n",
    "\n",
    "    # Example target data\n",
    "    #target = tf.random.normal((batch_size, num_nodes, output_dim))\n",
    "    target = np.c_[[np.c_[pd.DataFrame(transition_paths[ i ])['ux_fin uy_fin uz_fin'.split()]] for i in snapshots]]\n",
    "    target = tf.convert_to_tensor(target,dtype=tf.float32)\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    total_loss_hist = []\n",
    "    num_epochs = 5000 #100000\n",
    "    for epoch in range(num_epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(x, adj_matrices)\n",
    "    #         pdb.set_trace()\n",
    "            loss = loss_fn(target, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        if epoch == 0:\n",
    "            loss_min = loss.numpy()\n",
    "            best_model = model\n",
    "        total_loss_hist.append(loss.numpy())\n",
    "        if epoch % 100 == 0: \n",
    "            print('Epoch %s, Loss: %e'%(epoch,loss.numpy()))\n",
    "            if loss.numpy() < loss_min:\n",
    "                loss_min = loss.numpy()\n",
    "                best_model = model     \n",
    "    print('min loss:%e'%loss_min)\n",
    "    return best_model, num_epochs, total_loss_hist\n",
    "\n",
    "#model, num_epochs, total_loss_hist = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff93c42",
   "metadata": {},
   "source": [
    "## pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc7122c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "\n",
    "# For downloading pre-trained models\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "# PyTorch Lightning\n",
    "import lightning as L\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# PyTorch geometric\n",
    "import torch_geometric\n",
    "import torch_geometric.data as geom_data\n",
    "import torch_geometric.nn as geom_nn\n",
    "\n",
    "# PL callbacks\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torch import Tensor\n",
    "\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 256 if AVAIL_GPUS else 64\n",
    "# Path to the folder where the datasets are/should be downloaded\n",
    "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"data/\")\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/GNNs/\")\n",
    "\n",
    "# Setting the seed\n",
    "L.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a22a5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial7/\"\n",
    "# Files to download\n",
    "pretrained_files = [\"NodeLevelMLP.ckpt\", \"NodeLevelGNN.ckpt\", \"GraphLevelGraphConv.ckpt\"]\n",
    "\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "    if \"/\" in file_name:\n",
    "        os.makedirs(file_path.rsplit(\"/\", 1)[0], exist_ok=True)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(\"Downloading %s...\" % file_url)\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\n",
    "                \"Something went wrong. Please try to download the file from the GDrive folder,\"\n",
    "                \" or contact the author with the full output including the following error:\\n\",\n",
    "                e,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d78ccb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(c_in, c_out)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix):\n",
    "        \"\"\"Forward.\n",
    "\n",
    "        Args:\n",
    "            node_feats: Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
    "            adj_matrix: Batch of adjacency matrices of the graph. If there is an edge from i to j,\n",
    "                         adj_matrix[b,i,j]=1 else 0. Supports directed edges by non-symmetric matrices.\n",
    "                         Assumes to already have added the identity connections.\n",
    "                         Shape: [batch_size, num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "        # Num neighbours = number of incoming edges\n",
    "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
    "        node_feats = node_feats / num_neighbours\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3abf8390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features:\n",
      " tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "\n",
      "Adjacency matrix:\n",
      " tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "node_feats = torch.arange(8, dtype=torch.float32).view(1, 4, 2)\n",
    "adj_matrix = Tensor([[[1, 1, 0, 0], [1, 1, 1, 1], [0, 1, 1, 1], [0, 1, 1, 1]]])\n",
    "\n",
    "print(\"Node features:\\n\", node_feats)\n",
    "print(\"\\nAdjacency matrix:\\n\", adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31269515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "Input features tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "Output features tensor([[[1., 2.],\n",
      "         [3., 4.],\n",
      "         [4., 5.],\n",
      "         [4., 5.]]])\n"
     ]
    }
   ],
   "source": [
    "layer = GCNLayer(c_in=2, c_out=2)\n",
    "layer.projection.weight.data = Tensor([[1.0, 0.0], [0.0, 1.0]])\n",
    "layer.projection.bias.data = Tensor([0.0, 0.0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_feats = layer(node_feats, adj_matrix)\n",
    "\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "print(\"Input features\", node_feats)\n",
    "print(\"Output features\", out_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ae8be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_layer_by_name = {\"GCN\": geom_nn.GCNConv, \"GAT\": geom_nn.GATConv, \"GraphConv\": geom_nn.GraphConv}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "777baf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cora_dataset = torch_geometric.datasets.Planetoid(root=DATASET_PATH, name=\"Cora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "482df1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbfdd488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small function for printing the test scores\n",
    "def print_results(result_dict):\n",
    "    if \"train\" in result_dict:\n",
    "        print(\"Train accuracy: %4.2f%%\" % (100.0 * result_dict[\"train\"]))\n",
    "    if \"val\" in result_dict:\n",
    "        print(\"Val accuracy:   %4.2f%%\" % (100.0 * result_dict[\"val\"]))\n",
    "    print(\"Test accuracy:  %4.2f%%\" % (100.0 * result_dict[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05ac3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tu_dataset = torch_geometric.datasets.TUDataset(root=DATASET_PATH, name=\"MUTAG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "989e6570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object: Data(x=[3371, 7], edge_index=[2, 7442], edge_attr=[7442, 4], y=[188])\n",
      "Length: 188\n",
      "Average label: 0.66\n"
     ]
    }
   ],
   "source": [
    "print(\"Data object:\", tu_dataset.data)\n",
    "print(\"Length:\", len(tu_dataset))\n",
    "print(\"Average label: %4.2f\" % (tu_dataset.data.y.float().mean().item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c0fe3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "tu_dataset.shuffle()\n",
    "train_dataset = tu_dataset[:150]\n",
    "test_dataset = tu_dataset[150:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b61ab165",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_train_loader = geom_data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "graph_val_loader = geom_data.DataLoader(test_dataset, batch_size=BATCH_SIZE)  # Additional loader for a larger datasets\n",
    "graph_test_loader = geom_data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0efc7010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: DataBatch(edge_index=[2, 1512], x=[687, 7], edge_attr=[1512, 4], y=[38], batch=[687], ptr=[39])\n",
      "Labels: tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0])\n",
      "Batch indices: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(graph_test_loader))\n",
    "print(\"Batch:\", batch)\n",
    "print(\"Labels:\", batch.y[:10])\n",
    "print(\"Batch indices:\", batch.batch[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3ec56cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        c_in,\n",
    "        c_hidden,\n",
    "        c_out,\n",
    "        num_layers=2,\n",
    "        layer_name=\"GCN\",\n",
    "        dp_rate=0.1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"GNNModel.\n",
    "\n",
    "        Args:\n",
    "            c_in: Dimension of input features\n",
    "            c_hidden: Dimension of hidden features\n",
    "            c_out: Dimension of the output features. Usually number of classes in classification\n",
    "            num_layers: Number of \"hidden\" graph layers\n",
    "            layer_name: String of the graph layer to use\n",
    "            dp_rate: Dropout rate to apply throughout the network\n",
    "            kwargs: Additional arguments for the graph layer (e.g. number of heads for GAT)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        gnn_layer = gnn_layer_by_name[layer_name]\n",
    "\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for l_idx in range(num_layers - 1):\n",
    "            layers += [\n",
    "                gnn_layer(in_channels=in_channels, out_channels=out_channels, **kwargs),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dp_rate),\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "        layers += [gnn_layer(in_channels=in_channels, out_channels=c_out, **kwargs)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"Forward.\n",
    "\n",
    "        Args:\n",
    "            x: Input features per node\n",
    "            edge_index: List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            # For graph layers, we need to add the \"edge_index\" tensor as additional input\n",
    "            # All PyTorch Geometric graph layer inherit the class \"MessagePassing\", hence\n",
    "            # we can simply check the class type.\n",
    "            if isinstance(layer, geom_nn.MessagePassing):\n",
    "                x = layer(x, edge_index)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41cd5eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphGNNModel(nn.Module):\n",
    "    def __init__(self, c_in, c_hidden, c_out, dp_rate_linear=0.5, **kwargs):\n",
    "        \"\"\"GraphGNNModel.\n",
    "\n",
    "        Args:\n",
    "            c_in: Dimension of input features\n",
    "            c_hidden: Dimension of hidden features\n",
    "            c_out: Dimension of output features (usually number of classes)\n",
    "            dp_rate_linear: Dropout rate before the linear layer (usually much higher than inside the GNN)\n",
    "            kwargs: Additional arguments for the GNNModel object\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.GNN = GNNModel(c_in=c_in, c_hidden=c_hidden, c_out=c_hidden, **kwargs)  # Not our prediction output yet!\n",
    "        self.head = nn.Sequential(nn.Dropout(dp_rate_linear), nn.Linear(c_hidden, c_out))\n",
    "\n",
    "    def forward(self, x, edge_index, batch_idx):\n",
    "        \"\"\"Forward.\n",
    "\n",
    "        Args:\n",
    "            x: Input features per node\n",
    "            edge_index: List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
    "            batch_idx: Index of batch element for each node\n",
    "        \"\"\"\n",
    "        x = self.GNN(x, edge_index)\n",
    "        x = geom_nn.global_mean_pool(x, batch_idx)  # Average pooling\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "110a1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLevelGNN(L.LightningModule):\n",
    "    def __init__(self, **model_kwargs):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = GraphGNNModel(**model_kwargs)\n",
    "        self.loss_module = nn.BCEWithLogitsLoss() if self.hparams.c_out == 1 else nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        x, edge_index, batch_idx = data.x, data.edge_index, data.batch\n",
    "        x = self.model(x, edge_index, batch_idx)\n",
    "        x = x.squeeze(dim=-1)\n",
    "\n",
    "        if self.hparams.c_out == 1:\n",
    "            preds = (x > 0).float()\n",
    "            data.y = data.y.float()\n",
    "        else:\n",
    "            preds = x.argmax(dim=-1)\n",
    "        loss = self.loss_module(x, data.y)\n",
    "        acc = (preds == data.y).sum().float() / preds.shape[0]\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # High lr because of small dataset and small model\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.forward(batch, mode=\"train\")\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"val\")\n",
    "        self.log(\"val_acc\", acc)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"test\")\n",
    "        self.log(\"test_acc\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c280f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph_classifier(model_name, **model_kwargs):\n",
    "    L.seed_everything(42)\n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"GraphLevel\" + model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = L.Trainer(\n",
    "        default_root_dir=root_dir,\n",
    "        callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "#        accelerator=\"cuda\",\n",
    "#        devices=AVAIL_GPUS,\n",
    "        max_epochs=500,\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "    trainer.logger._default_hp_metric = None\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"GraphLevel%s.ckpt\" % model_name)\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = GraphLevelGNN.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        L.seed_everything(42)\n",
    "        model = GraphLevelGNN(\n",
    "            c_in=tu_dataset.num_node_features,\n",
    "            c_out=1 if tu_dataset.num_classes == 2 else tu_dataset.num_classes,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "        trainer.fit(model, graph_train_loader, graph_val_loader)\n",
    "        model = GraphLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    train_result = trainer.test(model, dataloaders=graph_train_loader, verbose=False)\n",
    "    test_result = trainer.test(model, dataloaders=graph_test_loader, verbose=False)\n",
    "    result = {\"test\": test_result[0][\"test_acc\"], \"train\": train_result[0][\"test_acc\"]}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d856f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v1.9.5. To apply the upgrade to your files permanently, run `python -m lightning.pytorch.utilities.upgrade_checkpoint --file saved_models/GNNs/GraphLevelGraphConv.ckpt`\n"
     ]
    }
   ],
   "source": [
    "model, result = train_graph_classifier(\n",
    "    model_name=\"GraphConv\", c_hidden=256, layer_name=\"GraphConv\", num_layers=3, dp_rate_linear=0.5, dp_rate=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir png\n",
    "# utl.PltErr(range(num_epochs),total_loss_hist,\n",
    "#           xscale='log',yscale='log',\n",
    "#            title='png/loss.png'\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bca0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_prediction(model, input_data, adj_matrices, target_displacements):\n",
    "#     ax = utl.PltErr(None,None,Plot=False)\n",
    "#     predicted_displacements = model(input_data, adj_matrices)\n",
    "#     for i_snapshot,u_pred in enumerate(predicted_displacements):\n",
    "        \n",
    "#         u_act  = target_displacements[i_snapshot]\n",
    "\n",
    "#         colors='black red green'.split()\n",
    "#         for idime in range(3):\n",
    "#             utl.PltErr(u_act[:,idime],u_pred[:,idime],\n",
    "#                    attrs={'fmt':'x','color':colors[idime]},\n",
    "#                   ax=ax, Plot=False,\n",
    "#                   )\n",
    "\n",
    "#         utl.PltErr( None,None,\n",
    "#                    Plot=False,\n",
    "#         ax=ax,\n",
    "#                 #xlim=(-2,2),ylim=(-2,2),\n",
    "#                    title='png/disp.png'\n",
    "#                   )\n",
    "\n",
    "\n",
    "# snapshots = [0] #range(len(transition_paths))\n",
    "# x = np.c_[[np.c_[pd.DataFrame(transition_paths[ i ])['x y z'.split()]] for i in snapshots]]\n",
    "# x = tf.convert_to_tensor(x,dtype=tf.float32)\n",
    "\n",
    "# #adj_matrices = tf.random.uniform((batch_size, num_nodes, num_nodes)) # for _ in range(2)]  # Example with 2 adjacency matrices\n",
    "# input_data = [torch.from_numpy( np.c_[pd.DataFrame(transition_paths[ i ])['x y z'.split()]] ).float() for i in snapshots]  \n",
    "# adj_matrices = compute_adjacency_matrices(torch.stack(input_data), rcut=3.0)\n",
    "# adj_matrices = tf.convert_to_tensor(adj_matrices,dtype=tf.float32)\n",
    "\n",
    "# # Example target data\n",
    "# #target = tf.random.normal((batch_size, num_nodes, output_dim))\n",
    "# target = np.c_[[np.c_[pd.DataFrame(transition_paths[ i ])['ux_fin uy_fin uz_fin'.split()]] for i in snapshots]]\n",
    "# target = tf.convert_to_tensor(target,dtype=tf.float32)\n",
    "\n",
    "\n",
    "# make_prediction(model, x, adj_matrices, target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5f72c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "269.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
