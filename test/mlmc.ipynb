{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6665ea1",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#MachineLeranedMC\" data-toc-modified-id=\"MachineLeranedMC-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MachineLeranedMC</a></span><ul class=\"toc-item\"><li><span><a href=\"#main()\" data-toc-modified-id=\"main()-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>main()</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0151ccbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from buildDescriptors.ipynb\n",
      "conf. file sections: ['flags', 'input files', 'descriptors', 'neural net', 'neural net classification', 'neural net regression', 'ml mc']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from buildDescriptors.ipynb\n",
      "conf. file sections: ['flags', 'input files', 'descriptors', 'neural net', 'neural net classification', 'neural net regression', 'ml mc']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#--- import sys. libs\n",
    "import pickle\n",
    "import import_ipynb\n",
    "import configparser\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import pdb\n",
    "import imp\n",
    "\n",
    "\n",
    "#--- sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#--- pytorch\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "#--- tensor flow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#--- user modules\n",
    "confParser = configparser.ConfigParser() #--- parse conf. file\n",
    "confParser.read('configuration.ini')\n",
    "list(map(lambda x:sys.path.append(x), confParser['input files']['lib_path'].split()))\n",
    "import LammpsPostProcess as lp\n",
    "import utility as utl\n",
    "import buildDescriptors as bd\n",
    "from neuralNetwork import DataSet #, GraphNet, GraphLevelGNN, GNNModel, GNNModel3rd #GraphLevelGNN_energy\n",
    "imp.reload(utl)\n",
    "imp.reload(lp)\n",
    "imp.reload(bd)\n",
    "\n",
    "#--- add graphnet class as attribute of main  \n",
    "import __main__\n",
    "# setattr(__main__, \"GraphNet\", GraphNet)\n",
    "# setattr(__main__, \"GraphLevelGNN\", GraphLevelGNN)\n",
    "# setattr(__main__, \"GNNModel\", GNNModel)\n",
    "# setattr(__main__, \"GNNModel3rd\", GNNModel3rd)\n",
    "setattr(__main__, \"DataSet\", DataSet)\n",
    "\n",
    "#--- lammps\n",
    "if eval(confParser['flags']['RemoteMachine']):\n",
    "    import lammps\n",
    "\n",
    "#--- increase width\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a30ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineLeranedMC( bd.ParseConfiguration,\n",
    "                        bd.EnergyBarrier,\n",
    "                      ):\n",
    "    '''\n",
    "    Performs Machine Learned Monte Carlo Swaps\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 confParser, \n",
    "                 verbose = False\n",
    "                ):\n",
    "        \n",
    "        self.verbose     =  verbose \n",
    "        self.confParser  =  confParser\n",
    "        \n",
    "        self.save_output = 'saved_output'\n",
    "        if os.path.isdir( self.save_output ):\n",
    "            !rm -r $self.save_output; mkdir $self.save_output\n",
    "        else: \n",
    "            !mkdir $self.save_output\n",
    "        \n",
    "        #--- assign units\n",
    "        temperature                  = eval(self.confParser[ 'ml mc' ][ 'temperature' ] ) #--- kelvin\n",
    "        self.rate_constant_prefactor = 1.0e+13 #s^-1\n",
    "        self.kbt                     = 8.617e-05 #eV K-1\n",
    "        self.kbt                    *= temperature\n",
    "        \n",
    "        #--- \n",
    "        self.model_energy_loaded_true  = False\n",
    "        self.model_defects_loaded_true = False\n",
    "        self.model_disps_loaded_true   = False\n",
    "        \n",
    "#     def Parse(self,fp):\n",
    "#         '''\n",
    "#         Parse lammps dump file\n",
    "#         '''\n",
    "#         t0           = time.time()\n",
    "#         self.lmpData = lp.ReadDumpFile( '%s'%(fp) ) \n",
    "#         self.lmpData.GetCords( ncount = sys.maxsize)\n",
    "#         if self.verbose:\n",
    "#             print('Parse %s ...'%fp)\n",
    "\n",
    "    \n",
    "    def Initialize( self ):\n",
    "        '''\n",
    "        Initialize variables\n",
    "        '''\n",
    "        self.lmpData0 = self.lmpData.coord_atoms_broken[0].copy()\n",
    "        \n",
    "        natom         = len( self.lmpData.coord_atoms_broken[0] )\n",
    "        ndime         = 3\n",
    "        self.disp     = np.zeros( natom * ndime ).reshape((natom,ndime))\n",
    "        self.tdisp    = np.zeros( natom * ndime ).reshape((natom,ndime))\n",
    " \n",
    "        self.mc_time  = 0.0\n",
    "                \n",
    "        self.box      = lp.Box(BoxBounds=self.lmpData.BoxBounds[0],AddMissing=np.array([0,0,0]))\n",
    "\n",
    "    def GetDescriptors( self ):\n",
    "        '''\n",
    "        Compute microstructural descriptors  \n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('Compute nodal features ...')\n",
    "        bd.EnergyBarrier.__init__( self,\n",
    "                                  None,\n",
    "                                  None,\n",
    "                                  self.lmpData,\n",
    "                                  None,\n",
    "                                   verbose    = False,#self.verbose,\n",
    "                                   nconf      = 2, #--- only two events\n",
    "                                   confParser = self.confParser,\n",
    "                                   species    = confParser['input files']['species'].split(),\n",
    "                                   r_cut      = eval(self.confParser['descriptors']['r_cut']),\n",
    "                                   dr         = eval(self.confParser['descriptors']['dr']),\n",
    "                                   dr_acsf    = eval(confParser['descriptors']['dr_acsf']),\n",
    "                                   scale      = eval(self.confParser['descriptors']['scale']),\n",
    "                                   n_max      = 8,\n",
    "                                   l_max      = 6,\n",
    "                      )\n",
    "        \n",
    "        self.perAtomData = self.lmpDataa\n",
    "        \n",
    "        #--- parinello: to be used for energy predictions\n",
    "        self.SetDescriptors(\n",
    "                      soap = True,   \n",
    "                     )\n",
    "        self.descriptors_acsf =  self.descriptors.copy()\n",
    "        #--- smooth density\n",
    "        self.SetDescriptors(\n",
    "                      gr = True,\n",
    "                     )\n",
    "\n",
    "    def BuildDataForClassifier( self, df, descriptors ):\n",
    "        gn                   = DataSet( self.confParser ) #GraphNet()\n",
    "        gn.noise_std         = float(self.confParser['neural net']['noise_std'])\n",
    "        gn.cutoff            = self.r_cut\n",
    "        natom                = df.shape[0]\n",
    "        tmp                  = {'id':np.c_[df.id].flatten(),'atom_indx':np.c_[df.index].flatten(),\n",
    "                                 'x':np.c_[df.x].flatten(),'y':np.c_[df.y].flatten(),'z':np.c_[df.z].flatten(),\n",
    "                                 'isNonCrystalline':np.c_[np.zeros(natom)].tolist(),'descriptors_acsf':descriptors.tolist()}\n",
    "        gn.descriptors       = [tmp, tmp]\n",
    "        \n",
    "        #--- build neighbor list:\n",
    "#         fp                   = 'dump_file_nl'\n",
    "#         fout                 = 'neighbor_list_xxx.xyz'\n",
    "#         if os.path.isfile( fout ):\n",
    "#             os.system('rm %s'%fout)\n",
    "#         if os.path.isfile( '%s/%s.xyz'%(self.save_output,fp) ):\n",
    "#             os.system('rm %s/%s.xyz'%(self.save_output,fp))\n",
    "            \n",
    "#         #\n",
    "#         self.Print( fp, 0, lammps_xyz = True ) #--- save as xyz file        \n",
    "#         atom_indices         = ' '.join(map(str,df.index))\n",
    "#         lib_path             = confParser['input files']['lib_path'].split()[0]\n",
    "#         #\n",
    "#         os.system('mv %s/%s.xyz .'%(self.save_output,fp))\n",
    "#         os.system('ovitos %s/OvitosCna.py %s.xyz %s 1 6 %s %s'%(lib_path,fp,fout,self.r_cut,atom_indices))\n",
    "        fout                 = self.dir_neighList\n",
    "        gn.neighlists        = [ fout, fout ] #--- to be used for computing the adj. matrix\n",
    "\n",
    "        #--- func. call\n",
    "        gn.DataBuilderForClassifier()\n",
    "        \n",
    "        #--- rm xyz files!!!\n",
    "#        os.system( 'rm %s.xyz %s'%(fp,fout))\n",
    "        \n",
    "        return gn.train_dataloaders.dataset\n",
    "        \n",
    "    def GetDefects( self, fp):\n",
    "        '''\n",
    "        Classify Defects\n",
    "        '''\n",
    "        if self.verbose:\n",
    "            print('identify defects ...')\n",
    "        \n",
    "        #--- load ml model\n",
    "        if not self.model_defects_loaded_true:\n",
    "#            version_xxx        = os.listdir(fp)[0]\n",
    "#            model              = os.listdir('%s/%s/checkpoints'%(fp,version_xxx))[0]\n",
    "            self.model_defects = tf.keras.models.load_model(fp) #GraphLevelGNN.load_from_checkpoint( '%s/%s/checkpoints/%s'%(fp,version_xxx,model ) ) \n",
    "            self.model_defects_loaded_true = True\n",
    "            \n",
    "        #--- build data for classifier: \n",
    "        self.predict_classes     = self.BuildDataForClassifier(self.lmpData.coord_atoms_broken[ 0 ], #--- nodal xyz\n",
    "                                                               self.descriptors_acsf, #--- nodal features\n",
    "                                                              )\n",
    "        \n",
    "#        self.predict_classes.y   = self.model_defects.predict( self.predict_classes ).int()\n",
    "        self.predict_classes.y   = ( self.model_defects.predict(self.predict_classes.x.numpy()) > 0.5 ).astype(int)\n",
    "#        assert torch.any(self.predict_classes.y), 'Detect no defect!'\n",
    "        assert np.any(self.predict_classes.y), 'Detect no defect!'\n",
    "    \n",
    "#     def DiscretizeTransitionPath( self ):\n",
    "#          #--- hard-coded values\n",
    "#         umax = float( self.confParser[ 'neural net regression' ][ 'umax' ] ) \n",
    "#         du   = float( self.confParser[ 'neural net regression' ][ 'du' ] ) \n",
    "#         xlin = np.arange(-umax,umax+du,du)\n",
    "#         ylin = np.arange(-umax,umax+du,du)\n",
    "#         zlin = np.arange(-umax,umax+du,du)\n",
    "#         self.nbinx = len(xlin)-1\n",
    "#         self.nbiny = len(ylin)-1\n",
    "#         self.nbinz = len(zlin)-1\n",
    "#         self.bins = (xlin, ylin, zlin)\n",
    "#         self.ux, self.uy, self.uz = np.meshgrid( self.bins[1][:-1], self.bins[0][:-1], self.bins[2][:-1] )\n",
    "\n",
    "        \n",
    "#     def GetDispsFromBinaryMaps( self, atomIndex, binaryMap ):\n",
    "#         binaryMapReshaped = binaryMap.reshape((self.nbinx, self.nbiny, self.nbinz ))\n",
    "#         filtr = binaryMapReshaped == 1\n",
    "#         disps = np.c_[self.uy[filtr],self.ux[filtr],self.uz[filtr]]\n",
    "#         nrows = disps.shape[ 0 ]\n",
    "#         assert nrows > 0, 'no diffusion path!'\n",
    "#         return np.c_[np.ones(nrows)*atomIndex,disps]\n",
    "    \n",
    "#     def GetDisp( self, fp, scaler ):\n",
    "#         '''\n",
    "#         Predict Displacements\n",
    "#         '''\n",
    "        \n",
    "#         #--- load ml model\n",
    "#         model               = keras.models.load_model(fp)\n",
    "        \n",
    "#         #---------------\n",
    "#         #--- zscore X\n",
    "#         #---------------        \n",
    "#         loaded_scaler       = pickle.load( open( scaler, 'rb' ) )\n",
    "#         filtr               = self.predict_classes > 0\n",
    "#         X                   = loaded_scaler.transform( np.c_[self.descriptors[ filtr ] ] )\n",
    "\n",
    "\n",
    "#         #--- reshape X\n",
    "#         shape               =  (self.shape[0],self.shape[1],self.shape[2],1) #--- rows, cols, thickness, channels\n",
    "#         n                   =  X.shape[ 0 ]\n",
    "#         X_reshaped          =  X.reshape((n,shape[0],shape[1],shape[2],1))\n",
    "        \n",
    "#         prediction          =  model.predict( X_reshaped )\n",
    "#         threshold           = 0.5 #--- hard-coded threshold\n",
    "#         binary_predictions  = (prediction > threshold).astype(int)\n",
    "\n",
    "#         #---\n",
    "#         atomIndices         = self.lmpDataa[ filtr ].index\n",
    "#         self.predict_disp     = np.concatenate([list(map(lambda x: self.GetDispsFromBinaryMaps( x[0],x[1] ) , zip(atomIndices,binary_predictions) ))])\n",
    "#         self.predict_disp     = self.predict_disp.reshape((self.predict_disp.shape[0],self.predict_disp.shape[2]))\n",
    "        \n",
    "    def BuildDataForRegressor( self, df, binary_nonCrystalAtoms ):\n",
    "        gn                   = DataSet(self.confParser) #GraphNet()\n",
    "        gn.noise_std         = float(self.confParser['neural net']['noise_std'])\n",
    "#        gn.c_out             = eval(self.confParser['gnn']['c_out'])\n",
    "        gn.cutoff            = self.r_cut\n",
    "        r_cut                = float( self.confParser['descriptors']['cutoff_kmc'] )\n",
    "        gn.verbose           = False\n",
    "        gn.transition_paths  = []\n",
    "        gn.energy            = []\n",
    "        natom                = df.shape[0]\n",
    "        nonCrystAtomIndices  = np.arange(natom)[ binary_nonCrystalAtoms ]\n",
    "        assert len( nonCrystAtomIndices ) > 0, 'no defect detected!'\n",
    "\n",
    "        #--- build neigh. list for non-crystalline atoms\n",
    "        #--- cutoff = cutoff_kmc used to define clusters\n",
    "        #--- save cords as a dump file\n",
    "        fp                   = 'dump_file_nl'\n",
    "        fout                 = 'neighbor_list.xyz'\n",
    "        if os.path.isfile( '%s/%s.xyz'%(self.save_output,fp) ):\n",
    "            os.system('rm %s/%s.xyz'%(self.save_output,fp))\n",
    "        if os.path.isfile( fout ):\n",
    "            get_ipython().system('rm $fout')\n",
    "        self.Print( fp, 0, lammps_xyz = True ) #--- save as xyz file        \n",
    "        os.system('mv %s/%s.xyz .'%(self.save_output,fp))\n",
    "        #--- update df\n",
    "        df[ 'atom_index' ]   = df.index\n",
    "        #\n",
    "        atom_indices         = ' '.join(map(str,nonCrystAtomIndices))\n",
    "        lib_path             = confParser['input files']['lib_path'].split()[0]\n",
    "        #\n",
    "        os.system('ovitos %s/OvitosCna.py %s.xyz %s 1 6 %s %s'%(lib_path,fp,fout,r_cut,atom_indices))\n",
    "        nl                   = lp.ReadDumpFile(fout)\n",
    "        nl.GetCords()\n",
    "        nl                   = nl.coord_atoms_broken[0]\n",
    "        #\n",
    "        os.system( 'rm %s.xyz %s'%(fp,fout))\n",
    "\n",
    "        #--- build clusters for non-crystalline atoms\n",
    "        groups               = nl.groupby(by='id').groups\n",
    "        for atomIndx in nonCrystAtomIndices:\n",
    "            atom_id          = df['id'].iloc[ atomIndx ]\n",
    "            neighbor_indices_nl = groups[ atom_id ]\n",
    "            xyz              = np.c_[ nl.iloc[ neighbor_indices_nl ]['DX DY DZ'.split()] ]\n",
    "            xyz              = np.concatenate([xyz,[[0.0,0.0,0.0]]],axis=0) #--- include center\n",
    "            #--- descriptors\n",
    "            neighbor_ids     = list( nl.iloc[ neighbor_indices_nl ].J.astype(int) )\n",
    "            neighbor_ids    += [ atom_id ]\n",
    "            neighbor_indices_df = utl.FilterDataFrame(df, key='id', val=neighbor_ids).atom_index\n",
    "            descriptors      = self.descriptors[ neighbor_indices_df ]\n",
    "            descriptors_acsf = self.descriptors_acsf[ neighbor_indices_df ]\n",
    "            center_atom_index= np.zeros( len( neighbor_ids ) )\n",
    "            center_atom_index[ -1 ] = 1\n",
    "            n                = len( neighbor_ids )\n",
    "            diffusion_paths  = np.random.random(size=(3*n)).reshape((n,3))\n",
    "            energy_barrier   = np.random.random(size=(1,))\n",
    "            #\n",
    "            sdict = {'x':xyz[:,0],'y':xyz[:,1],'z':xyz[:,2],\n",
    "                     'center_atom_index':center_atom_index,\n",
    "                     'descriptors':descriptors,\n",
    "                     'descriptors_acsf':descriptors_acsf,\n",
    "                     'diffusion_paths':diffusion_paths,\n",
    "                     'multi_hot_encoded_diffusion_paths':diffusion_paths,\n",
    "                     'energy_barrier':energy_barrier,\n",
    "                     'atom_indx':neighbor_indices_df\n",
    "                    }\n",
    "            gn.transition_paths.append( sdict )\n",
    "        #\n",
    "        gn.Process() #DataBuilder()        \n",
    "        loader             = DataLoader(gn.large_graph, batch_size=len(gn.large_graph), shuffle=False)\n",
    "        #\n",
    "#        gn.DataBuilderForEnergy()\n",
    "#        large_graph        = torch_geometric.data.Batch.from_data_list(gn.graphs)\n",
    "#        loader_energy      = DataLoader(large_graph, batch_size=len(gn.graphs), shuffle=False)\n",
    "\n",
    "        return loader.dataset, None #loader_energy.dataset\n",
    "\n",
    "    \n",
    "    def GetDisp2nd( self, fp ):\n",
    "        '''\n",
    "        Predict Displacements\n",
    "        '''\n",
    "        if self.verbose:\n",
    "            print('predict transition paths ...')\n",
    "        \n",
    "        #--- load ml model\n",
    "        if not self.model_disps_loaded_true:\n",
    "            #self.model_disps   = torch.load( fp, map_location=torch.device('cpu') )\n",
    "            self.model_disps    = tf.keras.models.load_model( fp )\n",
    "\n",
    "            self.model_disps_loaded_true = True\n",
    "        \n",
    "        self.spec_events, self.predict_energy   = self.BuildDataForRegressor(self.lmpData.coord_atoms_broken[ 0 ], #--- nodal coords\n",
    "#                                                        self.predict_classes.y.numpy().astype(bool) #--- binary list: non-crystalline atoms\n",
    "                                                        self.predict_classes.y.flatten().astype(bool) #--- binary list: non-crystalline atoms\n",
    "                                                       )\n",
    "#        self.spec_events.y = self.model_disps( self.spec_events.x, self.spec_events.edge_index )\n",
    "\n",
    "        self.spec_events.y  = self.model_disps.predict(self.spec_events.x.numpy())\n",
    "\n",
    "#     def BuildDataForEnergy( self, df, binary_nonCrystalAtoms ):\n",
    "#         gn                   = GraphNet()\n",
    "#         gn.noise_std         = float(self.confParser['gnn energy']['noise_std'])\n",
    "#         gn.c_out             = eval(self.confParser['gnn energy']['c_out'])\n",
    "#         gn.cutoff            = self.r_cut\n",
    "#         r_cut                = float( self.confParser['descriptors']['cutoff_kmc'] )\n",
    "#         gn.verbose           = False\n",
    "#         gn.transition_paths  = []\n",
    "#         natom                = df.shape[0]\n",
    "#         nonCrystAtomIndices  = np.arange(natom)[ binary_nonCrystalAtoms ]\n",
    "        \n",
    "#         #--- build neigh. list\n",
    "#         #--- save cords as a dump file\n",
    "#         fp                   = 'dump_file_nl'\n",
    "#         fout                 = 'neighbor_list.xyz'\n",
    "#         if os.path.isfile( '%s/%s.xyz'%(self.save_output,fp) ):\n",
    "#             os.system('rm %s/%s.xyz'%(self.save_output,fp))\n",
    "#         if os.path.isfile( fout ):\n",
    "#             !rm $fout\n",
    "#         self.Print( fp, 0, lammps_xyz = True ) #--- save as xyz file        \n",
    "#         os.system('mv %s/%s.xyz .'%(self.save_output,fp))\n",
    "#         #--- update df\n",
    "#         df[ 'atom_index' ]   = df.index\n",
    "#         #\n",
    "#         atom_indices         = ' '.join(map(str,nonCrystAtomIndices))\n",
    "#         lib_path             = confParser['input files']['lib_path'].split()[0]\n",
    "#         #\n",
    "#         os.system('ovitos %s/OvitosCna.py %s.xyz %s 1 6 %s %s'%(lib_path,fp,fout,r_cut,atom_indices))\n",
    "#         nl                   = lp.ReadDumpFile(fout)\n",
    "#         nl.GetCords()\n",
    "#         nl                   = nl.coord_atoms_broken[0]\n",
    "#         #\n",
    "#         os.system( 'rm %s.xyz %s'%(fp,fout))\n",
    "\n",
    "#         #--- build clusters for non-crystalline atoms\n",
    "#         groups               = nl.groupby(by='id').groups\n",
    "#         for atomIndx in nonCrystAtomIndices:\n",
    "#             atom_id          = df['id'].iloc[ atomIndx ]\n",
    "#             neighbor_indices_nl = groups[ atom_id ]\n",
    "#             xyz              = np.c_[ nl.iloc[ neighbor_indices_nl ]['DX DY DZ'.split()] ]\n",
    "#             xyz              = np.concatenate([xyz,[[0.0,0.0,0.0]]],axis=0) #--- include center\n",
    "#             #--- descriptors\n",
    "#             neighbor_ids     = list( nl.iloc[ neighbor_indices_nl ].J.astype(int) )\n",
    "#             neighbor_ids    += [ atom_id ]\n",
    "#             neighbor_indices_df = utl.FilterDataFrame(df, key='id', val=neighbor_ids).atom_index\n",
    "#             descriptors      = self.descriptors_acsf[ neighbor_indices_df ]\n",
    "#             center_atom_index= np.zeros( len( neighbor_ids ) )\n",
    "#             center_atom_index[ -1 ] = 1\n",
    "#             n                = len( neighbor_ids )\n",
    "#             energy_barrier   = np.random.random(size=(1,))\n",
    "#             #\n",
    "#             sdict = {'x':xyz[:,0],'y':xyz[:,1],'z':xyz[:,2],\n",
    "#                      'center_atom_index':center_atom_index,\n",
    "#                      'descriptors_acsf':descriptors,\n",
    "#                      'energy_barrier':energy_barrier,\n",
    "#                      'atom_indx':neighbor_indices_df\n",
    "#                     }\n",
    "#             gn.transition_paths.append( sdict )\n",
    "#         #\n",
    "#         gn.DataBuilderForEnergy()\n",
    "#         #        \n",
    "#         large_graph = torch_geometric.data.Batch.from_data_list(gn.graphs)\n",
    "#         loader      = DataLoader(large_graph, batch_size=len(gn.graphs), shuffle=False)\n",
    "\n",
    "#         return loader.dataset\n",
    "        \n",
    "    def GetBarrier( self, fp):\n",
    "        '''\n",
    "        Predict energy barriers\n",
    "        '''\n",
    "        if self.verbose:\n",
    "            print('predict energetics ...')\n",
    "        #--- load ml model: load every step !!!\n",
    "        if not self.model_energy_loaded_true:\n",
    "            self.model_energy     = torch.load( fp, map_location=torch.device('cpu') ) \n",
    "            self.model_energy_loaded_true = True\n",
    "\n",
    "#         self.predict_energy   = self.BuildDataForEnergy(self.lmpData.coord_atoms_broken[ 0 ], #--- nodal coords\n",
    "#                                                         self.predict_classes.y.numpy().astype(bool) #--- binary list: non-crystalline atoms\n",
    "#                                                        )\n",
    "        self.predict_energy.y = self.model_energy( self.predict_energy.x, \n",
    "                                                  self.predict_energy.edge_index, \n",
    "                                                  self.predict_energy.batch )\n",
    "\n",
    "        assert torch.all( self.predict_energy.y > 0.0 ), 'predicted barrier <= 0.0!'\n",
    "        \n",
    "#         #--- setup input\n",
    "#         atomIndices    = self.predict_disp[ :, 0 ].astype( int )\n",
    "\n",
    "#         pixel_maps_input = np.c_[self.descriptors[ atomIndices ] ]\n",
    "#         vectors_input    = self.predict_disp[ :, 1: ]\n",
    "#         X                = np.c_[pixel_maps_input,vectors_input]\n",
    "\n",
    "        \n",
    "#         #---------------\n",
    "#         #--- zscore X\n",
    "#         #---------------        \n",
    "#         loaded_scaler       = pickle.load( open( scaler, 'rb' ) )\n",
    "#         X                   = loaded_scaler.transform( X )\n",
    "\n",
    "\n",
    "#         #--- reshape X\n",
    "#         shape               =  (self.shape[0],self.shape[1],self.shape[2],1) #--- rows, cols, thickness, channels: pixel map\n",
    "#         shape_vector_input  = vectors_input.shape[ 1 ]\n",
    "\n",
    "#         mdime               = X.shape[ 1 ]\n",
    "#         X_pixels            = X[:,0:mdime-shape_vector_input]\n",
    "#         X_vector            = X[:,mdime-shape_vector_input:mdime]\n",
    "#         n                   =  X.shape[ 0 ]\n",
    "#         X_pixels            =  X_pixels.reshape((n,shape[0],shape[1],shape[2],1))\n",
    "\n",
    "        \n",
    "#         self.predict_energy =  model.predict( [X_pixels,X_vector] )\n",
    "\n",
    "#     def ReturnCenterAtom( self, disp_array ):\n",
    "#         disp_sq   = disp_array**2\n",
    "#         atom_indx = (disp_sq[:,0] + disp_sq[:,1] + disp_sq[:,2]).argmax()\n",
    "#         return np.concatenate([[atom_indx], disp_array[ atom_indx ]])\n",
    "\n",
    "    def BuildCatalog( self): #, filtr ):\n",
    "        pdb.set_trace()\n",
    "        center_atoms_rows   = self.spec_events.ptr[ 1 : ] - 1\n",
    "        disp_max_per_mode   = self.spec_events.y[ center_atoms_rows ] \n",
    "        atomIndices         = self.spec_events.atom_indx[ center_atoms_rows ].numpy().flatten()\n",
    "        atomIDs             = self.lmpData.coord_atoms_broken[0].iloc[atomIndices].id\n",
    "        atomTypes           = self.lmpData.coord_atoms_broken[0].iloc[atomIndices].type\n",
    "\n",
    "        self.nmode          = len( self.spec_events.ptr ) - 1\n",
    "        rates               = self.rate_constant_prefactor * np.exp(-self.predict_energy.y.detach().numpy()/self.kbt)\n",
    "        \n",
    "        self.catalog        = pd.DataFrame( np.c_[atomIDs, atomIndices,\\\n",
    "                                                  self.predict_energy.y.detach().numpy(),\\\n",
    "                                                  rates,\\\n",
    "                                                  disp_max_per_mode.detach().numpy() ],\n",
    "                                     columns = 'AtomId AtomIndex barrier true_rate ux uy uz'.split()\n",
    "                                   )\n",
    "    def MCsampling( self ):\n",
    "        ktot_inv         = 1.0 / self.catalog.true_rate.sum()\n",
    "        normalized_rates = np.cumsum( self.catalog.true_rate ) * ktot_inv\n",
    "        n                = len( normalized_rates )\n",
    "        x                = np.random.random()\n",
    "        self.event_indx  = np.arange( n )[ x < normalized_rates ][ 0 ]\n",
    "        \n",
    "        #--- advance time\n",
    "        inv_rate         = ktot_inv #1.0 / self.catalog.iloc[ self.event_indx ].true_rate\n",
    "        self.mc_time    += np.random.exponential( scale = inv_rate )\n",
    "\n",
    "    def UpdateDisp( self ):\n",
    "        row_ini                  = self.spec_events.ptr[ self.event_indx ]\n",
    "        row_fin                  = self.spec_events.ptr[ self.event_indx + 1 ]\n",
    "        udisp                    = self.spec_events.y[ row_ini : row_fin ].detach().numpy()\n",
    "        atom_indices             = self.spec_events.atom_indx[ row_ini : row_fin ].numpy().flatten()\n",
    "        self.disp[ :, : ]        = 0.0\n",
    "        self.disp[atom_indices]  = udisp \n",
    "        \n",
    "        \n",
    "        \n",
    "    def UpdateCords( self ):\n",
    "        \n",
    "        coords  = np.c_[ self.lmpData.coord_atoms_broken[ 0 ]['x y z'.split()] ]\n",
    "\n",
    "        self.lmpData0['x y z'.split()] = coords\n",
    "        \n",
    "        coords += self.disp        \n",
    "        \n",
    "        self.lmpData.coord_atoms_broken[0]['x y z'.split()] = coords\n",
    "        \n",
    "        #--- wrap coords\n",
    "        df      = self.lmpData.coord_atoms_broken[ 0 ]\n",
    "        atoms   = lp.Atoms(**df['id type x y z'.split() ].to_dict( orient = 'series' ) )\n",
    "        #  \n",
    "        wr      = lp.Wrap(atoms, self.box)\n",
    "        wr.WrapCoord()\n",
    "        #\n",
    "        self.lmpData.coord_atoms_broken[0] = pd.DataFrame(atoms.__dict__)\n",
    "        \n",
    "\n",
    "    def Print( self, fout, itime, **kwargs ):\n",
    "        '''\n",
    "        save configurations in lammps/kart formats\n",
    "        '''\n",
    "        #-----------------------\n",
    "        #--- lammps format\n",
    "        #-----------------------\n",
    "        if 'lammps_xyz' in kwargs and kwargs['lammps_xyz']:\n",
    "            df    = self.lmpData0\n",
    "            atomm = lp.Atoms(**df.to_dict(orient='series'),ux=self.disp[:,0],uy=self.disp[:,1],uz=self.disp[:,2])\n",
    "            #\n",
    "            wd    = lp.WriteDumpFile(atomm, self.box )\n",
    "            with open('%s/%s.xyz'%(self.save_output,fout),'a') as fp:\n",
    "                wd.Write(fp,itime = itime,\n",
    "                         attrs=['id', 'type', 'x', 'y', 'z','ux','uy','uz'],\n",
    "                         fmt='%i %i %4.3e %4.3e %4.3e %4.3e %4.3e %4.3e')\n",
    "            \n",
    "        #-----------------------\n",
    "        #--- k-art format\n",
    "        #-----------------------\n",
    "        elements = self.confParser['ml mc']['species'].split()\n",
    "        if 'kart' in kwargs and kwargs['kart']:\n",
    "            AtomIndices = kwargs[ 'AtomIndices' ] if 'AtomIndices' in kwargs else df.index\n",
    "            with open('%s/%s'%(self.save_output,fout),'a') as fp:\n",
    "                #--- half step\n",
    "                if itime > 0:\n",
    "                    fp.write('%s\\n'%df.iloc[AtomIndices].shape[0])\n",
    "                    fp.write(\"Lattice=\\\" %s \\\" Time=%e  Step=%s  Energy=0.0  Barrier=%e\\n\"\\\n",
    "                             %(' '.join(map(str,self.box.CellVector.flatten())),self.mc_time,itime-0.5,self.catalog.iloc[ self.event_indx ].barrier)\n",
    "                            )\n",
    "                    for item in np.c_[ df.iloc[AtomIndices] ]:\n",
    "                        element = elements[int(item[1])-1]\n",
    "                        fp.write('%s %e %e %e %d\\n'%(element,item[2],item[3],item[4],item[0]))\n",
    "                #\n",
    "                #--- full step\n",
    "                fp.write('%s\\n'%df.iloc[AtomIndices].shape[0])\n",
    "                fp.write(\"Lattice=\\\" %s \\\" Time=%e  Step=%s  Energy=0.0  Barrier=%e\\n\"\\\n",
    "                         %(' '.join(map(str,self.box.CellVector.flatten())),self.mc_time,itime,0.0)\n",
    "                        )\n",
    "                for item in np.c_[ df.iloc[AtomIndices] ]:\n",
    "                    fp.write('Ni %e %e %e %d\\n'%(item[2],item[3],item[4],item[0]))\n",
    "\n",
    "    def PrintMSD( self, fout, itime ):\n",
    "        msd = self.tdisp.var(axis=0)\n",
    "        with open('%s/%s'%(self.save_output,fout),'a') as fp:\n",
    "            if itime == 0:\n",
    "                 fp.write('#  Elapsed Time    Sqr DisplX.      Sqr DisplY.     Sqr DisplZ.  Sqr Displ\\n')\n",
    "#                fp.write('#  ************    ***Total***       Atom Ni        Atom NiV  ********\\n')\n",
    "            fp.write('%e %e %e %e %e\\n'%(self.mc_time,msd[0],msd[1],msd[2],msd[0]+msd[1]+msd[2]))\n",
    "\n",
    "                \n",
    "    def PrintCatalog( self, fout, itime ):\n",
    "        df    = self.lmpData.coord_atoms_broken[ 0 ]\n",
    "        natom = df.shape[ 0 ]\n",
    "        \n",
    "        rwj = utl.ReadWriteJson()\n",
    "        with open('%s/%s'%(self.save_output,fout),'a') as fp:\n",
    "            rwj.Write([ self.catalog.to_dict( orient = 'list' ) ], fp,\n",
    "                      mc_time = [ self.mc_time ],\n",
    "                      mc_step = [ itime ],\n",
    "                     )\n",
    "            \n",
    "        #--- save ovito\n",
    "        with open('%s/%s'%(self.save_output,'catalog_ovito.xyz'),'a') as fp:\n",
    "            for imode in range( self.nmode ):\n",
    "                row_ini = self.spec_events.ptr[imode]\n",
    "                row_fin = self.spec_events.ptr[imode+1]\n",
    "                udisp   = self.spec_events.y[row_ini:row_fin].detach().numpy()\n",
    "                atom_indices = self.spec_events.atom_indx[row_ini:row_fin].numpy().flatten()\n",
    "                disps = np.zeros(3*natom).reshape((natom,3))\n",
    "                disps[ atom_indices ] = udisp \n",
    "                atomm   = lp.Atoms(**df.to_dict(orient='series'),DisplacementX=disps[:,0],DisplacementY=disps[:,1],DisplacementZ=disps[:,2])\n",
    "                wd      = lp.WriteDumpFile(atomm, self.box )\n",
    "                wd.Write(fp,itime,\n",
    "                          attrs=['id', 'type', 'x', 'y', 'z','DisplacementX','DisplacementY','DisplacementZ'],\n",
    "                          fmt='%i %i %4.3e %4.3e %4.3e %4.3e %4.3e %4.3e')\n",
    "            \n",
    "            \n",
    "        \n",
    "#     @staticmethod    \n",
    "#     def AddGaussianNoise(X,scale = 0.1):\n",
    "\n",
    "#         epsilon_x = np.random.normal(scale=scale,size=X.size).reshape(X.shape)\n",
    "#         X += epsilon_x\n",
    "\n",
    "#     @staticmethod\n",
    "#     def Zscore( X ):\n",
    "#         scaler = StandardScaler()\n",
    "#         scaler.fit(X)\n",
    "#         return scaler.transform( X )\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def compute_adjacency_matrices(input_data, rcut):\n",
    "#         adj_matrices = []\n",
    "\n",
    "#         for positions in input_data:\n",
    "#             num_atoms = positions.shape[0]\n",
    "#             adj_matrix = torch.zeros((num_atoms, num_atoms), dtype=torch.float)\n",
    "\n",
    "#             for i in range(num_atoms):\n",
    "#                 adj_matrix[i, i] = 1\n",
    "#                 for j in range(i + 1, num_atoms):\n",
    "#                     distance = torch.norm(positions[i] - positions[j])\n",
    "#                     if distance <= rcut:\n",
    "#                         adj_matrix[i, j] = 1\n",
    "#                         adj_matrix[j, i] = 1\n",
    "#                 assert adj_matrix[i,:].sum() > 0, 'dangling node : increase the cutoff!'\n",
    "#             adj_matrices.append(adj_matrix)\n",
    "\n",
    "#         #--- assert no \n",
    "#         return adj_matrices\n",
    "\n",
    "#     def BuildNeighborList( self, indx, atom_indices,cutoff ):\n",
    "#         atom_indices = ' '.join(map(str,atom_indices))\n",
    "\n",
    "#         fp = self.dumpFiles[ indx ] #'%s/lammps_data.dat'%confParser['input files']['input_path']\n",
    "#         fout = 'neighbor_list.xyz'\n",
    "#         os.system('rm %s'%fout)\n",
    "#         lib_path = confParser['input files']['lib_path'].split()[0]\n",
    "#         #--- neighbor list\n",
    "#         os.system('ovitos %s/OvitosCna.py %s %s 1 6 %s %s'%(lib_path,fp,fout,cutoff,atom_indices))\n",
    "#         nl = lp.ReadDumpFile(fout)\n",
    "#         nl.GetCords()\n",
    "#         return nl.coord_atoms_broken[0]\n",
    "\n",
    "#     def GetIndxById( self, atom_ids, indx ):\n",
    "#         df              = pd.DataFrame(self.transition_paths[ indx ])\n",
    "#         df['indices']   = range(df.shape[0])\n",
    "#         atom_indices    = utl.FilterDataFrame(df,key='id',val=atom_ids)['indices']\n",
    "#         return np.c_[atom_indices].flatten()\n",
    "            \n",
    "#     def compute_adjacency_matrices2nd(self,input_data, rcut):\n",
    "#         adj_matrices       = []\n",
    "#         edge_attrs         = []\n",
    "#         for indx, positions in enumerate( input_data ):\n",
    "#             num_atoms      = positions.shape[0]\n",
    "#             adj_matrix     = torch.zeros((num_atoms, num_atoms), dtype=torch.float)\n",
    "#             nl             = self.BuildNeighborList(indx,range(len(positions)),rcut) #--- neighbor list\n",
    "#             #--- add \"index\" columns\n",
    "#             nl['index_i']=self.GetIndxById( np.c_[nl.id].flatten(), indx )\n",
    "#             nl['index_j']=self.GetIndxById( np.c_[nl.J].flatten(), indx )\n",
    "#             groups         = nl.groupby(by='id').groups\n",
    "#             atom_i_ids     = list(groups.keys())\n",
    "#             atom_i_indices = self.GetIndxById( atom_i_ids, indx )\n",
    "#             for i, atom_id in zip(atom_i_indices,atom_i_ids):\n",
    "# #                adj_matrix[i, i] = 1\n",
    "#                 atom_j_ids       = nl.iloc[groups[ atom_id ]].J\n",
    "#                 atom_j_indices   = self.GetIndxById( atom_j_ids, indx )\n",
    "#                 for j, jatom_id in zip(atom_j_indices, atom_j_ids ): #[ atom_j_indices > i ]:\n",
    "#                     if j < i :\n",
    "#                         continue\n",
    "#                     filtr = np.all([nl.id==atom_id,nl.J==jatom_id],axis=0)\n",
    "#                     edge_features = nl.iloc[ filtr ][ ''.split() ]\n",
    "#                     adj_matrix[i, j] = 1\n",
    "#                     adj_matrix[j, i] = 1\n",
    "#                 assert adj_matrix[i,:].sum() > 0, 'dangling node : increase the cutoff!'\n",
    "# #            pdb.set_trace()\n",
    "#             #--- edge attributes\n",
    "#             keys = 'DX  DY  DZ  PBC_SHIFT_X PBC_SHIFT_Y PBC_SHIFT_Z'.split()\n",
    "#             indices = adj_matrix.nonzero().numpy()\n",
    "#             nl_reindexed = nl.set_index(['index_i','index_j'],drop=False)\n",
    "#             edge_attr = list(map(lambda x: list(nl_reindexed[keys].loc[tuple(x)]),indices))\n",
    "\n",
    "# #            pdb.set_trace()\n",
    "#             edge_attrs.append( torch.Tensor( edge_attr ) )\n",
    "#             adj_matrices.append( adj_matrix )\n",
    "\n",
    "#         #--- assert no \n",
    "#         return adj_matrices, edge_attrs\n",
    "    \n",
    "    def PrintDensityMap(self, atomIndx, fout):\n",
    "        with open(fout,'w') as fp:\n",
    "#                     disp           = np.c_[self.perAtomData.iloc[atomIndx]['ux uy uz'.split()]].flatten()\n",
    "                    df             = pd.DataFrame(np.c_[self.positions.T,self.descriptors[atomIndx]],\n",
    "                                                  columns='x y z mass'.split())\n",
    "                    utl.PrintOvito(df, fp, ' ', attr_list='x y z mass'.split())\n",
    "        \n",
    "    def LammpsInit( self, lmp_script ):\n",
    "        '''\n",
    "        run minimization in lammps\n",
    "        '''\n",
    "        if self.verbose:\n",
    "            print('create initial data in lammps ...')\n",
    "            \n",
    "        #--- run  lammps\n",
    "        argss            = ' '.join(lmp_script.split()[1:])\n",
    "        MEAM_library_DIR = '/mnt/home/kkarimi/Project/git/lammps-27May2021/src/../potentials'\n",
    "        INC              = '/mnt/home/kkarimi/Project/git/crystalDefect/simulations/lmpScripts'\n",
    "        args             = \"-screen none -var OUT_PATH . -var PathEam %s -var INC %s -var buff 0.0\\\n",
    "                            -var nevery 1000  -var T 2000.0 -var time 1.0\\\n",
    "                            -var DumpFile dumpMin.xyz -var WriteData lammps_data.dat %s \"%(MEAM_library_DIR,INC,argss)+\\\n",
    "                           \"-var rnd %s -var rnd1 %s -var rnd2 %s -var rnd3 %s\"%tuple(np.random.randint(1001,9999,size=4))\n",
    "\n",
    "        lmp              = lammps.lammps( cmdargs = args.split() )\n",
    "        lmp.file( \"%s/%s\"%(INC,lmp_script.split()[0]) )\n",
    "        \n",
    "        #--- update coords\n",
    "        self.lmpData     = lp.ReadDumpFile('lammps_data.dat')\n",
    "        self.lmpData.ReadData()\n",
    "    \n",
    "    def Lammps( self ):\n",
    "        '''\n",
    "        run minimization in lammps\n",
    "        \n",
    "        version built at: /mnt/home/kkarimi/Project/git/lammps-2Aug2023/src\n",
    "        \n",
    "        follow instructions on 'https://docs.lammps.org/Python_head.html'\n",
    "        '''\n",
    "        if self.verbose:\n",
    "            print('minimization in lammps ...')\n",
    "\n",
    "        #--- lammps data file\n",
    "        df               = self.lmpData.coord_atoms_broken[ 0 ]\n",
    "        atom             = lp.Atoms(**df['id type x y z'.split() ].to_dict( orient = 'series' ) )\n",
    "        mass             = dict(zip(set(df.type),np.ones(len(set(df.type)))))\n",
    "        wd               = lp.WriteDataFile(atom, self.box, mass) #--- modify!!\n",
    "        fout             = 'lammps.dat'\n",
    "        wd.Write( fout )\n",
    "\n",
    "            \n",
    "        #--- run lammps\n",
    "        MEAM_library_DIR = '/mnt/home/kkarimi/Project/git/lammps-27May2021/src/../potentials'\n",
    "        INC              = '/mnt/home/kkarimi/Project/git/crystalDefect/simulations/lmpScripts'\n",
    "        args             = \"-screen none -var OUT_PATH . -var PathEam %s -var INC %s -var buff 0.0 \\\n",
    "                            -var nevery 1000 -var ParseData 1 -var DataFile %s -var ntype 3 -var cutoff 3.54\\\n",
    "                            -var DumpFile dumpMin.xyz -var WriteData data_minimized.dat\"%(MEAM_library_DIR,INC,fout)\n",
    "        lmp              = lammps.lammps( cmdargs = args.split() )\n",
    "        lmp.file( \"%s/in.minimization_constant_volume\"%INC )\n",
    "        \n",
    "        #--- update coords\n",
    "#        rd               = lp.ReadDumpFile('data_minimized.dat')\n",
    "        rd               = lp.ReadDumpFile('dumpMin.xyz')\n",
    "#        rd.ReadData()\n",
    "        rd.GetCords()\n",
    "    \n",
    "        itime            = list(rd.coord_atoms_broken.keys())[0]\n",
    "        cords            = np.c_[rd.coord_atoms_broken[itime]['x y z'.split()]]\n",
    "        disp_minimized   = np.c_[rd.coord_atoms_broken[itime]['c_dsp[1]  c_dsp[2]  c_dsp[3]'.split()]]\n",
    "        \n",
    "        self.lmpData.coord_atoms_broken[0]['x y z'.split()] = cords\n",
    "        self.disp       += disp_minimized\n",
    "        self.tdisp      += self.disp\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf38b8c5",
   "metadata": {},
   "source": [
    "# MachineLeranedMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65031dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fout = 'junk.json'\n",
    "# data={'eng':[1966,1974],'bra':[1970,1994]}\n",
    "# df=pd.DataFrame(data)\n",
    "# rwj = utl.ReadWriteJson()\n",
    "# rwj.Write([df.to_dict(orient='list')],fout,\n",
    "#          itime=[10],\n",
    "#          )\n",
    "# #help(utl.ReadWriteJson)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731564e2",
   "metadata": {},
   "source": [
    "## main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ba922c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dump.xyz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-35278ffbfadd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#--- parse atom positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmlmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/%s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfParser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ml mc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfParser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ml mc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dump_file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#--- initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-ff548f2a07e4>\u001b[0m in \u001b[0;36mParse\u001b[0;34m(self, fp)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlmpData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReadDumpFile\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'%s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlmpData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetCords\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mncount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'elapsed time=%s s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Tmp/txt/git/HeaDef/postprocess/LammpsPostProcess.py\u001b[0m in \u001b[0;36mGetCords\u001b[0;34m(self, ncount, sort, columns)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mGetCords\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mslist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dump.xyz'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    mc_steps = eval(confParser['ml mc']['mc_steps'])\n",
    "\n",
    "\n",
    "\n",
    "    mlmc     = MachineLeranedMC(confParser,\n",
    "                                verbose = True\n",
    "                             )\n",
    "\n",
    "    #--- parse atom positions\n",
    "#      mlmc.Parse('%s/%s'%(confParser['ml mc']['input_path'],confParser['ml mc']['dump_file']))\n",
    "    mlmc.LammpsInit(confParser['ml mc']['lammps_script'])\n",
    "\n",
    "    #--- initialization\n",
    "    mlmc.Initialize()\n",
    "#     mlmc.Print(    'allconf',        itime = 0, lammps_xyz = True, kart = True )\n",
    "#     mlmc.Print(    'allconf_defect', itime = 0, lammps_xyz = True, kart = True )        \n",
    "    mlmc.PrintMSD( 'Diffusion.dat',  itime = 0 )\n",
    "#    mlmc.DiscretizeTransitionPath()\n",
    "    \n",
    "    #--- mc loop\n",
    "    for mc_istep in range( mc_steps ):\n",
    "        print('mc_istep=',mc_istep)\n",
    "        \n",
    "        #--- build descriptors\n",
    "        mlmc.GetDescriptors()\n",
    "\n",
    "#         #--- identify defects\n",
    "        mlmc.GetDefects(fp     = '%s/%s'%(confParser['ml mc']['input_path'],confParser['ml mc']['classifier_load']))\n",
    "\n",
    "        #--- predict diffusion paths \n",
    "        mlmc.GetDisp2nd(fp        = '%s/%s'%(confParser['ml mc']['input_path'],confParser['ml mc']['regressor_load']))\n",
    "\n",
    "        #--- predict energy \n",
    "#        mlmc.GetBarrier(fp     = '%s/%s'%(confParser['ml mc']['input_path'],confParser['ml mc']['regressor_barrier']))\n",
    "\n",
    "        #--- build catalog\n",
    "        # fix atoms with type 2\n",
    "        mlmc.BuildCatalog() # filtr = mlmc.atomTypes == 1 ) #--- only include atomType = 1\n",
    "#        mlmc.PrintCatalog( 'catalog.json', itime = mc_istep )\n",
    "        \n",
    "        #--- mc sampling\n",
    "        mlmc.MCsampling()\n",
    "\n",
    "        #--- update disp\n",
    "        mlmc.UpdateDisp()\n",
    "\n",
    "        #--- update coord\n",
    "        mlmc.UpdateCords()\n",
    "\n",
    "        #--- minimize via lammps: further update disp\n",
    "        mlmc.Lammps()\n",
    "        \n",
    "        #--- save output\n",
    "#         mlmc.Print( 'allconf', itime = mc_istep + 1, lammps_xyz = True, kart = True )\n",
    "#         #\n",
    "#         mlmc.Print( 'allconf_defect', itime = mc_istep + 1,\n",
    "#                    AtomIndices = list(set(mlmc.catalog.AtomIndex.astype(int))),\n",
    "#                    lammps_xyz = True, kart = True\n",
    "#                   )        \n",
    "        mlmc.PrintMSD( 'Diffusion.dat',  itime = mc_istep + 1 )\n",
    "\n",
    "            \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnEnv2nd",
   "language": "python",
   "name": "gnnenv2nd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "476.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
